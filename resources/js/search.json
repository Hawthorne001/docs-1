[[{"l":"Observability for Serverless"},{"l":"1. Introduction","p":["Serverless architectures have gained momentum with engineers building distributed systems. However, due to their stateless, ephemeral nature, they are difficult to observe with standard observability approaches. Investigating bugs typically turns into crime scenes, piecing together data from multiple sources to get a blurry image of the root cause of the issue.","Typical serverless application","Baselime is an observability solution for serverless architectures. It enables developers to unlock key insights from their serverless architectures through the use of telemetry data."]},{"i":"2-what-is-serverless","l":"2. What is serverless?"},{"i":"21-microservices","l":"2.1. Microservices","p":["Microservices are a software development approach where an application is built as a collection of small, independent services that communicate with each other over a network. Each service performs a specific business function and can be deployed, scaled, and updated independently of the other services. Microservices are designed to be loosely coupled and use standard interfaces to communicate with other services.","The benefits of microservices include improved scalability, fault tolerance and resilience, as well as easier deployment and maintenance.","Microservices Architecture"]},{"i":"22-serverless","l":"2.2. Serverless","p":["Serverless computing is a cloud computing model where the cloud provider manages and allocates computing resources dynamically based on the demand of the application. In this model, the user does not need to manage or provision any servers, as the provider manages everything related to infrastructure.","The serverless model is event-driven and is commonly used for short-lived functions or applications that need to scale up or down quickly, such as in response to sudden traffic spikes. The application logic is broken down into small functions that can be executed independently, and each function is triggered by an event, such as an API request, a change in a database, or a message from a message queue.","The pricing model for serverless computing is based on the number of requests and the duration of execution, rather than on the underlying infrastructure. This makes it a cost-effective option for applications with unpredictable or variable workloads.","Serverless architecture"]},{"i":"221-functions-as-a-service-faas","l":"2.2.1 Functions-as-a-Service (FaaS)","p":["Functions as a Service (FaaS) is a cloud computing model where a cloud provider manages and runs small code functions in response to specific events or requests. FaaS enables developers to write and deploy individual functions performing a well-defined task as a response to an event. Once the event is triggered, the cloud provider spins up a new instance of the function, runs the code the developer wrote, and returns the result.","In this model, the cloud provider allocates compute resources only when required. When there’s an increase in traffic, more resources are allocated, and when traffic goes down, resources are deallocated. As such, developers don’t need to provision compute and storage resources in advance, or anticipate spikes in traffic with over-provisioning, as is typically the case in traditional architectures.","Lifecycle of serverless function invocations","The ephemeral, auto-scaling and pricing model of FaaS enables developers to accelerate development cycles, as they are freed from the challenges of provisioning, troubleshooting and maintaining servers."]},{"i":"222-troubleshooting","l":"2.2.2 Troubleshooting","p":["In the serverless computing model, accessing and modifying the infrastructure, typically within the cloud provider's environment, can be limited. This can make it challenging for developers to collect data about the behaviour of their applications. Unlike traditional servers, serverless services are managed and scaled by the cloud provider so developers need to adopt different strategies for collecting data about performance, errors and other metrics.","These strategies may include using logging and monitoring tools provided by the cloud provider, adding custom instrumentation to the application code, and implementing distributed tracing techniques to track requests across multiple services. By using these strategies, developers can gain insights into the behaviour of their serverless applications and optimize their performance and scalability."]},{"i":"3-what-is-observability","l":"3. What is Observability?","p":["Observability is the ability to understand and measure the internal state of a system based on its external outputs, achieved through instrumentation.","Imagine you are a developer responsible for maintaining a large e-commerce application that experiences frequent outages and performance issues. You have a hard time pinpointing the root cause of these issues because you lack visibility into the internal state of the application. You are unable to tell what happens to a customer's order when it goes through multiple services, and you struggle to identify which component is causing the slowdown. This is where observability comes in.","Observability is like turning on the lights in a dark room. Without it, developers are working in the dark, struggling to diagnose issues and maintain the health of the system. Observability provides the necessary tools and techniques for developers to gain visibility into the system's internal workings, allowing them to diagnose issues and optimize performance.","Telemetry data is at the heart of observability. Developers need to instrument their systems with various tools that collect and analyze telemetry data. This data may include logs, metrics, traces, and events among other things.","Telemetry data in modern observability","Metrics can include things like the number of requests served, the latency of each request, and the memory usage of each service. Logging enables developers to capture important events and metrics, such as error messages and service availability, while tracing enables them to follow the path of a request through multiple services. By using these observability techniques, developers can quickly diagnose issues and improve the overall health and performance of the system."]},{"i":"31-logs","l":"3.1 Logs","p":["Application logs are time-stamped messages capturing a pre-defined event about an application's behaviour. In a cloud-based environment, where applications are distributed across multiple servers and services, application logs are essential for developers to gain visibility into the system and quickly diagnose issues.","However, text based logs are pretty bad at providing insights into the entire lifecycle of a request or event throughout the entire distributed system. This is why we encourage that you log structured JSON events that provide detailed information about what the application code has been doing. From this you can more easily build an understanding about the state of the system. The @baselime/lambda-logger helps you apply these best practices in a tiny package.","Application logs from AWS CloudWatch"]},{"i":"32-metrics","l":"3.2 Metrics","p":["Application metrics capture quantitative measurements of an application's behavior and performance aggregated over a defined period of time. These metrics provide developers with valuable insights into how the application is performing, how resources are being utilized, and how users are interacting with the system.","Examples of application metrics include response time, error rate, throughput and resource utilization, which can be tracked and analyzed to identify areas for optimization and improvement.","Metrics can give a signal to indicate a defect, but they lack the granularity necessary to pinpoint the root cause of the issue.","Account-level AWS Lambda metrics in AWS CloudWatch"]},{"i":"33-traces","l":"3.3 Traces","p":["Distributed tracing is a technique used to track the path of a request or event as it travels through a complex distributed system, consisting of multiple interconnected services and components. This involves capturing and correlating data from each service involved in processing the request, including information about service dependencies, communication protocols and processing times.","Let’s look at a typical distributed application:","A typical distributed application","The diagram above tells us what components are part of the application, but it doesn’t tell us how the request propagates through the system, which services are performance bottlenecks and which can be optimised. Distributed tracing shines in those scenarios:","A typical distributed trace","The trace above consists of multiple components, each referred to as a span. A span represents a unit of work and its associated processing time. The root span is the first span in the trace and typically represents a request's lifecycle from start to finish. The subsequent spans are child spans, each representing a specific unit of work within the request's journey.","From the trace, we can identify that the calls to service F and service G can be made in parallel, which will make the call to services D and E shorter, ultimately resulting in a faster response to the client.","One of the key ingredients of distributed tracing is their ability to propagate metadata about the request from parent spans to child spans. This context propagation provides developers with a detailed view of how requests flow through the system and enables faster troubleshooting and debugging of complex distributed applications, as well as easier performance optimization.","Distributed trace context propagation"]},{"l":"4. Challenges of Observability for Serverless","p":["Observability for serverless can pose several challenges for developers. One of the major issues is the stateless and ephemeral nature of serverless functions, which makes it harder to pinpoint and troubleshoot issues related to specific requests or users. Serverless platforms abstract much of the underlying infrastructure, reducing the visibility into the runtime environment, and making it harder to troubleshoot issues when they inevitably occur.","Additionally, the event-driven nature of serverless applications can exacerbate the observability challenge. In contrast to traditional HTTP-based architectures, serverless functions very often communicate with each other through event brokers like message queues, event buses or event streams, introducing new complexities, such as asynchronous processing, message ordering, and event aggregation. The non-linear path of event-driven architectures can make it more difficult to trace requests, especially when multiple services consume and produce events asynchronously.","Typical event-driven architecture","Event-driven serverless systems bring unique challenges that developers must address:","Context propagation: context propagation is critical to creating a complete trace of the request's lifecycle. Events must carry the context of upstream services and propagate it to downstream services and functions. This context propagation needs to account for the specificity of each event bus, queue or stream to create an unbroken trace across the lifecycle of the request.","Identity and Access Management: Identity and access management (IAM) is essential to ensure each service in the architecture has the necessary permissions to interact with other components. Improperly configured IAM can lead to process failures that are difficult to track.","Metrics: Metrics provide valuable context about requests and services that should not be ignored. Developers must observe each service used to communicate between serverless functions through metrics and logs when available.","To overcome these challenges, developers can leverage tools like AWS CloudTrail, which enables them to track IAM activities and diagnose operational issues."]}],[{"i":"what-is-baselime","l":"What is Baselime?","p":["Baselime is an observability solution built for modern cloud-native environments. It enables teams to identify defects, correlate telemetry data, and find the root case of issues faster, without compromising on cost, cardinality, or scale.","Baselime Console","Baselime makes it easy to observe your cloud services, containers orchestrators (AWS ECS), and serverless functions. Monitor everything, from function latencies and cold-starts to business domain metrics derived from your logs and distributed traces."]},{"l":"Our mission","p":["Our mission is to simplify the complexity of serverless stacks. We make observability easy for developers such that they can focus on what truly matters: building better products."]},{"i":"how-is-baselime-different","l":"How is Baselime different?","p":["Traditional monitoring solutions were built for a world where most applications were single monolithic applications. These solutions lack when it comes to microservices and serverless in 3 major ways:","Data volume and cardinality: Microservices generate a higher amounts of data with higher cardinality. The cost of traditional solutions scales exponentially with higher cardinality data.","Distributed transactions: Microservices handle distributed transactions across multiple services and serverless functions. Traditional solutions based on logs struggle to capture the end-to-end transactions, and limit the ability to diagnose performance bottlenecks and failures.","Silos and fragmentation: Traditional solutions provide typically previde logs, metrics and traces in isolation without the contextual correlation to understand the business impact of defects and performance issues.","Baselime directly connects to your AWS account and uses OpenTelemetry to automatically instrument your microservices and serverless functions.","Also, Baselime uses our proprietary query engine built on top of ClickHouse to effectively index all your telemetry data. From day 0, everything is queriable and searchable, and correlation between data sources is a breeze."]},{"i":"how-does-baselime-work","l":"How does Baselime work?","p":["Connecting your AWS account to Baselime takes under 2 minutes, after which you can start troubleshooting and solving infrastructure and application issues unbelievably fast with high cardinalty data.","Baselime is built on top of ClickHouse, the open-source fastest database in the world and leverages OpenTelemetry to instrument your applications.","Do you want to learn more about OpenTelemetry? Start here.","Baselime does not perform any pre-aggregation of data before ingestion; as such developers can run arbitrary queries on their telemetry data, and get answers about the state of their application, regardless of how unusual or unique this state is.","Baselime in your ecosystem"]},{"i":"why-baselime","l":"Why Baselime?"},{"l":"End-to-end observability","p":["Slice and dice millions of logs, metrics, traces and events faster than on any other serverless solution. Full visibility across your stack, no blind-spots."]},{"i":"detect-solve-issues-faster","l":"Detect & solve issues faster","p":["Troubleshoot infrastructure and application issues unbelievably fast with high cardinality data."]},{"i":"unlimited-cardinality-all-indexed-all-searchable","l":"Unlimited cardinality, all indexed, all searchable","p":["Query against any nested field and automatically surface anomalies; no matter how rare, specific, or deep in your serverless stack."]},{"l":"Take control of your data and costs","p":["Up to 6x more value than the big dogs. No per-function pricing, no per-seat pricing, no per-alert pricing. Start at $0 and scale up as your apps grow, with no hidden fees."]}],[{"l":"FAQ"},{"i":"what-is-baselime","l":"What is Baselime?","p":["Baselime is an observability solution that makes observability for cloud-native microservices easy. Baselime covers your logs, metrics, traces (both OpenTelemetry and AWS X-Ray) and security events in a single solution. Baselime is built on top of ClickHouse, the fastest analytics database in the world."]},{"i":"how-much-does-it-cost","l":"How much does it cost?","p":["Baselime pricing is based on the number of invocations or traces your systems produce. This scales linearly with the traffic your applications handle. Moreover, Baselime has a full free tier for up to 200K invocations / traces.","Check out our pricing page for more details."]},{"i":"how-does-baselime-count-invocations-and-traces-for-billing","l":"How does Baselime count invocations and traces for billing?","p":["Invocations are counted using the REPORT line of your AWS Lambda function logs. Each instance of the REPORT line is an invocations. Distributed traces are counted using the number of unique trace ids received from your services. If a traceID is matched with an existing invocation using the REPORT line, the trace is not counted towards your monthly number of traces.","If a trace goes through multiple function invocations, each invocation is counted individually, and the trace is not counted. As such you are not charged twice for the same invocations.","Baselime counts the number of invocations and traces daily and updates your dashboard accordingly."]},{"i":"does-baselime-support-containers","l":"Does Baselime support containers?","p":["Baselime works with any environment where OpenTelemetry is available. Moreover, Baselime provides an events HTTP API that could be used to send events individually from environments where OpenTelemetry is not available.","That being said, Baselime has a native integration with both serverless and container platforms on AWS:","AWS Lambda","Amazon ECS (Fargate and EC2)","Amazon AppRunner"]},{"i":"does-baselime-support-multi-accounts-and-multi-regions","l":"Does Baselime support multi-accounts and multi-regions?","p":["Yes, Baselime has support for multi-accounts and multi-region. When you connect your first cloud account to Baselime, Baselime created a Baselime Environment. You can subsequently add as many new cloud accounts or region to the Baselime environment. All your telemetry data from those separate accounts and regions will be unified in the Baselime environment."]},{"i":"how-easy-is-it-to-instrument-my-applications","l":"How easy is it to instrument my applications?","p":["When you connect your AWS account to Baselime, logs from your AWS Lambda functions, API Gateways and AppRunner services, and metrics from your entire AWS account are automatically ingested into Baselime. No further setup is required.","Moreover, if you have Amazon X-Ray enabled on your services (both serverless functions and containers), these traces are automatically ingested into Baselime.","To adopt OpenTelemetry distributed tracing, add the baselime:tracing tag to your AWS Lambda functions using the Node.js runtime and these will be automatically instrumented. We're currently working on more runtimes."]},{"i":"how-do-i-get-distributed-tracing","l":"How do I get distributed tracing?","p":["Baselime supports both OpenTelemetry and AWS X-Ray for distributed tracing. If you application is already instrumented with OpenTelemetry, change the destination of your instrumetation to the Baselime endpoint:","URL: https://otel.baselime.io/v1","Header: x-api-key: BASELIME_API_KEY","Alternatively, you can instrument your AWS Lambda function with the Baselime OpenTelemetry tracer. Simply add the baselime:tracing tag to your AWS Lambda functions, and set it to true.","The automatic OpenTelemetry tracing with the tag is available for Node.js AWS Lambda functions, we're currently working on enabling this for other runtimes.","If you use AWS X-Ray, Baselime automatically capture traces from X-Ray when your AWS Account is connected."]},{"i":"how-hard-is-it-to-remove-baselime-from-my-aws-account","l":"How hard is it to remove Baselime from my AWS account?","p":["If you decide to remove Baselime from your AWS account, delete the CloudFormation template Baselime creates on your AWS account. That's all, all resources Baselime created, including the instrumentation layers, will be removed."]},{"i":"does-baselime-automatically-recognise-new-functions-and-services","l":"Does Baselime automatically recognise new functions and services?","p":["Yes, when you deploy new serverless functions and services to your cloud infrastructure, Baselime automatically detects them and starts ingesting logs, metrics and traces from those function. To add OpenTelemetry tracing, add the baselime:tracing tag to your new functions and set it to true."]},{"i":"where-is-my-data-stored","l":"Where is my data stored?","p":["You own your data.","All the telemetry data your cloud infrastructure generate is storred in two data tiers:","hot tier: on Baselime accounts, to enable fast queries","cold tier: in an Amazon S3 bucket in your AWS cloud account for long terms storage","It is possible to rehydrate data from the cold tier to the hot tier for queriyng historical incidents free of charge."]},{"i":"is-my-data-secure","l":"Is my data secure?","p":["Baselime is fully GDPR compliant and your data is storred in data centers that are all SOC2 compliant."]},{"i":"how-can-i-work-with-my-team","l":"How can I work with my team?","p":["Once you sign up to Baselime with your organisation domain email, you can configure Baselime such that anyone with the same email domain can join your workspace.","Moreover, you can invite your teammates individurally. Additionally, every query result, dashboards, an alerts have a unique permalink in Baselime that you can share with your team."]},{"i":"does-baselime-have-an-impact-on-my-aws-bill","l":"Does Baselime have an impact on my AWS bill?","p":["Baselime relies on a few AWS resources in your AWS account, most notably:","Amazon CloudWatch metrics stream: to enable CloudWatch metrics to be queried using the Baselime query engine","Amazon CloudTrail: to enable CloudTrail events, and also register new subscription filters as soon as new serverless functions or services are created","Amazon Kinesis Data Firehose: To store telemetry data in cold storage in your AWS account","These services may add a minimal cost on your AWS monthly bill. Please refer to the AWS princing calculator for estimates based on your usage."]}],[{"l":"Quickstart Guide","p":["Welcome to Baselime! This quickstart guide will help you get up and running with the platform in just a few steps.","All you need is:","An AWS Account","Permissions to deploy a CloudFormation stack with IAM role.","A deployed application leveraging AWS Lambda and other AWS serverless services","If you do not have a deployed application, you can use our example pokedex.","Baselime in your ecosystem"]},{"i":"step-1-sign-up-for-baselime","l":"Step 1: Sign up for Baselime","p":["To use Baselime, you'll need to sign up for an account. You can sign up for a free account here.","Create a workspace. Typically this will be the name of your organisation.","Start exploring the telemetry data in your workspace sandbox."]},{"i":"step-2-optional-install-the-baselime-cli","l":"Step 2: (Optional) Install the Baselime CLI","p":["The Baselime CLI is a command-line tool that you can use to interact with the platform. Installing the CLI unlocks Observability as Code and provides additional functionality not available in the web console. To install the CLI, follow the instructions here."]},{"i":"step-3-connect-your-aws-account","l":"Step 3: Connect your AWS account","p":["To start collecting telemetry data from your serverless application, connect your AWS account to Baselime. This is done by deploying a CloudFormation template onto your AWS account.","You can generate the CloudFormation template from the Baselime console.","Next, you must deploy the template to your AWS account:","Click the link provided by the Baselime Console or CLI to open the CloudFormation service in your AWS account","Check the box to acknowledge that the template creates IAM roles","Click \"Create stack\" to deploy the stack, making sure to use the correct credentials and region for your AWS account","We've open-sourced the CloudFormation template here.","Once the stack is deployed, telemetry data from your AWS account will be automatically ingested by Baselime and will be available through the various clients.","To verify the connection, invoke any deployed AWS Lambda function in your account and you should see data from it in the Baselime console within seconds. Additionally, you can stream all the events ingested by Baselime directly in your terminal using the baselime tail command.","If you do not complete any of the above steps, Baselime will not be able to ingest data from your AWS account.","If you do not see any data in the Baselime UI or using the baselime tail command within seconds of completing the above steps, something went wrong. Please contact us.","If you want to disconnect your AWS Account from Baselime, delete the CloudFormation stack it creates in your account. Baselime will delete the resources it created in your account, including the CloudWatch logs subscription filters. A few resources will remain, such as the Baselime IAM role and the S3 bucket where all the telemetry data lives."]},{"i":"step-4-explore-your-data","l":"Step 4: Explore your data","p":["Once your AWS Account is connected, you can start exploring the telemetry data it generates. You can use the web console or the CLI (if installed) to access and analyze the data.","Baselime ingests and indexes every field and nested field in your telemetry data."]},{"l":"Accessing data in the web console","p":["To explore the data in the web console:","Go to the Baselime Console and sign in with your account","Select your environment from the list of environments","Use the various filters and tools in the console to slice and dice the data, such as:","Filtering by resource type, key-value pair, operation type, or time range","Searching for specific strings or regexes in the data","Viewing the trace data for a specific request or operation","Viewing the logs and metrics for a specific resource or operation","Segmenting the results by specific field or nested field"]},{"l":"Accessing data in the CLI","p":["To access the data in the CLI:","If you installed the CLI, you can use the baselime query command to interactively explore the data. Here's how it works:","If you haven't already done so, sign in to the CLI using the baselime login command","Run the following command:","Select the service you want to query","Select one of your saved queries or interactively build a query","Enter the start and end time for the query (optional - defaults to the past hour)","The command will output a table with the results of the query and a unique URL that you can share with your team"]},{"l":"Guides","p":["Sending Data: Learn how to ingest telemetry data from your serverless applications","Analyzing Data: Discover how to use the various interfaces provided by Baselime to analyze and understand your data","Integrations: Find out how to connect Baselime with your favorite tools"]},{"l":"Reference","p":["Baselime CDK Reference Guide: Learn about how to use Baselime with the AWS CDK to define your Observability as Code","ORL Reference Guide: Learn about the Baselime Observability Reference Language (ORL) and how to use it to define observability configurations","CLI Reference: Complete reference for the Baselime command-line interface"]},{"l":"Community","p":["Join the Baselime community to get help with using the platform, share your own experiences, and stay up-to-date with the latest developments.","Slack: Join our Slack community to connect with other Baselime users and get real-time support from the Baselime team","Blog: Read about the latest features, best practices, and more from the Baselime team","Social media: Follow us on Twitter, LinkedIn, and YouTube to stay up-to-date with the latest news and updates from Baselime","We look forward to connecting with you!"]}],[{"l":"Sending Data to Baselime","p":["Baselime supports a variety of data sources, including logs, metrics, traces, and wide events. You can start sending your data to Baselime and gain valuable insights into the performance and reliability of your serverless applications with a few steps.","Once ingested, the data is securely stored in hot storage for querying and in cold storage within your own AWS environment, in an Amazon S3 bucket in your AWS account. This ensures that you have complete long-term control over your data and its storage location.","Sending Telemetry data to Baselime"]},{"l":"Data sources","p":["Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime"]}],[{"l":"AWS Lambda Logs","p":["Once you connect your AWS account to Baselime, Baselime automatically create CloudWatch Logs subscription filters to automatically ingest logs from your AWS Lambda functions.","Baselime automatically captures logs for newly created AWS Lambda functions, and enables you to query and visualise logs across log groups and log streams."]},{"l":"How it works","p":["Once Baselime is connected to an AWS Account, it automatically creates Logs subscription filters for all the AWS Lambda functions in the account. Log subscription filters enable Baselime to asynchronously ingest logs from the AWS Lambda functions through Amazon CloudWatch, without any impact on the performance of the AWS Lambda functions.","Sending Lambda Logs to Baselime","Moreover, Baselime automatically creates new subscription filters for newly deployed AWS Lambda functions. Baselime listens to new AWS Lambda events in Amazon CloudTrail and creates subscription filters for newly created AWS Lambda functions.","An alternative method to ingesting AWS Lambda logs is with the use of the Baselime Lambda Extension."]},{"l":"Logging best practices","p":["In order to get the most out of Baselime, we recommend adding two log messages to all your AWS Lambda functions:","the event which triggered your Lambda function","the response your Lambda function returns","These can be added as follows:","To facilitate this in Node.js runtimes, we maintain a custom logger well suited for AWS Lambda.","It's a 2.5kb JavaScript file with 0 dependencies, and does not have any significant impact on performance or cold-starts.","It also provides an interface to be used as a middy middleware."]},{"l":"Logging format","p":["We recommend using structured logging across your application, preferably in JSON format. Feel free to use your favourite logging library; we recommend:","Baselime Lambda Logger for Node.js","Lambda Power Tools","It is particularly important to format errors and exception correctly to appropriately log stack traces.","or with the Baselime Lambda Logger for Node.js:"]},{"l":"Discovered Keys","p":["Baselime automatically discovers key - value pairs from your AWS Lambda logs. This enables you to run complex queries and setup alerts on data that otherwise would be difficult to work with from the AWS Lambda service. For instance, from the discovered keys from the Lambda logs, it's possible to set alerts on the maximum memory used by lambda functions during execution, compared to the amount of memory they are assigned at deployment time."]},{"l":"Lambda Discovered Keys","p":["The Lambda service automatically writes logs at the start and end of every function invocation. These logs are parsed as events in Baselime, and keys are automatically discovered from those messages."]},{"l":"START Log Message","p":["The following keys are discovered from the START message:","@type: is always START","@requestId: the request ID of the Lambda invocation","@version: the invoked version of the Lambda function"]},{"l":"END Log Message","p":["The following keys are discovered from the END message:","@type: is always END","@requestId: the request ID of the Lambda invocation"]},{"l":"REPORT Log Message","p":["The following keys are discovered from the REPORT message:","@type: is always REPORT","@requestId: the request ID of the Lambda invocation","@duration: the duration in milliseconds","@billedDuration: the billed duration in milliseconds","@memorySize: the total memory available to the invocation, in MB","@maxMemoryUsed: the max memory used, in MB","@initDuration: the duration of the lambda initialisation in milliseconds (cold starts)","If the Lambda function is instrumented with XRAY, additional keys are discovered:","@xRAYTraceId: the XRAY trace ID","@segmentId: the XRAY segment ID","@sampled: always true"]},{"l":"Timeout Invocations","p":["If your async Lambda invocation times out, Additional keys are automatically discovered:","@timedOut: always true","@timeout: the duration after which the invocation timed-out in seconds","@message: always Task timed out after {@timeout} seconds","@timestamp: the timestamp at the moment the invocation timed out."]},{"i":"consolelog-log-message","l":"console.log Log Message","p":["For Node.js environments, AWS Lambda uses a modified version of console.log(and other console logging functions) to write to stdout and stderr. These add fields to the log message which are parsed as follows:","@timestamp: the timestamp at the moment the log message was written","@requestId: the request ID of the Lambda invocation","LogLevel: the log level ( INFO, DEBUG, WARN, ERROR)","@message: the message.","If the message in @message is a valid JSON object, Baselime will parse it, otherwise it will be considered a string."]},{"l":"Troubleshooting","p":["If you're having trouble sending data from your AWS Lambda logs to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as CloudWatch Metrics or CloudTrail Events","Check that your Lambda functions are not already using the maximum number of subscription filters allowed per log group. AWS limits each log group to 2 subscription filters at most. If you're already at the limit, you can remove subscription filters with the cloudwatch-subscription-filters-remover to delete the ones you don't need anymore.","Make sure that your AWS Lambda functions are being invoked and you can view the logs in the CloudWatch section of the AWS Console"]}],[{"l":"OpenTelemetry Traces","p":["If your codebase is already instrumented with OpenTelemetry, you can start sending us your tracing data today.","Add the Baselime OTel endpoint to your exporter:","Endpoint https://otel.baselime.io/v1/","Header: x-api-key: YOUR_BASELIME_API_KEY","You can get your Baselime API key from the Baselime CLI with","If you have not instrumented your codebase with OpenTelemetry yet, do not worry. We are building OpenTelemetry instrumentation methods for serverless that will not significantly negatively impact the performance of your systems."]},{"l":"OpenTelemetry Auto Instrumentation","p":["OpenTelemetry Traces"]}],[{"i":"opentelemetry-for-nodejs-on-aws-lambda","l":"Opentelemetry for Node.js on AWS Lambda","p":["The Baselime Node.js OpenTelemetry tracer for AWS Lambda(Star us ⭐) instruments your Node.js AWS Lambda functions with OpenTelemetry and automatically sends the OpenTelemetry compatible trace data to Baselime. This is the most powerful and flexible way to instrument your Node.js AWS Lambda functions."]},{"l":"Automatic Instrumentation","p":["To automatically instrument your AWS Lambda functions with the Baselime Node.js OpenTelemetry tracer for AWS Lambda, set the following tag to your AWS Lambda functions: baselime:tracing=true.","To add the Baselime tag to all your AWS Lambda functions in a service or stack add this line to your AWS CDK code.","To add the Baselime tag to all your AWS Lambda functions in a service or stack add this line to your sst.config.ts file.","To add the Baselime tag to all your AWS Lambda functions in a add this snippet to your serverless.yml file.","To add the Baselime tag to all your AWS Lambda functions in a add this snippet to your AWS SAM configuration file.","OpenTelemetry Automatic Instrumentation works only once you have connected your AWS Account to Baselime. Adding the tag to AWS Lambda functions in an AWS Account not connected to Baselime will not have any effect."]},{"l":"How it works","p":["The automatic instrumentation makes changes to your AWS Lambda functions once they are deployed:","Add the Baselime Node.js OTel AWS Lambda Layer to your AWS Lambda function: arn:aws:lambda:${region:097948374213:layer:baselime-node:6- This layer is a slimmed down version of the OpenTelemetry JavaScript Client that will have minimal impact on the cold starts of your AWS Lambda functions","Add the Baselime Extension added to your AWS Lambda function: arn:aws:lambda:${region}:097948374213:layer:baselime-extension-${'x86_64' || 'arm64'}:1- This extension enables the Baselime Layers to send the trace data to the Baselime backend after the invocation is complete, as such, distributed tracing will not have any negative impact on the latency of your AWS Lambda functions","Set the BASELIME_KEY environment variable with the value of your environments Baselime API Key","These changes are kept in sync with your AWS Lambda function as you iterate on your architecture via events from Amazon CloudTrail.","OpenTelemetry Automatic Instrumentation FLow"]},{"l":"Manual Instrumentation","p":["If you prefer to send the OpenTelemetry traces to Baselime manually, you can use the Baselime Node.js OpenTelemetry tracer for AWS Lambda(Star us ⭐) independently from connecting your AWS Account to Baselime."]},{"l":"Step 1","p":["Install the Baselime Node.js OpenTelemetry tracer for AWS Lambda."]},{"l":"Step 2","p":["Wrap the handlers of your AWS Lambda functions with the baselime.wrap(handler) method."]},{"l":"Step 3","p":["Set the environment variables of your AWS Lambda functions to include the Baselime API Key and set the NODE_OPTIONS enviroment variable to preload the OpenTelemetry SDK into your AWS Lambda bundle.","Key","Value","Description","BASELIME_KEY","Get this key from the cli running baselime iam","NODE_OPTIONS","--require @baselime/lambda-node-opentelemetry","Preloads the OpenTelemetry SDK at startup"]},{"l":"Step 4","p":["Ensure that the OpenTelemetry SDK is included in the .zip file that is uploaded to AWS Lambda during your deployment. The step depends on your deployment framework.","Set the default function props of your service to include the wrapper in the bundle and add the environment variables","By default the Serverless Framework includes the entire node_module folder in the .zip bundle of your AWS Lambda functions. If you are using the serverless-esbuild plugin or any other plugin to prevent this, it is necessary to edit the configuration of your project.","Add the following line to the package.patterns block of your serverless.yml file.","Add the following environment variables","Copy the lambda-wrapper.js file from the node_modules folder in the shared folder of your Architect project, it will be automatically included in all of your AWS Lambda .zip bundles.","Add the environment variables to your architect project","Note the '--' in the NODE_OPTIONS command. This is required to escape options parsing.","This method will however send the traces to the Baselime backend during the invocation of your AWS Lambda functions, and will result in a degradation in the latency performace of your functions.","In production we recommend additionally adding the Baselime AWS Lambda extension, as it will enable the OTel tracer to send traces to the Baselime backend after the excecution of your AWS Lambda functions."]},{"l":"Adding custom OpenTelemetry events","p":["The Baselime Node.js OpenTelemetry tracer for AWS Lambda provides and extension that enables you add context rich events to your traces using an API that feels like a logger. These events can be useful to show more detailed context on errors, add steps that you want recorded for a business process or adding extra debugging information.","The extension provides an object that includes four logging functions - info, warn, debug, and error - enabling you to log messages with varying levels of severity. By setting the LOG_LEVEL environment variable, you can control the visibility of the events.","It shares the same interface as @baselime/lambda-logger so if you are moving from cloudwatch to open telemetry this makes the transision seamless."]},{"l":"Adding custom OpenTelemetry spans","p":["To add custom spans to your OpenTelemetry traces, it is necessary to install the @opentelemetry/api package. It is left out of the Baselime Node.js OpenTelemetry tracer for AWS Lambda to limit the impact on cold-starts, such that your can add it only to the AWS Lambda functions that require it."]},{"l":"Sending data to another OpenTelemetry backend","p":["OpenTelemetry is an open standard, and you can use the Baselime Node.js OpenTelemetry tracer for AWS Lambda to send telemetry data to another backend of your choice.","Add the environment variable COLLECTOR_URL to send the data somewhere else than the Baselime backend."]},{"l":"Limitations","p":["The AWS JS SDK v2 can result in errors when interacting with OpenTelemetry during automatic request retries. This is the result of trace headers changing between retries and failing the signing verification processes. We've submitted a Pull Request to the AWS JS SDK and will be updating accordingly.","To prevent this issue from arising, add the code snippet below to your code."]}],[{"l":"Amazon API Gateway Logs","p":["Once you connect your AWS account to Baselime, Baselime automatically create CloudWatch Logs subscription filters to automatically ingest logs from your Amazon API Gateways."]},{"l":"Setup","p":["Baselime can ingest logs only for Amazon API Gateways where access logs are appropriately configured.","We recommend this configuration for Amazon API Gateway logs:","It is possible to enable Amazon API Gateway access logs from your favourite Infrastructure as Code tool, using the CLI or in the AWS console. Below is an example of how to enable Amazon API Gateway logs using the serverless framework."]},{"l":"How it works","p":["Once Baselime is connected to your AWS Account, it automatically creates Logs subscription filters for all the Amazon API Gateways in the account.","Sending API Gateway Logs to Baselime","Moreover, Baselime automatically creates new subscription filters for newly deployed Amazon API Gateways. Baselime listens to new API Gateway events in Amazon CloudTrail and creates subscription filters for newly created Amazon API Gateways."]}],[{"l":"Amazon ECS Container Logs","p":["This page describes how to collect application container logs from Amazon ECS clusters launched with AWS ECS using AWS FireLens. This method can also be used to collect ECS clusters with EC2 containers.","Sending AWS ECS container logs to Baselime is optional, but highly recommended to gain full visibility into your containerized applications."]},{"l":"How it works","p":["FireLens is an Amazon ECS native log router that enables you to send logs from your containerized applications to different destinations, including Baselime. By adding the FireLens sidecar to your task definitions, you can configure and route your container logs to different destinations without modifying your application code.","Sending ECS Logs to Baselime","Each of your ECS tasks can take a sidecar container running the FireLens log driver that will forward all the logs from the containers to Baselime."]},{"l":"Configuring your ECS Tasks"},{"i":"step-1-obtaining-your-baselime-api-key","l":"Step 1: Obtaining your Baselime API Key","p":["You can get your Baselime API key using the Baselime CLI. Ensure you have downloaded the Baselime CLI and logged in your environment. To get your Baselime API Key, run the following command","Copy the Baselime API Key in the output of the command and keep it safe. In the following instructions we will use BASELIME_API_KEY to refer to your Baselime API key, make sure to replace it with the key you copied from the output of the command."]},{"i":"step-2-adding-the-firelens-sidecar-to-your-task-definitions","l":"Step 2: Adding the FireLens sidecar to your task definitions","p":["Adding the FireLens sidecar to your task definitions is a straightforward process that can be accomplished using various Infrastructure as Code solutions or manually in the console.","Add the Baselime ECS endpoint to your FireLens configuration:","Endpoint ecs-logs-ingest.baselime.io","Header: x-api-key BASELIME_API_KEY"]},{"l":"Using Terraform"},{"l":"Using AWS CDK for TypeScript"},{"l":"Directly in AWS console"},{"l":"Troubleshooting","p":["If you're having trouble sending data from your AWS ECS logs to Baselime, here are a few things to check:","Verify that you're using the correct API key and host in the FireLens configuration","Make sure that your containers are receiving traffic and are writing logs to either stdout or stderr","Check the logs of the FireLens container to look for any anomaly"]}],[{"l":"Lambda Telemetry Extension","p":["Instrumenting AWS Lambda functions with Baselime is straightforward using our Lambda Extension. The Lambda Extension listens to invocation events and collects telemetry data, such as logs and runtime metrics. Once collected, the telemetry data is sent to Baselime for storage, analysis, and visualization. In this section, we'll walk you through the process of instrumenting your Lambda functions with the Baselime Lambda Extension.","The Baselime Lambda telemetry extension is an optional tool that provides additional telemetry data for your AWS Lambda functions. It is only necessary if you choose not to ingest Lambda logs directly from CloudWatch.","Before getting started you'll need to make sure that you have your Baselime API key ready. You can get it by running the following command using the Baselime CLI."]},{"l":"How it works","p":["The Baselime Lambda Extension is language agnostic and is compressed as a single binary, such that it minimises its impact cold-starts and performance.","The diagram below illustrates how the Baselime Lambda Extension works within your architecture.","Using the Baselime Lambda Extension","All the telemetry data from your Lambda function is collected asynchronously from your invocation, and sent to the Baselime backend in a separate process from your invocation."]},{"l":"Instrumenting","p":["To instrument your AWS Lambda Functions with the Baselime Lambda Extension, we recommend using your Infrastructure as Code tool of choice, and add the Extension as a Lambda Layer.","It is necessary to add the Baselime API key to the extension as an environment variable. The example below shows the process with the serverless framework.","Where the BASELIME_KEY is your Baselime API Key and the BASELIME_LAMBDA_LAYER_ARN is the ARN of the Baselime Layer in your region."]}],[{"l":"AWS X-Ray Traces","p":["AWS X-Ray enables developers to gather traces across their distributed services. In order to gain visibility into their applications, developers can use AWS X-Ray to trace requests as they travel through their application, and collect data about the performance of their application.","Baselime enables you to ingest this tracing data and make it available for analysis and troubleshooting."]},{"l":"How it works","p":["To start ingesting traces from AWS X-Ray to Baselime, you'll need to connect your AWS account to Baselime. Once connected, Baselime will periodically poll your AWS account for new traces and automatically ingest them into your Baselime dataset.","Sending X-Ray Traces to Baselime"]},{"l":"Troubleshooting","p":["If you're having trouble sending data from AWS X-Ray to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as CloudWatch Metrics or CloudTrail Events","Check that the Baselime IAM user has the appropriate permissions to access X-Ray","Make sure that your applications emit X-Ray traces and you can view the traces in the X-Ray section of the AWS Console"]}],[{"l":"Amazon CloudTrail","p":["Baselime automatically ingests Amazon CloudTrail events when you connect your AWS account. Baselime will automatically create a new Amazon CloudTrail trail and an Amazon S3 bucket, and configure both to send data to your Baselime account. No additional setup is required.","Once connected, Amazon CloudTrail events will be sent to Baselime and become available for querying."]},{"i":"why-amazon-cloudtrail-","l":"Why Amazon CloudTrail ?","p":["Amazon CloudTrail is a service provided by AWS that records API activity in your AWS account. This data can be used to track changes to your resources, troubleshoot issues, and improve security.","By sending Amazon CloudTrail events to Baselime, you can use our query and visualization tools to analyze and understand your API activity. You can also set up alerts to be notified of specific API activity or trends.","With Amazon CloudTrail events in Baselime, you can gain a deeper understanding of your AWS API activity and use that knowledge to improve the security and reliability of your applications."]},{"l":"How it works","p":["Amazon CloudTrail periodically writes trail data in a pre-configured Amazon S3 bucket in your AWS account. Once the data is written, an Amazon SNS topic is triggered.","Baselime configures this Amazon SNS to invoke an AWS Lambda function. This function reads the data from the bucket and sends it to the Baselime backend.","Sending CloudTrail data to Baselime"]},{"l":"Amazon CloudTrail management events","p":["Amazon CloudTrail events fall into multiple categories, and Baselime automatically ingests CloudTrail management events. Please refer to the complete CloudTrail docs for further details on the CloudTrail concepts."]}],[{"l":"Amazon CloudWatch Metrics","p":["Baselime automatically collects Amazon CloudWatch Metrics from your AWS account. Once you connect your AWS account to Baselime, the necessary resources including a CloudWatch Metrics Stream and a Kinesis Firehose will be automatically created and configured. No additional setup or configuration is required."]},{"i":"why-amazon-cloudwatch-metrics-","l":"Why Amazon CloudWatch Metrics ?","p":["Amazon CloudWatch is a monitoring service provided by AWS that enables you to collect and track metrics for your AWS resources and applications. Metrics are important as they provide insight into the performance and behavior of your serverless applications and the underlying infrastructure.","Amazon CloudWatch Metrics can help you identify issues such as high error rates and latencies, which can help improve the overall reliability and scalability of your applications.","Amazon CloudWatch Metrics cover all aspects of your serverless architecture automatically, from DynamoDB tables to S3 buckets and SQS Queues."]},{"l":"How it works","p":["Once Baselime is connected to an AWS Account, it automatically created the telemetry pipeline for ingesting Amazon CloudWatch metrics into Baselime. The pipeline comprises a CloudWatch Metrics Stream, a Kinesis Firehose and all IAM roles and permissions associated.","This pipeline automatically and continuously sends metrics from your AWS account to Baselime.","Sending Amazon CloudWatch Metrics to Baselime","Amazon CloudWatch Metrics Stream might incur a minimal cost on your AWS account. AWS charges $0.003 per 1,000 metric updates. Refer to the AWS docs for more details."]},{"l":"Custom Amazon CloudWatch Metrics","p":["Baselime automatically ingests all metrics published to Amazon CloudWatch. This includes both standard Amazon CloudWatch metrics and any custom metrics that you may have created.","There is no need to manually configure or set up anything to start ingesting custom Amazon CloudWatch metrics. Once your AWS account is connected, all metrics will be available for querying in Baselime."]},{"l":"Querying Amazon CloudWatch Metrics","p":["Once your AWS account is connected to Baselime, you can use any of the our clients to visualize and query your Amazon CloudWatch Metrics. You'll have access to all the metrics available in your AWS account, and you can use the Observability Reference Language (ORL) to filter and aggregate the data in near real-time."]},{"l":"Troubleshooting","p":["If you're having trouble sending metrics from Amazon CloudWatch to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as AWS Lambda Logs or CloudTrail Events","Check that the Kinesis Firehose created in your AWS account as part of the Baselime connection has the appropriate API key to connect with the Baselime backend. If the API key is missing, please contact us."]}],[{"l":"Events API","p":["Baselime provides an HTTP events API which enables developers to send data to Baselime by making a POST request to the API endpoint. This enables developers to send data directly from their applications or services to Baselime, rather than using a logging or monitoring service as an intermediary."]},{"l":"Request Format","p":["Each request ingests a batch of events into Baselime. Events are part of the request body. Baselime supports Content-Type application/json.","The request body must be an array of JSON objects. Any element of the array that cannot be parsed as valid JSON will be rejected.","Requests must be made to the /dataset/service/namespace route:","dataset is the name of the dataset that the events should be ingested into. You can either use an existing dataset or create a new one using the Baselime CLI.","service is the service that the events belong to. If the service doesn't exist beforehand, the events can be queried through the default service. Once you create the service (in the web console or using the Baselime CLI), the events will be available from the service too.","namespace is the namespace within the dataset that the events should be ingested into. The namespace is created automatically for you when events are received, if it didn't exist beforehand."]},{"l":"Authentication","p":["The HTTP API requires a valid Baselime API key to be sent in the x-api-key request header.","You can obtain your API key using the Baselime CLI."]},{"l":"Validation","p":["The HTTP API validates the provided events and returns a 400 Bad Request status code if any of the events fail validation with a list of all the events that failed validation. If some events pass validation and others fail, we will ingest the events that pass validation. If you encounter a 400 Bad Request error when submitting events to the HTTP API, the events that failed validation will be listed in the body of the request under the invalid key."]},{"l":"High-level requirements","p":["Baselime accepts up to 6MB of uncompressed data per request","Each event must be a properly formatted JSON","Each event must be smaller than 128kb of uncompressed JSON"]},{"l":"API Response codes","p":["Baselime returns a 202 response for all valid requests to the HTTP Events API, and a range on of non- 200 responses for errors.","We welcome feedback on API responses and error messages. Reach out to us in our Slack community with any request or suggestion you may have."]},{"l":"Successful responses","p":["Status Code","Body","Meaning","202","{message: Request Accepted}","All the events were successfully queued for ingestion"]},{"l":"Failure responses","p":["Status Code","Body","Meaning","405","{message: Method Not Allowed}","The HTTP method is now allowed","401","{message: Unauthorised}","Missing or invalid API Key","400","{message: Bad Request}","- Missing or invalid path parameters ( v1, dataset, service or namespace) - Unable to parse the request body as valid JSON- Empty request body - At least one of the events exceed the 128kb size limit - At least one of the events could not be parsed as valid JSON","500","{message: Internal Error}","An unexpected error occurred"]}],[{"l":"Rehydrating telemetry data from Amazon S3","p":["This page describes how you can rehydrate your telemetry data from Amazon S3 into Baselime."]},{"l":"How it works","p":["When your data is streamed to Baselime, through different sources described in the Sending Data to Baselime section, it is also streamed to a Kinesis Firehose created in your AWS account. The Data Firehose stores the telemetry data in a S3 bucket in your AWS account.","This gives you full ownership of your data and enables you to use it outside the Baselime; for example to feed it into a data lake. It is also possible to rehydrate the data from the S3 bucket into Baselime once the data is past its expiration period on Baselime.","Data flow","We set the default TTL for objects stored in the bucket to 180 days to prevent extremely long storage of telemetry data you might not need; Feel free to adjust it to your needs."]},{"l":"How to use it","p":["First, you'll need to have Baselime CLI installed. You can find the installation instructions here.","Once you have it installed, you can use the following command to rehydrate your data:","Start date should be formatted in RFC3339 format, and hours to recover should be a number. The process will recover all the data from the start date, for the number of consecutive hours from that date."]}],[{"l":"Data Validation in Baselime","p":["Baselime has a size limit for events of 256kb. This size limit helps ensure that the ingestion process is efficient and that the data stored in Baselime is manageable and fast to query. If an event exceeds this 256kb size limit, it will not be ingested into Baselime."]},{"l":"Sending Semi-Structured Logs to Baselime","p":["Semi-structured logs are logs that are not in the strict JSON format, but still contain structured data that can be extracted.These logs contain a mixture of structured and unstructured data, making them difficult to parse and analyze. Fortunately, Baselime has built-in mechanisms to parse and extract relevant data from semi-structured logs.","Baselime will automatically detect log events that contain JSON data, but are prepended or appended by a generic string.","The generic string will be wrapped in a message attribute, and the JSON data will be wrapped in a data attribute. This enables you to extract and analyze relevant data from semi-structured logs."]},{"l":"Examples","p":["Here are examples of automatic semi-structured logs detection."]}],[{"l":"Baselime Service Discovery","p":["Baselime's Service Discovery enables you to automatically discover services in your AWS account, and organize logs, metrics, traces, and other telemetry data into services. This helps you to organize your telemetry data around services, create boundaries between services and teams, and make queries much faster.","This guide provides an overview of the Baselime Service Discovery, and how it can help you manage your serverless application more effectively."]},{"l":"Discovering Services","p":["Baselime automatically discovers Lambda functions, API Gateway endpoints, SQS queues, and all other serverless resources in your AWS account. Each resource is associated. The association is done through CloudFormation. All resource belonging to the same CloudFormation stack or group of stacks (if using the sst framework for example) will be grouped in the same service.","Baselime then groups the telemetry data generated by these resources into services.","You can view the list of discovered services in the Baselime dashboard, as shown in the screenshot below:","Services list in the Baselime console","Each service in the list provides an overview of the telemetry data that is available for that service, including logs, metrics, and traces. You can click on a service to view more detailed information about the service, such as the resources associated with the service, and query the telemetry data generated by those resources.","Services also enable you to organise your queries, alerts and dashboards within services."]},{"l":"Querying Telemetry Data","p":["Baselime's Service Discovery also makes querying much faster. Instead of searching through a large amount of data to find the information you need, you can query the data for a specific service. This accelerate your debugging and helps you identify issues and troubleshoot problems in your application."]},{"l":"Overriding the service discovery","p":["If you want to change the service a Cloud Formation stack creates then add a tag with the key baselime:service. This will take precedence over any other tags or the service name."]}],[{"l":"Baselime AI","p":["Baselime AI provides explanations for any chart, log, event, metric or trace on Baselime. It enables you to identify patterns and anomalies in your serverless systems, providing deeper insights into your system performance and behavior."]},{"l":"Getting Started","p":["You’ll need a Baselime account to start using Baselime AI. If you don't have one already, you can quickly sign up for a free trial on our website.","Once you have an account, follow these steps to use Baselime AI:","Navigate to the Baselime dashboard and select the chart, log, event, metric, or trace you want to analyze.","Click on the \"Ask AI\" button located next to the chart.","Wait for Baselime AI to process your query and provide a response.","Baselime AI explaining an error"]},{"l":"How it Works","p":["Baselime AI is built using OpenAI's Large Language Models. When you ask Baselime AI a question, it analyzes the data in the selected chart or event, and uses machine learning algorithms to identify any anomalies or patterns.","Once Baselime AI has identified potential issues, it generates a response that explains the root cause of the problem, as well as any recommended actions to fix it. The response is presented in natural language, enabling developers of all skill levels to understand."]},{"l":"Benefits","p":["Baselime AI offers a range of benefits, including:","Streamlined debugging: Baselime AI enables you to quickly identify and fix issues, reducing the time and effort required for debugging.","Improved system observability: With Baselime AI, you can gain deeper insights into your system performance and behavior, enabling you to optimize your systems and improve overall system observability.","Accessible to all skill levels: Baselime AI's natural language explanations enable developers at all skill levels to understand and interpret results."]},{"l":"Privacy","p":["Baselime is committed to protecting the privacy of our users' data. We understand the importance of keeping data secure and confidential, and we take appropriate measures to safeguard it.","When you use Baselime AI, your data is processed in accordance with our privacy policy. Before any data is sent to OpenAI, it is completely anonymized to protect the privacy of our users. This is done through a process called obfuscation, which replaces identifiable information with obfuscated tokens.","Once data has been anonymized, it is sent to OpenAI for processing. OpenAI is a trusted provider of AI services, and we have taken steps to ensure that your data is processed in accordance with our privacy policy and our high standards for data security.","Simplified Baselime AI architecture diagram","The diagram above shows a simplified overview of the data flow and privacy measures taken in Baselime AI. When you request an explanation for a chart, log, event, metric, or trace, the data is first processed and analyzed by Baselime AI. This analysis is done locally within our system to ensure that sensitive data is not transmitted. Only after data is anonymized through obfuscation is it sent to OpenAI for processing. OpenAI processes the data and returns an explanation to our system, which is then delivered to you.","We take data privacy seriously and are committed to ensuring that your data is protected at all times. If you have any questions or concerns about our privacy policy or data security, please don't hesitate to contact us."]}],[{"l":"Queries","p":["Queries are the primary way of interacting with your data in Baselime."]},{"l":"Queries in the Console","p":["You can run queries in the Baselime console by navigating to your service and clicking on the New Query button. This will bring up the Visual Query Editor. You can edit the query visually, but also switch to the embedded code editor to write your query using the Observability Reference Language (ORL).","To execute a query, click the Run Query button. The query results will be displayed in visually and in a table below the editor."]},{"l":"Queries in the CLI","p":["You can also run queries using the Baselime CLI. To do so, use the baselime query command.","Use the baselime query without any flags to enter interactive mode where you can specify all the arguments of your query interactively.","You can also run saved queries using the CLI, either in interactive mode or by passing the arguments as flags","You can also save your query results to a file. Use the --format to print the results of the query in JSON, and pipe them to a file.","For more advanced usage of the baselime query command, please refer to the CLI reference."]}],[{"l":"Alerts","p":["Baselime's alerting feature enables you to set up notifications for when certain conditions are met in your telemetry data. This can be helpful for detecting and responding to issues in your system in real-time."]},{"l":"Setting up alerts","p":["To set up an alert, you will need to specify a query and a threshold. When the result of the query meets the conditions the threshold, the alert will be triggered. You must also specify the frequency to check the query, and time window to consider for the alert.","You can set up alerts using the Baselime CLI with Observability as Code using the Observability Reference Language or the web console. Here is an example of how to set up an alert with ORL:","To create this alert, add it to a any .yml file in your .baselime folder. If you don't have a .baselime folder for your service, create it with baselime init.","Once you have the .baselime folder configured, run the following command to create your alert:"]},{"l":"Receiving alerts","p":["When an alert is triggered, you can choose to receive notifications through a variety of channels, such as email, Slack, or PagerDuty (coming soon)."]},{"l":"Tips for effective alerting","p":["Make sure to set appropriate thresholds for your alerts. Setting the threshold too low may result in false positives, while setting it too high may result in missed issues.","Keep alerts specific and actionable: Alerts should be specific and provide clear instructions on what action to take.","Set up alerts for the right things: Make sure to set up alerts for the most important issues that need immediate attention.","Use multiple alerting methods: Use a combination of Slack, email, and webhooks to ensure that you are notified of important issues in a timely manner.","Use alert suppression: Silence repeated alerts to avoid alert fatigue and ensure that you are only notified of important issues.","Consider using webhook alerts to build self-healing systems","Test your alerts to ensure they are working as expected.","Use alert analytics: Use alert analytics to analyze the effectiveness of your alerting strategy and make improvements where necessary.","Regularly review and update alert thresholds and configurations to ensure they are still relevant and effective."]}],[{"l":"Tailing your data","p":["The baselime tail command enables you to stream telemetry data in real time to your terminal. This can be useful for debugging or quickly checking the status of your services.","By default, the baselime tail command will stream all telemetry data for your Baselime environment. You can further filter the data by adding query parameters, such as:","This will only show the events where data.user.id is 123456 and the word error appears in the event.","You can also specify a time range for the data being streamed:","Alternatively, you can define the timerange in relative format","This will stream telemetry data between the specified start and end times.","The baselime tail command can be a useful tool for quickly checking the status of your application and identifying any issues that may be occurring."]}],[{"l":"Snapshots","p":["Snapshots enable you to capture the current state of all your alerts in a service at a given point in time. This can be useful for debugging purposes or for creating a record of the health of your system at a specific moment."]},{"l":"Using Snapshots","p":["To create a snapshot, run the baselime test command in your terminal. This will create a snapshot of the current state of all alerts in the current service, display the results in the terminal, and output them to a file in JSON format. You can specify the output file path using the --out-file flag.","This will create a snapshot of the alerts in the my-service service and save it to the snapshot.json file."]},{"i":"viewing-snapshots-coming-soon","l":"Viewing Snapshots (Coming soon)","p":["You can view your snapshots in the Baselime console under the \"Snapshots\" tab in the navigation menu. From here, you can view the details of each snapshot, including the time it was created, the service it was created for, and the state of each alert at that time."]},{"l":"Tips for Effective Snapshotting","p":["Use snapshots as a debugging tool to help you understand the state of your system at a specific moment in time","Use snapshots to compare the state of your alerts before and after making changes to your service.","Save snapshots for compliance purposes."]}],[{"l":"Reports","p":["Baselime reports allow you to monitor the health of your services and get notified when issues arise. Reports can be triggered on-demand, and can be sent to third party integrations such as Slack or Github."]},{"i":"when-to-use-reports-","l":"When to use reports ?","p":["Baselime reports are a powerful tool that enable your team to compare the state of a service before and after making changes. By incorporating Baselime reports into your CI/CD pipeline, your team can see the impact of their changes on alerts, dashboards, and SLOs in production. This not only improves the reliability of your deployments, but it also enables your team to build self-healing systems. For example, if a report after deployment is negative, your team can roll back or roll forward to ensure the stability of your service.","Here is an example of how you can use the baselime report command in a GitHub Action to compare the state of a service before and after a deployment:","This workflow will take a snapshot with baselime report github before and after running the deployment script ( npm run deploy). The report will be posted on the commit that triggered the workflow as a comment. It contains the current state of your service, including alerts, dashboards, and SLOs (coming soon). By comparing the two snapshots, you can see how the deployment affected your service and take appropriate action if needed."]},{"l":"Running a report","p":["To run a report, use the baselime report command. By default, this will create a snapshot of all the alerts in the current service, display the results in the terminal, and output them to a file.","To generate and publish a report, run the baselime report command followed by the name of the integration you want to publish the report to:","For example, to publish a report to GitHub, you would run:"]},{"i":"slos-and-dashboards-coming-soon","l":"SLOs and Dashboards (Coming Soon)","p":["In the future, the report command will also include support for publishing Service Level Objectives (SLOs) and creating dashboards to visualize your report data. Stay tuned for updates!"]}],[{"l":"Baselime CDK Quick Start","p":["Observability is a first class citizen of your infrastructure with Baselime. You can use the AWS CDK to define your observability configurations in Baselime."]},{"l":"Installation","p":["Download the Baselime CDK on npm:@baselime/cdk"]},{"l":"Configuration","p":["Initialise the Baselime CDK with your Baselime API Key."]},{"l":"Example alert","p":["Set up an alert everytime there's an error in your application logs:","This alert will notify you on Slack when there is an event with LogLevel equal ERROR in your telemetry data."]}],[{"l":"Baselime CDK Queries","p":["Queries are used to retrieve and analyze data from various datasets in order to gain insights from your services."]},{"l":"Sample Query Spec","p":["Here’s a sample query in Baselime CDK that uses all of the supported settings for defining queries in Baselime. Use it to get started creating your own queries."]},{"l":"properties","p":["Queries have a set of properties that define the query's characteristics and behavior."]},{"i":"description-optional","l":"description (optional)","p":["The description of the query is a string that provides more information about the query. It can include details about the data being queried, the calculations being performed, and any other relevant information.","Example:"]},{"l":"parameters","p":["The parameters of a query define the datasets to query, the calculations to perform on the data, and any filters or groupings to apply."]},{"i":"datasets-optional","l":"datasets (optional)","p":["The datasets parameter is an array of strings that specify the names of the datasets to query. Baselime supports querying multiple datasets simultaneously, allowing you to analyze data from different sources in a single query. If no datasets are provided, Baselime CDK defaults to lambda-logs.","Example:"]},{"i":"filters-optional","l":"filters (optional)","p":["eq: Equals","Example:","exists: Exists (applies to fields that may or may not exist in the data)","Filters can be used to narrow down the data being analyzed and focus on specific events or attributes.","gt: Greater than","gte: Greater than or equal to","inArray: In (applies to arrays only)","includes: Includes","lt: Less than","lte: Less than or equal to","Moreover, it is possible to add a filter to a query after the query has been initialised.","neq: Does not equal","notExists: Does not exist (applies to fields that may or may not exist in the data)","notInArray: Not in (applies to arrays only)","notIncludes: Does not include","regex: Matches a regular expression","startsWith: Starts with (applies to strings only)","The filters parameter is an array of strings that specify conditions to filter the data by. Baselime CDK provides multiple helper functions to create query filters:"]},{"i":"calculations-optional","l":"calculations (optional)","p":["The calculations parameter is an array of strings that specify the calculations to perform on the data. Baselime CDK provides multiple helper functions to create query calculations:","count: Counts the number of events.","countDistinct: Counts the number of distinct occurences of a field (applies to strings only).","max: returns the maximum value of a field.","min: returns the minimum value of a field.","sum: returns the sum of all values of a field.","avg: returns the average of all values of a field.","median: returns the median of all values of a field.","stdDev: returns the sample standard deviation of a field.","variance: returns the sample variance of a field.","p001, p01, p05, p10, p25, p75, p90, p95, p99, p999: return the specified percentile of all values of a field.","Calculations can be used to perform statistical analysis on the data and derive insights such as the average request duration, the maximum response size, or the 95th percentile of request latencies.","It is possible to pass an optional alias to each of these functions, such that the results are displayed in the Baselime console or CLI using the alias.","Example:"]},{"i":"groupby-optional","l":"groupBy (optional)","p":["The groupBy parameter is an object that specifies how to segment the data by a field. It has the following fields:","value: The field to group the data by","limit: The maximum number of results to return (default: 10)","type: The type of the data field to group by (string, boolean, or number)","orderBy: The calculation to order the results by (default: the first calculation in the query)","order: The order in which to return the results (ASC or DESC, default: DESC)","Grouping the data by a field allows you to segment the results into distinct groups and analyze them separately.","Example:"]},{"i":"needle-optional","l":"needle (optional)","p":["The needle parameter is an object that specifies a search to perform on the data. It has the following fields:","value: The string to search for","matchCase: A boolean indicating whether the search should be case-sensitive(default: false)","isRegex: A boolean indicating whether the search value is a regular expression (default: false)","The needle can be used to find specific set of events or patterns in the data.","Example:"]},{"l":"Adding an alert","p":["Baselime CDK enables you to add an alert to a query. The alert will run the query on a defined schedule and notify you on your preferred channels when specific conditions are met."]},{"l":"Example Queries","p":["Here are example Baselime CDK queries that combine all of the above properties.","This query retrieves data from the otel traces dataset and performs several calculations on the data. It computes the average request duration, maximum response size, and 95th percentile of request latencies for each user ID in the dataset.","It filters the data to only include user IDs with a request duration greater than 500ms, and limits the results to the top 100 user IDs based on the average request duration. The results are ordered by the average request duration in descending order. The query also searches for the word \"error\" in the data and filters the results based on whether or not the word is present.","This Baselime CDK query calculates the total consumed read capacity units for each DynamoDB table in a service. It filters the data to only include events with a metric_name of ConsumedReadCapacityUnits and a unit of Count, and groups the results by TableName. The query returns the top 10 tables with the highest consumed read capacity units."]}],[{"l":"Baselime CDK Alerts","p":["Alerts are used to run a query on a schedule and notify you if a threshold is crossed. Baselime alerts are based on Baselime queries, which gives you you the flexibility to specify alerts on defects or events of interest, and reduce false positives and alert fatigue."]},{"l":"Sample Alert Spec","p":["Here’s a sample alert in Baselime CDK that uses all of the supported settings for defining alert in Baselime. Use it to get started creating your own alert."]},{"l":"properties"},{"i":"description-optional","l":"description (optional)","p":["The description of the alert is a string that provides more information about the alert. It can include details about the conditions being monitored and any other relevant information.","Example:"]},{"i":"enabled-optional","l":"enabled (optional)","p":["The enabled property is a boolean that indicates whether the alert is enabled or disabled. If set to true, the alert will be active and trigger notifications when thresholds are met. If set to false, the alert will be inactive and no notifications will be sent.","Example:"]},{"l":"parameters","p":["The parameters of an alert define the query to run, the threshold to evaluate, and the frequency and window for monitoring. query"]},{"l":"query","p":["The query parameter specifies the query to run for monitoring. It can reference an existing query object or include an inline query definition.","Example:","With the inline query the calculation defaults to [calc.count()]","or"]},{"l":"threshold","p":["The threshold parameter specifies the condition to evaluate from the query results. It can use helper functions to create comparisons or calculations, such as gt, lt, eq, count, etc.","Example:"]},{"l":"frequency","p":["The frequency parameter specifies the frequency at which the alert should run the query and evaluate the threshold. It uses a string representation of the frequency, such as '5 mins', '1 hour', '1 day', etc.","Example:"]},{"l":"window","p":["The window parameter specifies the time window to look back for data when evaluating the threshold. It uses a string representation of the time window, such as '10 mins', '1 hour', '1 day', etc.","Example:"]},{"l":"channels","p":["The channels property specifies the destinations to send the alert notifications. It is an array of channel objects, where each object defines the channel type and targets."]},{"l":"type","p":["The type property specifies the type of channel for the alert. Baselime CDK supports various channel types, such as 'email', 'slack', 'webhook', etc."]},{"l":"targets","p":["The targets property specifies the target destinations for the alert notifications. The targets can be specific emails, channels, or URLs depending on the channel type.","Example:","Moreover, you can define a defaultChannel when initialising your Baselime CDK, this channel will be used for all alerts in the service, simplifying your CDK code."]}],[{"l":"Baselime CDK Dashboards","p":["Dashboards give you a birds eye view of a collection of your query results. This can help you look at multiple related graphs on a single page to spot interesting trends.","Dashboards are a collection of queries and charts that you want to keep for future reference. Boards help you visualise multiple queries at once, to spot interesting trends and share your findings with your team."]},{"l":"Sample Dashboard Spec","p":["Here’s a sample dashboard in Baselime CDK that uses all of the supported settings for defining dashboard in Baselime. Use it to get started creating your own dashboard."]},{"l":"properties"},{"i":"description-optional","l":"description (optional)","p":["The description of the dashboard is a string that provides more information about the dashboard. It can include high-level details or any other relevant information.","Example:"]},{"l":"parameters","p":["The parameters of a dashboard define the widgets to display on the dashboard."]},{"l":"widgets","p":["The widgets parameter is an array of widget objects that specify the queries to run and the names of the widgets to display on the dashboard. name and description are both optional parameters for a widget.","Example:"]}],[{"i":"observability-reference-language-orl","l":"Observability Reference Language (ORL)","p":["This is the documentation for Baselime's Observability as Code configurations using the Observability Reference Language (ORL).","ORL (Observability Reference Language) is a language used to express queries for observability telemetry data. ORL queries can be used to extract insights from logs, metrics, and traces data sources. ORL queries are defined by a set of parameters that specify the data sources, filters, and calculations to be performed on the data. The result of an ORL query is a set of events that match the criteria defined in the query, optionally aggregated by calculations.","ORL configurations are defined in YAML files.","Generally, ORL files live in the .baselime folder in the root directory of a given project. We refer to this folder as .baselime elsewhere in the documentation, although users can rename it at will.","Multiple integrations and connectors with your favourite Infrastructure as Code platforms are currently being developed."]},{"l":"Best Practice","p":["To streamline your Observability workflows, we recommend keeping your .baselime folder in Git alongside your source code. This enables you to sync and version control your queries, alerts and dashboards, and collaborate with other team members.","To pull the pregenerated queries and dashboards to your local machine, run baselime pull using the Baselime CLI. If the service has not been initialized locally, the CLI will prompt you to select the relevant service from a list of all your services. Once selected, Baselime will download all the queries, alerts, and dashboards for that service, enabling you to work with them locally."]}],[{"l":"ORL Services","p":["ORL (Observability Reference Language) services are used to organize and manage observability resources such as queries, alerts, and dashboards.","Note that the service must be defined in the madatory index.yml file in the .baselime folder."]},{"l":"Sample Service Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining a service in Baselime. Use it to get started creating your own services."]},{"l":"Properties","p":["Services have a set of properties that define the service's characteristics and behavior."]},{"i":"version-required","l":"version (required)","p":["The version property is a string that specifies the version of the Baselime CLI used to generate or deploy the service. It is used for version control and management.","Example:"]},{"i":"service-required","l":"service (required)","p":["The service property is a string that specifies the name of the service. It is used to identify the service and distinguish it from other services.","Example:"]},{"i":"description-optional","l":"description (optional)","p":["The description property is a string that provides more information about the service. It can include details about the purpose of the service, the components it includes, and any other relevant information.","Example:"]},{"i":"provider-required","l":"provider (required)","p":["The provider property is a string that specifies the cloud provider for the service. It is used to identify the provider and distinguish it from other providers. ORL supports the following providers:","aws","gcp(coming soon)","azure(coming soon)","cloudflare(coming soon)","vercel(coming soon)","Example:"]},{"i":"templates-optional","l":"templates (optional)","p":["The templates property is an array of strings that specifies the templates to automatically download and implement for the service. Templates are used to define observability rules that can be shared and reused across multiple services. Each string is in the format workspace/template, where workspace is the name of the workspace where the template was defined and template is the unique ID of the template.","Example:"]},{"i":"variables-optional","l":"variables (optional)","p":["The variables property is an object that enables you to define variables that can be used in the ORL queries and alerts within the service. These variables can be used to parameterize the ORL queries and alerts and make them more flexible and reusable.","Each variable has a name and one or more values. The values can be grouped by environment (e.g. prod, dev, etc.) or by any other criteria that makes sense for your service.","For example, you might define a threshold variable that has different values for different environments:","In this example, the threshold variable has a default value of 30, and different values for the prod and dev environments: 10 and 20, respectively.","To use this variable in an ORL query or alert, you can use the syntax:","In this example, the threshold variable will be replaced with the appropriate value depending on the environment in which the service is deployed.","It is important to note that variables are optional in services. If a variable is defined, it must have at least one value."]},{"l":"Example ORL Services","p":["Here are example ORL services that combine all of the above properties.","This ORL service is for a web application that is hosted on Amazon Web Services (AWS).","The cloud provider is AWS and the infrastructure consists of two CloudFormation stacks: webapp-stack and database-stack.","The service has two templates defined: baselime/lambda-logs-basics and workspace-name/template-name.","The service has two variables defined: threshold and frequency. The threshold variable has a default value of 30 and a value of 10 for the prod environment. The frequency variable has a default value of 30mins and a value of 5mins for the prod environment and a value of 0 9 ? * 2#1 * for the dev environment.","This ORL service is for a microservices architecture that is hosted on AWS."]}],[{"l":"ORL Queries","p":["ORL (Observability Reference Language) queries are used to retrieve and analyze data from various datasets in order to gain insights and improve observability of systems and services."]},{"l":"Sample Query Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining queries in Baselime. Use it to get started creating your own queries."]},{"l":"properties","p":["ORL queries have a set of properties that define the query's characteristics and behavior."]},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL query is a string that provides more information about the query. It can include details about the data being queried, the calculations being performed, and any other relevant information.","Example:"]},{"l":"parameters","p":["The parameters of an ORL query define the datasets to query, the calculations to perform on the data, and any filters or groupings to apply."]},{"l":"datasets","p":["The datasets parameter is an array of strings that specify the names of the datasets to query. ORL supports querying multiple datasets simultaneously, allowing you to analyze data from different sources in a single query.","Example:"]},{"i":"filters-optional","l":"filters (optional)","p":[": Greater than",": Less than","!=: Does not equal","=: Equals","=: Greater than or equal to","=: Less than or equal to","DOES_NOT_EXIST: Does not exist (applies to fields that may or may not exist in the data)","DOES_NOT_INCLUDE: Does not include","Example:","EXISTS: Exists (applies to fields that may or may not exist in the data)","Filters can be used to narrow down the data being analyzed and focus on specific events or attributes.","IN: In (applies to arrays only)","INCLUDES: Includes","MATCH_REGEX: Matches a regular expression","NOT_IN: Not in (applies to arrays only)","STARTS_WITH: Starts with (applies to strings only)","The filters parameter is an array of strings that specify conditions to filter the data by. Each string follows this format: 'key operation value', where key is the field to filter on, operation is the comparison operator to use, and value is the value to compare against. ORL supports the following operations:"]},{"i":"calculations-optional","l":"calculations (optional)","p":["The calculations parameter is an array of strings that specify the calculations to perform on the data. ORL supports the following calculations:","COUNT: Counts the number of events.","COUNT_DISTINCT: Counts the number of distinct occurences of a field (applies to strings only).","MAX: returns the maximum value of a field.","MIN: returns the minimum value of a field.","SUM: returns the sum of all values of a field.","AVG: returns the average of all values of a field.","MEDIAN: returns the median of all values of a field.","STDDEV: returns the sample standard deviation of a field.","VARIANCE: returns the sample variance of a field.","P001, P01, P05, P10, P25, P75, P90, P95, P99, P999: return the specified percentile of all values of a field.","Calculations can be used to perform statistical analysis on the data and derive insights such as the average request duration, the maximum response size, or the 95th percentile of request latencies.","Example:"]},{"i":"groupby-optional","l":"groupBy (optional)","p":["The groupBy parameter is an object that specifies how to segment the data by a field. It has the following fields:","value: The field to group the data by","limit: The maximum number of results to return (default: 10)","type: The type of the data field to group by (string, boolean, or number)","orderBy: The calculation to order the results by (default: the first calculation in the query)","order: The order in which to return the results (ASC or DESC, default: DESC)","Grouping the data by a field allows you to segment the results into distinct groups and analyze them separately.","Example:"]},{"i":"needle-optional","l":"needle (optional)","p":["The needle parameter is an object that specifies a search to perform on the data. It has the following fields:","value: The string to search for","matchCase: A boolean indicating whether the search should be case-sensitive (default: false)","isRegex: A boolean indicating whether the search value is a regular expression (default: false)","The needle can be used to find specific set of events or patterns in the data.","Example:"]},{"l":"Example ORL Queries","p":["Here are example ORL queries that combine all of the above properties.","This ORL query retrieves data from the otel traces dataset and performs several calculations on the data. It computes the average request duration, maximum response size, and 95th percentile of request latencies for each user ID in the dataset.","It filters the data to only include user IDs with a request duration greater than 500ms, and limits the results to the top 100 user IDs based on the average request duration. The results are ordered by the average request duration in descending order. The query also searches for the word \"error\" in the data and filters the results based on whether or not the word is present.","This ORL query calculates the total consumed read capacity units for each DynamoDB table in a service. It filters the data to only include events with a metric_name of ConsumedReadCapacityUnits and a unit of Count, and groups the results by TableName. The query returns the top 10 tables with the highest consumed read capacity units."]}],[{"l":"ORL Alerts","p":["ORL (Observability Reference Language) alerts are used to monitor data from various datasets and trigger notifications when specific conditions are met. ORL alerts are defined by a set of properties that specify the characteristics and behavior of the alert. They are based on ORL queries, which are used to retrieve and analyze the data. This allows you to monitor your systems and services and be notified when there are issues or anomalies that require attention.","Note that an alert can only be set for queries that include calculations. It is not possible to set an alert for a query that does not have any calculations."]},{"l":"Sample Alert Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining alerts in Baselime. Use it to get started creating your own alerts."]},{"l":"properties","p":["ORL alerts have a set of properties that define the alert's characteristics and behavior."]},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL alert is a string that provides more information about the alert. It can include details about the data being monitored, the conditions or thresholds being checked, and any other relevant information.","Example:"]},{"i":"enabled-optional","l":"enabled (optional)","p":["The enabled property is a boolean that specifies whether the ORL alert is currently active or inactive. If set to true, the alert will be triggered when the conditions or thresholds are met. If set to false, the alert will be disabled and will not trigger."]},{"l":"parameters","p":["The parameters of an ORL alert define the query to use, the frequency at which the query is run, the window of time over which the query's results are analyzed, and the threshold or condition that triggers the alert."]},{"l":"query","p":["The query parameter is a reference to an ORL query that defines the data to be monitored for the alert. It is specified as a string in the format !ref query_id, where query_id is the id of the ORL query.","Example:"]},{"l":"frequency","p":["The frequency parameter is a string that specifies how often the alert is checked. It can follows the format number time_unit, where number is a positive integer and time_unit is one of the following:","mins/ minutes: minutes","h/ hours: hours","d/ days: days","months: months","y/ years: years","The frequency can also be defined as a cron expression, following the AWS Cron Reference","Examples:","15 10 * * ? *: 10:15 AM (UTC) every day 0 18 ? * MON-FRI *: 6:00 PM Monday through Friday 0 8 1 * ? *: 8:00 AM on the first day of the month 0/10 * ? * MON-FRI *: Every 10 min on weekdays 0/5 8-17 ? * MON-FRI *: Every 5 minutes between 8:00 AM and 5:55 PM weekdays 0 9 ? * 2#1 *: 9:00 AM on the first Monday of each month","The alert is checked at the specified interval, and if the conditions are met, the alert is triggered.","Example:"]},{"l":"window","p":["The window parameter is a string that specifies the time window to consider for the alert. It follows the same format as the frequency parameter, but cannot be defined as a CRON expression.","The alert is only triggered if the conditions are met within the specified time window.","Example:"]},{"l":"threshold","p":["The threshold parameter is a string that specifies the threshold at which the alert is triggered. It is a value that inculdes the comparison and the value (e.g. 5).","The threshold is compared to the result of the first calculation in the query of the alert. If the result meets the specified condition, the alert is triggered.","The following comparison operators are supported:","=: Equals","!=: Does not equal",": Greater than","=: Greater than or equal to",": Less than","=: Less than or equal to","Example:"]},{"l":"channels","p":["The channels parameter is an array of objects that specify the channels to send the alert to. ORL supports the following types of channels:","slack: Sends the alert to a Slack channel email: Sends the alert to an email address pagerduty: Triggers a PagerDuty incident webhook: Sends the alert to a custom webhook URL","Each channel type has its own set of properties that define the behavior of the channel."]},{"l":"slack","p":["The slack channel type sends the alert to a Slack channel. It has the following properties:","targets: An array of strings that specify the Slack channels to send the alert to. Each string should be the name of a Slack channel (e.g. general). Example:","Note that it is necessary to install the Baselime Slack app and follow the Slack onboarding to get alerts on Slack."]},{"l":"email","p":["The email channel type sends the alert to an email address. It has the following properties:","targets: An array of strings that specify the email addresses to send the alert to. Each string should be a valid email address.","Example:"]},{"i":"pagerduty-coming-soon","l":"pagerduty [Coming Soon]","p":["The pagerduty channel type triggers a PagerDuty incident. It has the following properties:","serviceKey: A string that specifies the PagerDuty service key to use for the incident. This key is used to identify the PagerDuty service that the incident should be created in. eventAction: A string that specifies the action to take when creating the PagerDuty incident. Valid values are trigger (default) and resolve. client: A string that specifies the name of the client that the incident should be associated with. This is optional and can be used to provide context for the incident. clientUrl: A string that specifies the URL of the client that the incident should be associated with. This is optional and can be used to provide context for the incident.","Example:"]},{"l":"webhook","p":["The webhook channel type sends the alert to a custom webhook URL. It has the following properties:","url: A string that specifies the URL to send the alert to. method: A string that specifies the HTTP method to use when sending the alert. Valid values are POST (default) and GET. headers: An object that specifies the headers to include in the request. body: A string or object that specifies the body of the request. If a string is provided, it will be sent as-is. If an object is provided, it will be serialized as JSON and sent as the request body. (Coming Soon)","Example:"]},{"l":"Example ORL Alerts","p":["Here are example ORL alerts that combine all of the above properties."]},{"l":"DynamoDB ConsumedWriteCapacityUnits Alert","p":["This alert is triggered when the ConsumedWriteCapacityUnits metric for a DynamoDB table exceeds a specified threshold over a specified time window.","The alert is set to run every 15 minutes and check the metric over the past hour.","If the ConsumedWriteCapacityUnits exceed 5 over the past hour, the alert is triggered.","The alert is sent to a Slack channel called #dynamodb-alerts."]},{"l":"Lambda Timeout Alarm","p":["This alert checks the number of invocations that have timed out for Lambda functions in the service, and triggers if the count exceeds 10 over the past 15 minutes. It sends a notification to a custom webhook URL every 5 minutes."]}],[{"l":"ORL Dashboards","p":["ORL (Observability Reference Language) dashboards are used to visualise data."]},{"l":"Sample Dashboard Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining dashboards in Baselime. Use it to get started creating your own dashboards."]},{"l":"properties"},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL dashboard is a string that provides more information about the dashboard.","Example:"]},{"l":"parameters","p":["The parameters of an ORL dashboard define the widgets to display in the dashboard."]},{"l":"widgets","p":["A widget is a graphical element that displays data in a compact and user-friendly way. It can be customized and configured to show specific the results of a specific query in a given view."]},{"l":"query","p":["The query parameter is a reference to an ORL query that defines the data to be displayed in the widget. It is specified as a string in the format !ref query_id, where query_id is the id of the ORL query.","Example:"]},{"l":"view","p":["In ORL, there are three types of widget views:","calculations: This view presents your data as line charts, enabling you to quickly calculate key performance metrics like averages, sums, and counts.","events: This view allows you to explore individual events by filtering and searching, making it a valuable tool for investigating specific occurrences and trends.","traces: This view provides a scatter plot of distributed traces, giving you insights into bottlenecks and latency issues. It's perfect for investigating specific requests or flows and optimizing performance.","Example:"]},{"l":"Example ORL Dashboard","p":["Here is an example ORL dashboard that combine all of the above properties."]}],[{"l":"Installing the Baselime CLI","p":["The Baselime CLI is the primary way to interact with Baselime and your serverless observability data. It allows you to connect your serverless applications, query and explore your data, and set up integrations with your tools.","You can install the Baselime CLI using one of the following methods:"]},{"l":"Installing"},{"i":"installing-with-homebrew-for-macos","l":"Installing with Homebrew (for MacOS)","p":["Make sure you have Homebrew installed on your system. If you don't, you can install it by following the instructions here.","Run the following commands to add the Baselime tap to your Homebrew installation:"]},{"i":"installing-with-curl-for-macos-and-linux","l":"Installing with curl (for MacOS and Linux)","p":["Run the following command to download and install the Baselime CLI:","MacOS","Linux"]},{"i":"installing-with-npm-for-macos-linux-and-windows","l":"Installing with npm (for MacOS, Linux, and Windows)","p":["Make sure you have npm installed on your system. If you don't, you can install it by following the instructions here.","Run the following command to install the Baselime CLI:"]},{"i":"downloading-the-binary-for-macos-and-linux","l":"Downloading the binary (for MacOS, and Linux)","p":["You can download the latest version of the Baselime CLI binary from the releases page on GitHub.","Download the binary for your operating system and architecture (e.g., baselime_linux_x64 or baselime_darwin_x64).","Unzip the tarball with tar -xf baselime-os-arch-version.tar.gz","Make the binary executable with chmod +x baselime.","Move the binary to a directory in your PATH, such as /usr/local/bin, with mv baselime /usr/local/bin/baselime.","On some systems, you might need to run these commands with sudo."]},{"l":"Verifying the installation","p":["To verify that the Baselime CLI has been installed correctly, run the following command:","You should see the version number of the Baselime CLI that you installed.","If you encounter any issues during the installation process, please don't hesitate to contact us."]},{"l":"Authenticating the CLI","p":["Before you can use the Baselime CLI, you must authenticate it with your Baselime account. To do this, run the following command:","This command opens a new browser window and asks you to sign in to your Baselime account. Once you sign in, the CLI is authenticated and you can start using it to interact with your Baselime account."]},{"l":"Updating the Baselime CLI","p":["To update the Baselime CLI to the latest version, use one of the following commands depending on how you originally installed it:","If you installed with brew, run brew upgrade @baselime/cli","If you installed with curl, run baselime upgrade","If you installed with npm, run npm update -g @baselime/cli"]}],[{"l":"Getting Started with the Baselime CLI","p":["Welcome to the Baselime CLI! This guide will help you get up and running with the CLI so you can start using Baselime to gain visibility into your serverless architecture."]},{"l":"Prerequisites","p":["Before you can use the Baselime CLI, you'll need to:","Install the Baselime CLI. See the installation instructions for more details.","Connect your AWS Account to Baselime. See the quick start guide for instructions on how to do this."]},{"l":"First Steps","p":["Once you have the Baselime CLI installed and your AWS Account connected, you're ready to start using Baselime! Here are a few commands to get you started:","baselime iam: displays information about the current user logged in to the CLI.","baselime query: Run a query against your telemetry data to find specific events or metrics.","baselime tail: Stream all of the events ingested into Baselime in real-time."]},{"l":"Next Steps","p":["Now that you've gotten your feet wet with the Baselime CLI, you can learn more about the other commands and features available. Here are a few places to start:","Check out the CLI reference for a full list of available commands and their options."]}],[{"l":"Anonymous Telemetry","p":["Baselime collects completely anonymous telemetry data about general CLI usage. Participation in this anonymous program is optional, and you can opt-out if you'd not like to share any information."]},{"i":"how-do-i-opt-out","l":"How do I opt-out?","p":["You can opt out-by running the following command:","You can re-enable telemetry if you'd like to rejoin the program by running."]},{"i":"why-do-we-collect-telemetry-data","l":"Why do we collect telemetry data?","p":["Telemetry data help up to accurately measure the Baselime CLI feature usage, pain points, and customisation across all developers. This data empowers us to build a better product for more developers.","It also allows us to verify if the improvements we make to the Baselime CLI are having a positive impact on the developer experience."]},{"i":"what-is-being-collected","l":"What is being collected?","p":["We measure the following anonymously:","Command invoked (ie. baselime deploy, baselime query, or baselime tail)","Version of Baselime in use","General machine information (e.g. number of CPUs, macOS/Windows/Linux, whether or not the command was run within CI)","An example telemetry event looks like:","These events are then sent to an endpoint hosted on our side."]},{"i":"what-about-sensitive-data-or-secrets","l":"What about sensitive data or secrets?","p":["We do not collect any metrics which may contain sensitive data.","This includes, but is not limited to: environment variables, file paths, contents of files, logs, or serialized errors."]},{"i":"will-the-telemetry-data-be-shared","l":"Will the telemetry data be shared?","p":["The data we collect is completely anonymous, not traceable to the source, and only meaningful in aggregate form.","No data we collect is personally identifiable.","In the future, we plan to share relevant data with the community through public dashboards or reports."]}],[{"l":"baselime connect","p":["Use the baselime connect command to connect your AWS account to Baselime."]}],[{"l":"baselime console","p":["Use the baselime console command to open the Baselime console."]}],[{"l":"baselime deploy","p":["Use the baselime deploy command to deploy your Observability as Code configurations from your local folder to your Baselime account."]}],[{"l":"baselime iam","p":["Use the baselime iam command to display the currently logged-in user and environment."]}],[{"l":"baselime init","p":["Use the baselime init command to initialize a new service in the current directory."]}],[{"l":"baselime login","p":["Use the baselime login command to log in your Baselime account and select an environment."]}],[{"l":"baselime logout","p":["Use the baselime logout command to log out of Baselime."]}],[{"l":"baselime mark","p":["Use the baselime mark command to create a marker."]}],[{"l":"baselime pull","p":["Use the baselime pull command to update local observability as code configurations with the latest state from the remote systems. If the service has not been initialised locally yet then the cli will prompt you to select a service and download everything to your machine."]}],[{"l":"baselime query","p":["Use the baselime query command to run a query on your telemetry data data."]}],[{"l":"baselime rehydrate","p":["Use the baselime rehydrate to rehydrate Baselime hot storage with data from your Amazon S3 Bucket."]}],[{"l":"baselime report","p":["Use the baselime report command to generate a report based on your observability data and assess the health and performance of your service."]}],[{"l":"baselime tail","p":["Use the baselime tail command to stream events from your telemetry data in real-time."]}],[{"l":"baselime telemetry","p":["Use the baselime telemetry command to manage the usage telemetry data collected by the Baselime CLI."]}],[{"l":"baselime templates","p":["Use the baselime templates command to manage your observability templates."]}],[{"l":"baselime test","p":["Use the baselime test command to check all the alerts in your current service, display the results in the terminal, and output them to a file."]}],[{"l":"baselime upgrade","p":["Use the baselime upgrade command to upgrade the Baselime CLI to the latest version. This method will work only if you installed the Baselime CLI with curl -s https://get.baselime.io | bash."]}],[{"l":"baselime validate","p":["Use the baselime validate command to validate your ORL configuration files."]}],[{"l":"Webhook Integration","p":["The Webhook integration enables you and your team to send POST requests to an http endpoint when an alert is triggered.","To set this up, set [channel].properties.type to webhook and a valid URL in the targets array.","When an alert triggers to a webhook channel, HTTP requests are made to the channel targets using the /POST method. Each request carries an event similar to the example outlines below."]}],[{"i":"baselime--slack-integration","l":"Baselime + Slack Integration","p":["The Baselime integration for Slack gives you and your team full visibility into your applications right in Slack channels, where you can get alerted, investigate incidents, and manage your observability, as a team."]}],[{"l":"Authentication","p":["Install the Baselime integration for Slack.","Once you install the app in your Slack workspace, you can start interacting with Baselime app as a Personal app or access from channels. By default, the Baselime app is enabled in all the public channels. For private channels, you need to explicitly invite /invite @baselime","At this point, your Slack and Baselime user accounts are not linked. You will be prompted to log in Baselime. This is a primary step required to access the app.","Slack welcome message","The primary button will redirect you to the Baselime console where you can login and connect your Slack.","Once this is completed, you will be greeted with a help message \uD83C\uDF89.","Successful login"]}],[{"l":"Automated Alerts","p":["You can set one or multiple Slack channels to receive automated alerts.","Please make sure that the channels defined either are public, or you have manually added the Baselime Slack app to those.","Once configured, When an alert that is configured to send notifications to Slack is triggered, the Baselime Slack app will notify all the configured channels.","Slack alert"]}],[{"l":"Commands","p":["You can use the /baselime command to interact with Baselime straight from Slack."]},{"l":"queries"},{"l":"run","p":["Run a query.","Options","--application: Name of the application","--ref: Query reference","--from: UTC start time - may also be relative eg: 1h, 20mins","--to: UTC end time - may also be relative eg: 1h, 20mins, now","--id: Query id","Result","Slack queries run result"]},{"l":"list","p":["[Coming Soon]"]},{"l":"help","p":["Displays help."]}],[{"l":"Data Security","p":["Baselime is committed to ensuring the security and privacy of our users' data. We have implemented a number of measures to ensure that data is encrypted in transit and at rest, and that it is not accessible from the public internet. Here are some of the key data security features of Baselime:"]},{"l":"Data Encryption","p":["All data transferred to and from Baselime is encrypted in transit using industry-standard protocols such as HTTPS and TLS. In addition, all data is encrypted at rest."]},{"l":"Private VPCs and IAM Roles","p":["Baselime runs in private Virtual Private Clouds (VPCs) and utilizes IAM roles to ensure that data is only accessed by authorized users and processes."]},{"l":"No Public Access","p":["Baselime does not expose any data to the public internet. All data is accessed via secure, authenticated channels."]},{"l":"Modern Best Practices","p":["Baselime follows modern best practices for data security, including regularly updating and patching our systems, implementing network segmentation and access controls, and conducting regular security audits and penetration testing."]},{"l":"Data Scrubbing and Obfuscation","p":["Baselime provides tools for scrubbing and obfuscating sensitive data, such as passwords, secrets, and API keys. Users can block or obfuscate specific keys by dataset using the .baselimeignore file. In addition, Baselime automatically scrubs a predefined list of sensitive fields, including \"password\" and \"secret\".","To learn more about how to use these features to protect your data, see the Baselime Telemetry Data Privacy documentation."]},{"l":"Compliance","p":["We're currently working towards compliance with a number of industry-standard security and privacy frameworks, including GDPR, SOC2 and HIPAA. Please contact us for more information on our compliance status."]},{"l":"Support","p":["If you have any questions or concerns about the security of your data in Baselime, please don't hesitate to contact our support team. We are always here to help!"]}],[{"l":"Telemetry Data Privacy","p":["Baselime is designed to help you observe the health and performance of your applications, and part of that involves collecting telemetry data. To ensure the privacy of your data, Baselime provides a number of features that enable you to control which data is collected and how it is used."]},{"l":"Obfuscating Keys","p":["Baselime enables you to obfuscate certain keys from being ingested into your datasets. This is particularly useful for sensitive information such as passwords, API keys, and other personal data. You can obfuscate keys for a specific dataset by using the baselime obfuscate-key command:","You can also obfuscate keys for multiple datasets at once by specifying the --dataset flag multiple times:","In addition to the command-line interface, you can also use a .baselimeignore file to block keys. The .baselimeignore file should be located in the root of your repository and should contain a list of keys to block, one per line, with the associated dataset. For example:","Moreover, you can obfuscate keys using the Baselime console, in the datasets section.","Keep in mind that obfuscating keys is a one-way process, meaning that once a key has been obfuscated, there is no way to recover the original value. Make sure to carefully consider which keys you want to obfuscate."]},{"i":"baselimeignore-coming-soon","l":".baselimeignore [Coming soon]","p":["The .baselimeignore file allows you to specify keys that should be obfuscated when data is ingested into Baselime. You can use this file to block or obfuscate multiple keys across multiple datasets.","To obfuscate a key, add a line to the .baselimeignore file in the following format:","For example, to obfuscate the data.user.email key in the lambda-logs dataset, you would add the following line to your .baselimeignore file:","Note that the .baselimeignore file should be placed in the root folder of your service and will be applied when you run baselime deploy.","Keep in mind that the .baselimeignore file is only applied to data that is ingested after the .baselimeignore file is pushed. Data that was ingested before the .baselimeignore file was pushed will not be affected."]},{"l":"Automatic scrubbing","p":["access_token","Any nested field in your telemetry data that contains any of these automatically scrubbed keys will be blocked from ingestion by default.","api_key","apikey","auth","Baselime that automatically obfuscate sensitive information from being ingested into the telemetry data by default. This is done to ensure that sensitive data is not accidentally exposed.","credentials","creds","Or using the Baselime console, in the datasets section.","passwd","password","pwd","secret","sourceip","The following keys are automatically scrubbed:","To turn automatic scrubbing on or off for a specific dataset, use the following commands [Coming soon]:"]}],[{"l":"Connectors","p":["Baselime uses connctors to automatically ingest telemetry data from your cloud environments."]}],[{"l":"AWS Connector on Baselime","p":["The AWS Connector allows you to send data from your AWS resources to Baselime. This includes logs, traces, and metrics. By connecting your AWS account to Baselime, you can get a unified view of your serverless architecture, query your data, and set up alerts."]},{"l":"Setting up the AWS Connector","p":["The connector is an automated flow based on a CloudFormation template.","It can be done using the Baselime CLI or through the web console."]},{"l":"Using the CLI","p":["To connect a cloud account to Baselime using the CLI, run the following command in your terminal","Once you've followed the interactive steps, the CLI will generate a CloudFormation template for you to deploy on your AWS account. FOllow the link in your terminal to deploy the temple on your AWS account.","Once deployed, login in your newly connected environment from the CLI.","The interactive prompt should list your newly connected environment.","Within minutes you should get telemetry data flowing through with the command"]},{"l":"Using the Web Console","p":["Navigate to the Baselime web console and login.","Follow the steps on the homescreen to connect a new AWS Account. Baselime will generate a CloudFormation template for you to deploy on your AWS account.","Once the template is deployed on AWS, return to the Baselime web console and refresh the page. You should see the newly connected AWS environment in the list of connected environment.","Within minutes telemetry data from your AWS environment should start displaying in the events streams in the Baselime web console."]},{"l":"Troubleshooting","p":["If you encounter any issues or error when connecting your AWS environment, please don't hesitate to contact us, or join our Slack community where we are always available to support."]},{"l":"CloudFormation Template","p":["The CloudFormation template is open-source and available here."]},{"l":"Your data","p":["Once connected, Baselime will automatically ingest data from your AWS environment. This includes:","Lambda Logs","API Gateway Logs","Cloudtrail Logs","Cloudwatch Metrics","ECS Logs (through fluentd)","Open Telemetry Metrics","X-Ray Traces","Once ingested, the telemetry data is streamed through a Kinesis Firehose to an Amazon S3 bucket in your AWS account for cold storage. There you can access the raw data and use it for your own purposes.","The default retention period of the telemetry data in your bucket is set to 180 days by default."]}],[{"l":"Baselime CDK","p":["Baselime natively integrates with any CDK or SST application to let you set up your applications observability along side your existing infrastructure. By defining your Observability resources along side your application using the CDK you get some awesome productivity benefits like being able to reuse the types and resources created by your IAC directly in your observability code.","These queries can then be used within your alerts and dashboards to paint a complete picture of how your application is performing"]},{"l":"Usage"},{"l":"Setting up the Baselime CDK","p":["The @baselime/cdk package should be installed in the same folder as your cdk application.","Next and store your api key in ssm. You can find your api key in CLI when running","or download it from console.baselime.io","Get your API Key from the Console","Then add the parameter to SSM","If you deploy your application to multiple aws accounts and multiple Baselime environments make sure you add a baselime api key per environment and prefix with the correct stage variable","Finally you can initialise @baselime/cdk in your CDK stack."]},{"l":"Instrumenting your application","p":["Here we have an lambda function that creates a subscription. This is a critical flow within the application. We need to know about any problems asap but also the business metrics that it produces can tell us about harder to detect issues in other parts of the system. Using @baselime/cdk we are going to create a comprehensive set of alerts and a dashboard that gives us insight into the business metrics and performance of our application.","In this lambda function we have added structured json logs to each critical path of this application that give us context of what happens. The lambda runtime will also emit START, END, and REPORT logs that we can use to understand the performance of the application."]},{"l":"Catching Errors","p":["The first thing we want to do is set up alerts for any errors in our new lambda function. Our billing team want to be informed about any subscription related errors separately. This can be done by putting a custom target in the channel once the slack integration is set up","This adds a query and alert to your applications service in Baselime that notify you in slack when ever any log messages contain an error or Unhandled Exceptions caught by the lambda runtime. The alerts will check every 30 minutes for any errors.","The billing team come back and explain that they want to see the errors broken down by customer so they can see which customers where effected by the broken code.","This now shows us exactly the customers that where effected by the outage."]},{"l":"Business Metrics","p":["O11y is not just for code errors. It's also about painting a richer picture of your application. Imagine the scenario where your company doesn't start any new subscriptions in a day. This is an example of where having sensible alerts and dashboards for your system metrics can spot issues in your whole application. i.e. maybe the new marketing campaign emails failed or your signup page has a glitch and the submit button has been set to display:hidden;. It's hard to write tests for every possibility but having alerts on key business metrics can give you useful feedback where tests cannot.","To do this we are going to set up a query that tracks the amount of revenue we are taking per hour.","We can then use this query in dashboards and alerts to show the performance of our business.","An alert that warns us if the subscriptions fall bellow the expected level could warn us of a wide range of problems so we are going to set that up like this f"]},{"l":"Conclusion","p":["Baselime CDK is a useful tool to test in prod. It can help catch issues as they happen so you can take effective corrective action, setting it up in your CDK stack is super effective because its now front of mind when designing the infrastructure for your service."]}],[{"i":"materialized-keys-in-baselime-coming-soon","l":"Materialized Keys in Baselime [Coming Soon]","p":["Materialized keys in Baselime allow you to calculate and create new keys from existing keys in your events. These new keys are based on calculations performed on one or multiple existing keys, which must be of type number.","To create a materialized key, you can use the baselime keys materialize command and specify the key name and calculation you want to perform. For example","This will create a new materialized key called total_revenue which is the result of multiplying the price and quantity keys in your events in the dataset lambda-logs.","Once you have created a materialized key, it will automatically be included in all your events going forward. You can view and manage your materialized keys in the Baselime console.","It's important to note that materialized keys are only recalculated when a new event is ingested, so any changes to the calculation or the underlying keys will not be reflected in historical data."]},{"l":"Syntax","p":["Materialized keys are calculated keys that are generated based on one or multiple existing keys in your events. Materialized keys can only be generated from existing keys of type number.","Materialized keys can only take existing key s of type number.","In Baselime, you can create Materialized Keys through the following operations:"]},{"l":"Basic arithmetic operations","p":["Addition: +","Subtraction: -","Multiplication: *","Division: /"]},{"l":"Advanced operations","p":["Modulo: %","Exponentiation: ^"]},{"l":"Trigonometric operations","p":["Sine: sin","Cosine: cos","Tangent: tan","Inverse Sine: asin","Inverse Cosine: acos","Inverse Tangent: atan"]},{"l":"Creating materialized keys","p":["To create a Materialized Key, you can use the baselime keys materialize command and specify the operation you want to perform on the existing keys. For example:","This command will create a Materialized Key called materialized_key in the dataset lambda-logs that is the result of adding key1 and key2.","You can also use multiple operations and keys to create more complex Materialized Keys. For example:","This command will create a Materialized Key called materialized_key that is the result of adding key1 and key2, and then multiplying that result with key3.","Once you have created a Materialized Key, you can use it just like any other key in your queries and alert rules."]},{"l":"Examples","p":["Here are some examples of how you can use materialized keys in your events:","Calculate the average response time for an API by dividing the total response time by the number of requests","Calculate the total revenue for an e-commerce store by multiplying the price and quantity keys for each order","Calculate the conversion rate for a marketing campaign by dividing the number of conversions by the number of impressions","You can use these materialized keys to set up alerts, create dashboards, and run queries to gain deeper insights into your data."]},{"l":"Troubleshooting","p":["If you encounter any issues with materialized keys, here are a few things you can try:","Check the syntax of your calculation to make sure it is correct","Make sure that all the keys used in your calculation exist in your events and are of type number","If you are using multiple keys, make sure they are all present in every event","If you are still experiencing issues, you can contact the Baselime support team for help."]}]]
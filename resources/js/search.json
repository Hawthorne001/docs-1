[[{"l":"Baselime Documentation","p":["To get started, read the quick start guide.","This documentation highlights what Baselime is, how to get it working with your stack and how it can help you discover, investigate, and resolve issues faster.","Quick Start Baselime Documentation Analyzing Data in Baselime Baselime CLI Observability as Code Data Security"]},{"l":"Support","p":["Join the Baselime Slack Community if you have any questions, or you want to learn and discuss ideas around observability practices."]}],[{"i":"what-is-baselime","l":"What is Baselime?","p":["Baselime is an observability solution built for modern cloud-native environments. It combines logs, metrics, and distributed traces to give you full visibility across your microservices at scale.","Baselime Console","Baselime makes it easy to observe your cloud services, containers orchestrators (AWS ECS), and serverless functions. Monitor everything, from latencies to business domain metrics derived from your logs and distributed traces."]},{"l":"Our mission","p":["Our mission is to simplify the complexity of distributed cloud-native systems. We make observability easy for you such that you can focus on what truly matters: building better products."]},{"i":"how-is-baselime-different","l":"How is Baselime different?","p":["Traditional monitoring solutions were built for a world where most applications were single monolithic applications. These solutions lack when it comes to microservices in 3 major ways:","Data volume and cardinality: Microservices generate a higher amounts of data with higher cardinality. The cost of traditional solutions scales exponentially with higher cardinality data.","Query speed: Traditional solutions are typically based on slower data stores, and queries are fast only when your dataset is small.","Distributed tracing: Microservices handle distributed transactions across multiple services and serverless functions. Traditional solutions based on logs struggle to capture the end-to-end transactions.","Baselime directly connects to your cloud account and uses OpenTelemetry to automatically instrument your microservices.","Do you want to learn more about OpenTelemetry? Start here.","Baselime uses our proprietary query engine built on top of ClickHouse, the fastest columnar database in the world. Baselime indexes all your telemetry data, regardless of the cardinality and dimensionality. From day 0, everything is queriable and searchable, and correlation between data sources is a breeze.","Baselime does not perform any pre-aggregation of data before ingestion; this means you can run arbitrary queries on you telemetry data, and get answers about the state of your application, regardless of how unusual or unique this state is.","Baselime in your ecosystem","Moreover, Baselime gives your control over the residency of your data. Either using our backend or a Bring Your Own Backend solution where all the data is stored on your cloud account."]},{"i":"why-baselime","l":"Why Baselime?"},{"l":"Reduce downtime","p":["Troubleshoot infrastructure and application issues with high cardinality data and a fast query enginer."]},{"i":"search-anything-anywhere-its-all-indexed","l":"Search anything, anywhere. It's all indexed","p":["Query against any nested field and automatically surface anomalies fast; regardless of how unusual or unique this state of your application is."]},{"l":"Take control of your data and costs","p":["Use or backend or Bring Your Own Backend. Up to 6x more value than incumbents. No per-function pricing, no per-seat pricing, no per-alert pricing. Start at $0 and scale up as your applications grow, with no hidden fees."]}],[{"l":"FAQ"},{"i":"what-is-baselime","l":"What is Baselime?","p":["Baselime is an observability solution that makes observability for cloud-native microservices easy. Baselime covers your logs, metrics, traces in a single solution. Baselime is built on top of ClickHouse, the fastest columnar database in the world."]},{"i":"how-much-does-it-cost","l":"How much does it cost?","p":["Baselime pricing is based on the number of events your systems produce. This scales linearly with the traffic your applications handle. Moreover, Baselime has a full free tier for up to 4M events per month.","Check out our pricing page for more details."]},{"i":"how-does-baselime-count-traces-for-billing","l":"How does Baselime count traces for billing?","p":["Distributed traces are counted using the number of unique trace IDs received from your services. If a trace ID is not found, Baselime counts the unique request ID associated with your transactions.","If a trace goes through multiple request ID, each request is counted individually, and the trace is not counted. As such you are not charged twice for the same requests.","If Baselime receives a trace with a unique trace ID, but no request matching this trace ID, the trace is counted towards your monthly number of traces.","Baselime counts the number of traces daily and updates your dashboard accordingly."]},{"i":"does-baselime-support-containers","l":"Does Baselime support containers?","p":["Baselime works with any environment where OpenTelemetry is available. Moreover, Baselime provides an HTTP API where you can send events individually from environments where OpenTelemetry is not available.","Baselime has a native integration with both serverless and container platforms on AWS:","AWS Lambda","Amazon ECS (Fargate and EC2)","Amazon AppRunner"]},{"i":"does-baselime-support-multi-accounts-and-multi-regions","l":"Does Baselime support multi-accounts and multi-regions?","p":["Yes, Baselime supports for multi-account and multi-region setups. When you connect your first cloud account to Baselime, Baselime creats a Baselime environment. You can subsequently add as many new cloud accounts or regions to the Baselime environment. All your telemetry data from those separate accounts and regions will be unified in the Baselime environment."]},{"i":"how-easy-is-it-to-instrument-my-applications","l":"How easy is it to instrument my applications?","p":["When you connect your AWS account to Baselime, logs from your AWS Lambda functions, API Gateways and AppRunner services, and metrics from your entire AWS account are automatically ingested into Baselime. No further setup is required.","Moreover, if you have Amazon X-Ray enabled on your services (both serverless functions and containers), these traces are automatically ingested into Baselime.","To use OpenTelemetry distributed tracing, add the baselime:tracing tag to your AWS Lambda functions using the Node.js runtime and these will be automatically instrumented. We're currently working on more runtimes.","For any other runtimes or environments, instrument your applications with OpenTelemetry or send your logs via the HTTP API."]},{"i":"how-do-i-get-distributed-tracing","l":"How do I get distributed tracing?","p":["Baselime supports both OpenTelemetry and AWS X-Ray for distributed tracing. If you application is already instrumented with OpenTelemetry, change the destination of your instrumetation to the Baselime endpoint:","URL: https://otel.baselime.io/v1","Header: x-api-key: BASELIME_API_KEY","Alternatively, you can instrument your AWS Lambda function with the Baselime OpenTelemetry tracer. Add the baselime:tracing tag to your AWS Lambda functions, and set it to true.","The automatic OpenTelemetry tracing with the tag is available for Node.js AWS Lambda functions, we're currently working on enabling this for other runtimes.","If you use AWS X-Ray, Baselime automatically capture traces from X-Ray when your AWS Account is connected."]},{"i":"how-hard-is-it-to-remove-baselime-from-my-aws-account","l":"How hard is it to remove Baselime from my AWS account?","p":["If you decide to remove Baselime from your AWS account, delete the CloudFormation template Baselime creates on your AWS account. That's all, all resources Baselime created, including the instrumentation layers, will be removed."]},{"i":"does-baselime-automatically-recognise-new-functions-and-services","l":"Does Baselime automatically recognise new functions and services?","p":["Yes, when you deploy new serverless functions and services to your cloud infrastructure, Baselime automatically detects them and starts ingesting logs, metrics and traces from those function. To add OpenTelemetry tracing, add the baselime:tracing tag to your new functions and set it to true."]},{"i":"where-is-my-data-stored","l":"Where is my data stored?","p":["You own your data.","You can select to use either our cloud offering, or our Bring Your Own Backend solution. With Bring Your Own Backend, all the data is stored on your AWS account and your use the Baselime clients to access it."]},{"l":"Cloud offering","p":["All the telemetry data your cloud infrastructure generate is stored in two data tiers:","hot tier: on Baselime AWS accounts in the eu-west-1 region. This data is used for fast questions","cold tier: in an Amazon S3 bucket in your AWS cloud account. This data is used for long terms storage in a resource you own","It is possible to rehydrate data from the cold tier to the hot tier for queriyng historical incidents free of charge."]},{"l":"Bring Your Own Backend","p":["Baselime can integrate with your own backend. As such, all the telemetry data is stored and queried in your cloud account. The enables you to keep maximum flexibility and privacy for storing sensitive data. You will be able to set your own retention periods, your own storage type, and your own privacy settings.","Bring Your Own Backend is available on our Enterprise Plans."]},{"i":"is-my-data-secure","l":"Is my data secure?","p":["Baselime is fully GDPR compliant and your data is stored in data centers that are all SOC2 compliant."]},{"i":"how-can-i-work-with-my-team","l":"How can I work with my team?","p":["Once you sign up to Baselime with your organisation domain email, you can configure Baselime such that anyone with the same email domain can join your workspace.","Moreover, you can invite your teammates individurally. Additionally, every query result, dashboards, an alerts have a unique permalink in Baselime that you can share with your team."]},{"i":"does-baselime-have-an-impact-on-my-aws-bill","l":"Does Baselime have an impact on my AWS bill?","p":["Baselime relies on a few AWS resources in your AWS account, most notably:","Amazon CloudWatch metrics stream: to enable CloudWatch metrics to be queried using the Baselime query engine","Amazon CloudTrail: to enable CloudTrail events, and also register new subscription filters as soon as new serverless functions or services are created","Amazon Kinesis Data Firehose: To store telemetry data in cold storage in your AWS account","These services may add a minimal cost on your AWS monthly bill. Please refer to the AWS princing calculator for estimates based on your usage."]}],[{"l":"Quick Start Guide"},{"i":"step-1-sign-up-for-baselime","l":"Step 1: Sign up for Baselime","p":["You can sign up for a free Baselime account here."]},{"i":"step-2-add-an-environment","l":"Step 2: Add an Environment","p":["You can add an environment by connecting your cloud account, or by creating an environment manually to send data manually to Baselime.","Generate the connector CloudFormation template from the Baselime console and deploy it to your AWS account.","Once the stack is deployed, telemetry data from your AWS account will be automatically ingested in Baselime. You will receive an email once the connection is complete.","Next, navigate invide the newly created environment and to the API Keys sections, and retrieve your Baselime API key.","Your Baselime API Key","If you do not see any data in the Baselime console within minutes of deploying the CloudFormation stack, something went wrong. Please contact us.","Once you're logged in, add a new environment manually from the Baselime console."]},{"i":"step-3-send-a-log-event","l":"Step 3: Send a log event","p":["Once you've added your first environment, execute this cURL command to send your log event to Baselime.","Replace your BASELIME_API_KEY with the API key your got from step 2."]},{"i":"step-4-explore-your-data","l":"Step 4: Explore your data","p":["Congratulations! Your first event should be available to query in Baselime. You can start exploring your data using the Baselime console, the Baselime CLI or any other of our clients.","View of an AWS Lambda function in Baselime"]},{"l":"Guides","p":["Sending Data: Learn how to ingest telemetry data from your cloud-native applications","Analyzing Data: Discover how to use the various interfaces provided by Baselime to analyze and understand your data","Integrations: Find out how to connect Baselime with your favorite tools"]},{"l":"Reference","p":["Baselime CDK Reference Guide: Learn about how to use Baselime with the AWS CDK to define your Observability as Code","ORL Reference Guide: Learn about the Baselime Observability Reference Language (ORL) and how to use it to define observability configurations","CLI Reference: Complete reference for the Baselime command-line interface"]},{"l":"Community","p":["Join the Baselime community to get help with using the platform, share your own experiences, and stay up-to-date with the latest developments.","Slack: Join our Slack community to connect with other Baselime users and get real-time support from the Baselime team","Blog: Read about the latest features, best practices, and more from the Baselime team","Social media: Follow us on Twitter, LinkedIn, and YouTube to stay up-to-date with the latest news and updates from Baselime","We look forward to connecting with you!"]}],[{"l":"Sending Data to Baselime","p":["Baselime supports a variety of data sources, including logs, metrics, traces, and wide events. You can start sending your data to Baselime and gain valuable insights into the performance and reliability of your microservices with a few steps.","Once received, all telemetry data is securely stored in hot storage for querying and in cold storage within your own AWS environment, in an Amazon S3 bucket in your AWS account. This ensures that you have complete long-term control over your data and its storage location.","Sending Telemetry data to Baselime","The retention period of your telemetry data on Baselime is independent of the retention periods in AWS. You can safely reduce the retention period of your CloudWatch log groups."]},{"l":"Data Sources"},{"l":"OpenTelemetry","p":["Sending Data to Baselime"]},{"l":"AWS","p":["Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime Sending Data to Baselime"]},{"l":"Other Sources","p":["Sending Data to Baselime Sending Data to Baselime"]}],[{"l":"OpenTelemetry Traces","p":["If your codebase is already instrumented with OpenTelemetry, you can start sending your tracing data to Baselime today.","Add the Baselime OpenTelemetry endpoint to your exporter:","Endpoint https://otel.baselime.io/v1/","Header: x-api-key: BASELIME_API_KEY","You can get your Baselime API key in the Baselime console from the Baselime CLI with:","If you have not instrumented your codebase with OpenTelemetry yet, do not worry. We are building OpenTelemetry SDKs to facilitate instrumenting your code."]},{"l":"OpenTelemetry SDKs","p":["OpenTelemetry Traces OpenTelemetry Traces"]}],[{"i":"opentelemetry-for-nodejs","l":"OpenTelemetry for Node.js","p":["The Baselime Node.js OpenTelemetry SDK(Star us ⭐) enables you to instrument your Node.js services with OpenTelemetry without the boilerplate of using the OpenTelemetry SDK directly.","This SDK uses OpenTelemetry for JavaScript and provides a layer that facilitates instrumenting your Node.js applications.","If your application is already instrumented with OpenTelemetry, you can start sending your tracing data to Baselime without any additional code changes.","Add the Baselime OpenTelemetry endpoint to your exporter:","Endpoint https://otel.baselime.io/v1/","Header: x-api-key: BASELIME_API_KEY"]},{"l":"Instrumentation"},{"i":"step-1-install-the-sdks","l":"Step 1: Install the SDKs","p":["Install the Baselime Node.js OpenTelemetry SDK."]},{"i":"step-2-initialise-the-tracer","l":"Step 2: Initialise the tracer","p":["Create a tracing.cjs file inside your application working directory."]},{"i":"step-3-set-the-baselime-environment-variables","l":"Step 3: Set the Baselime environment variables","p":["Set the environment variables of your comntainer service to include the Baselime API Key and set the NODE_OPTIONS enviroment variable to preload the OpenTelemetry SDK into your application.","Key","Value","Description","BASELIME_KEY","your-api-key","Get this key from the Baselime console or the Baselime CLI running baselime iam","NODE_OPTIONS","-r ./src/tracing.cjs --experimental-loader=import-in-the-middle/hook.mjs","Preloads the OpenTelemetry SDK at startup","Once these steps are completed, distributed traces from your Node.js container applications should be available in Baselime to query via the console or the Baselime CLI.","Example OpenTelemetry Trace"]},{"l":"Configuration","p":["An array of instrumentation options.","baselimeKey","collectorUrl","Description","Field","InstrumentationOption[]","instrumentations","namespace","service","string (optional)","The Baselime API key.","The BaselimeSDK class of the Baselime Node.js OpenTelemetry SDK takes the following configuration options.","The namespace.","The service name.","The URL of the collector.","Type"]}],[{"i":"opentelemetry-for-nodejs-on-aws-lambda","l":"OpenTelemetry for Node.js on AWS Lambda","p":["The Baselime Node.js OpenTelemetry tracer for AWS Lambda(Star us ⭐) instruments your Node.js AWS Lambda functions with OpenTelemetry and automatically sends the OpenTelemetry compatible trace data to Baselime. This is the most powerful and flexible way to instrument your Node.js AWS Lambda functions."]},{"l":"Automatic Instrumentation","p":["To automatically instrument your AWS Lambda functions with the Baselime Node.js OpenTelemetry tracer for AWS Lambda, set the following tag to your AWS Lambda functions: baselime:tracing=true.","To add the Baselime tag to all your AWS Lambda functions in a service or stack add this line to your AWS CDK code.","To add the Baselime tag to all your AWS Lambda functions in a service or stack add this line to your sst.config.ts file.","To add the Baselime tag to all your AWS Lambda functions in a add this snippet to your serverless.yml file.","To add the Baselime tag to all your AWS Lambda functions in a add this snippet to your AWS SAM configuration file.","OpenTelemetry automatic instrumentation is available only once you have connected your AWS Account to Baselime. Adding the tag to AWS Lambda functions in an AWS Account not connected to Baselime will not have any effect.","To remove the OpenTelemetry instrumentation from your AWS Lambda functions, remove the baselime:tracing=true tag from the function and Baselime will revert the function to un-instrumentate state."]},{"l":"How it works","p":["The automatic instrumentation makes changes to your AWS Lambda functions once they are deployed:","Add the Baselime Node.js OpenTelemetry AWS Lambda Layer to your AWS Lambda function: arn:aws:lambda:${region}:097948374213:layer:baselime-node:${version}- This layer is a slimmed down version of the OpenTelemetry JavaScript Client that will have minimal impact on the cold starts of your AWS Lambda functions","Add the Baselime Extension added to your AWS Lambda function: arn:aws:lambda:${region}:097948374213:layer:baselime-extension-${'x86_64' || 'arm64'}:${version}- This extension enables the Baselime Layers to send the trace data to the Baselime backend after the invocation is complete, as such, distributed tracing will not have any negative impact on the latency of your AWS Lambda functions","Set the BASELIME_KEY environment variable with the value of your environments Baselime API Key","These changes are kept in sync with your AWS Lambda function as you iterate on your architecture via events from Amazon CloudTrail.","OpenTelemetry Automatic Instrumentation FLow"]},{"l":"Adding custom OpenTelemetry events","p":["The Baselime Node.js OpenTelemetry tracer for AWS Lambda provides and extension that enables you add context rich events to your traces using an API that feels like a logger. These events can be useful to show more detailed context on errors, add steps that you want recorded for a business process or adding extra debugging information.","The extension provides an object that includes four logging functions - info, warn, debug, and error- enabling you to log messages with varying levels of severity. You can control the visibility of the events by setting the LOG_LEVEL environment variable, ."]},{"l":"Adding custom OpenTelemetry spans","p":["To add custom spans to your OpenTelemetry traces, it is necessary to install the @opentelemetry/api package. It is left out of the Baselime Node.js OpenTelemetry tracer for AWS Lambda to limit the impact on cold-starts, such that your can add it only to the AWS Lambda functions that require it."]},{"l":"Manual Instrumentation","p":["If you prefer to send the OpenTelemetry traces to Baselime manually, you can use the Baselime Node.js OpenTelemetry tracer for AWS Lambda(Star us ⭐) independently from connecting your AWS Account to Baselime."]},{"i":"step-1-install-the-sdk","l":"Step 1: Install the SDK","p":["Install the Baselime Node.js OpenTelemetry tracer for AWS Lambda."]},{"i":"step-2-manually-wrap-your-function-handlers-with-the-sdk","l":"Step 2: Manually wrap your function handlers with the SDK","p":["Wrap the handlers of your AWS Lambda functions with the baselime.wrap(handler) method."]},{"i":"step-3-set-the-baselime-environment-variables","l":"Step 3: Set the Baselime environment variables","p":["Set the environment variables of your AWS Lambda functions to include the Baselime API Key and set the NODE_OPTIONS enviroment variable to preload the OpenTelemetry SDK into your AWS Lambda bundle.","Key","Value","Description","BASELIME_KEY","your-api-key","Get this key from the Baselime console or the Baselime CLI running baselime iam","NODE_OPTIONS","--require @baselime/lambda-node-opentelemetry","Preloads the OpenTelemetry SDK at startup"]},{"i":"step-4-bundle-the-opentelemetry-sdk-with-your-code","l":"Step 4: Bundle the OpenTelemetry SDK with your code","p":["Ensure that the OpenTelemetry SDK is included in the .zip file that is uploaded to AWS Lambda during your deployment. The step depends on your deployment framework.","Set the default function props of your service to include the wrapper in the bundle and add the environment variables","By default the Serverless Framework includes the entire node_module folder in the .zip bundle of your AWS Lambda functions. If you are using the serverless-esbuild plugin or any other plugin to prevent this, it is necessary to edit the configuration of your project.","Add the following line to the package.patterns block of your serverless.yml file.","Add the following environment variables","Copy the lambda-wrapper.js file from the node_modules folder in the shared folder of your Architect project, it will be automatically included in all of your AWS Lambda .zip bundles.","Add the environment variables to your architect project","Note the '--' in the NODE_OPTIONS command. This is required to escape options parsing.","This method will however send the traces to the Baselime backend during the invocation of your AWS Lambda functions, and will result in a degradation in the latency performace of your functions.","In production we recommend additionally adding the Baselime AWS Lambda extension, as it will enable the OpenTelemetry tracer to send traces to the Baselime backend after the excecution of your AWS Lambda functions."]},{"l":"Sending data to another OpenTelemetry backend","p":["OpenTelemetry is an open standard, and you can use the Baselime Node.js OpenTelemetry tracer for AWS Lambda to send telemetry data to another backend of your choice.","Add the environment variable COLLECTOR_URL to send the data somewhere else than the Baselime backend."]},{"l":"Limitations","p":["The AWS JS SDK v2 can result in errors when interacting with OpenTelemetry during automatic request retries. This is the result of trace headers changing between retries and failing the signing verification processes. We've submitted a Pull Request to the AWS JS SDK and will be updating accordingly.","To prevent this issue from arising, add the code snippet below to your code."]}],[{"l":"AWS","p":["When you connect your AWS account to Baselime, logs, metrics, traces and events from your AWS services are sent to Baselime automatically."]},{"l":"AWS Data Sources","p":["AWS AWS AWS AWS AWS AWS AWS AWS"]}],[{"l":"AWS Lambda Logs","p":["Once you connect your AWS account to Baselime, Baselime automatically create CloudWatch Logs subscription filters to automatically ingest logs from your AWS Lambda functions.","Baselime automatically captures logs for newly created AWS Lambda functions, and enables you to query and visualise logs across log groups and log streams."]},{"l":"How it works","p":["Once Baselime is connected to an AWS Account, it automatically creates Logs subscription filters for all the AWS Lambda functions in the account. Log subscription filters enable Baselime to asynchronously ingest logs from the AWS Lambda functions through Amazon CloudWatch, without any impact on the performance of the AWS Lambda functions.","Sending Lambda Logs to Baselime","Moreover, Baselime automatically creates new subscription filters for newly deployed AWS Lambda functions. Baselime listens to new AWS Lambda events in Amazon CloudTrail and creates subscription filters for newly created AWS Lambda functions.","AWS Lambda Logs in Baselime"]},{"l":"Logging best practices","p":["In order to get the most out of Baselime, we recommend adding two log messages to all your AWS Lambda functions:","the event which triggered your Lambda function","the response your Lambda function returns","These can be added as follows:","To facilitate this in Node.js runtimes, we maintain a custom logger well suited for AWS Lambda.","It's a 2.5kb JavaScript file with 0 dependencies, and does not have any significant impact on performance or cold-starts.","It also provides an interface to be used as a middy middleware."]},{"l":"Logging format","p":["We recommend using structured logging across your application, preferably in JSON format. Feel free to use your favourite logging library; we recommend:","Baselime Lambda Logger for Node.js","Lambda Power Tools","It is particularly important to format errors and exception correctly to appropriately log stack traces.","or with the Baselime Lambda Logger for Node.js:"]},{"l":"Discovered Keys","p":["Baselime automatically discovers key - value pairs from your AWS Lambda logs. This enables you to run complex queries and setup alerts on data that otherwise would be difficult to work with from the AWS Lambda service. For instance, from the discovered keys from the Lambda logs, it's possible to set alerts on the maximum memory used by lambda functions during execution, compared to the amount of memory they are assigned at deployment time."]},{"l":"Lambda Discovered Keys","p":["The Lambda service automatically writes logs at the start and end of every function invocation. These logs are parsed as events in Baselime, and keys are automatically discovered from those messages."]},{"l":"START Log Message","p":["The following keys are discovered from the START message:","@type: is always START","@requestId: the request ID of the Lambda invocation","@version: the invoked version of the Lambda function"]},{"l":"END Log Message","p":["The following keys are discovered from the END message:","@type: is always END","@requestId: the request ID of the Lambda invocation"]},{"l":"REPORT Log Message","p":["The following keys are discovered from the REPORT message:","@type: is always REPORT","@requestId: the request ID of the Lambda invocation","@duration: the duration in milliseconds","@billedDuration: the billed duration in milliseconds","@memorySize: the total memory available to the invocation, in MB","@maxMemoryUsed: the max memory used, in MB","@initDuration: the duration of the lambda initialisation in milliseconds (cold starts)","If the Lambda function is instrumented with XRAY, additional keys are discovered:","@xRAYTraceId: the XRAY trace ID","@segmentId: the XRAY segment ID","@sampled: always true"]},{"l":"Timeout Invocations","p":["If your async Lambda invocation times out, Additional keys are automatically discovered:","@timedOut: always true","@timeout: the duration after which the invocation timed-out in seconds","@message: always Task timed out after {@timeout} seconds","@timestamp: the timestamp at the moment the invocation timed out."]},{"i":"consolelog-log-message","l":"console.log Log Message","p":["For Node.js environments, AWS Lambda uses a modified version of console.log(and other console logging functions) to write to stdout and stderr. These add fields to the log message which are parsed as follows:","@timestamp: the timestamp at the moment the log message was written","@requestId: the request ID of the Lambda invocation","LogLevel: the log level ( INFO, DEBUG, WARN, ERROR)","@message: the message.","If the message in @message is a valid JSON object, Baselime will parse it, otherwise it will be considered a string."]},{"l":"Troubleshooting","p":["If you're having trouble sending data from your AWS Lambda logs to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as CloudWatch Metrics or CloudTrail Events","Check that your Lambda functions are not already using the maximum number of subscription filters allowed per log group. AWS limits each log group to 2 subscription filters at most. If you're already at the limit, you can remove subscription filters with the cloudwatch-subscription-filters-remover to delete the ones you don't need anymore.","Make sure that your AWS Lambda functions are being invoked and you can view the logs in the CloudWatch section of the AWS Console"]}],[{"l":"Amazon ECS Container Logs","p":["This page describes how to collect application container logs from Amazon ECS clusters launched with AWS ECS using AWS FireLens. This method can also be used to collect ECS clusters with EC2 containers."]},{"l":"How it works","p":["FireLens is an Amazon ECS native log router that enables you to send logs from your containerized applications to different destinations, including Baselime. By adding the FireLens sidecar to your task definitions, you can configure and route your container logs to different destinations without modifying your application code.","Sending ECS Logs to Baselime","Each of your ECS tasks can take a sidecar container running the FireLens log driver that will forward all the logs from the containers to Baselime."]},{"l":"Configuring your ECS Tasks"},{"i":"step-1-obtaining-your-baselime-api-key","l":"Step 1: Obtaining your Baselime API Key","p":["You can get your Baselime API key using the Baselime CLI. Ensure you have downloaded the Baselime CLI and logged in your environment. To get your Baselime API Key, run the following command","Copy the Baselime API Key in the output of the command and keep it safe. In the following instructions we will use BASELIME_API_KEY to refer to your Baselime API key, make sure to replace it with the key you copied from the output of the command."]},{"i":"step-2-adding-the-firelens-sidecar-to-your-task-definitions","l":"Step 2: Adding the FireLens sidecar to your task definitions","p":["Adding the FireLens sidecar to your task definitions is a straightforward process that can be accomplished using various Infrastructure as Code solutions or manually in the console.","Add the Baselime ECS endpoint to your FireLens configuration:","Endpoint ecs-logs-ingest.baselime.io","Header: x-api-key BASELIME_API_KEY","Amazom ECS Logs in Baselime"]},{"l":"Troubleshooting","p":["If you're having trouble sending data from your AWS ECS logs to Baselime, here are a few things to check:","Verify that you're using the correct API key and host in the FireLens configuration","Make sure that your containers are receiving traffic and are writing logs to either stdout or stderr","Check the logs of the FireLens container to look for any anomaly"]}],[{"l":"AWS X-Ray Traces","p":["AWS X-Ray enables developers to generate and collect traces across their distributed services. In order to gain visibility into their applications, developers can use AWS X-Ray to trace requests as they travel through their application, and collect data about the performance of their application.","Baselime enables you to ingest this tracing data and make it available for analysis and troubleshooting.","AWS X-Ray trace diagram in Baselime AWS X-Ray trace waterfall in Baselime"]},{"l":"How it works","p":["Once Baselime is connected to an AWS Account, it will periodically poll your AWS account for new traces and automatically ingest them into your Baselime dataset.","Sending X-Ray Traces to Baselime"]},{"l":"Troubleshooting","p":["If you're having trouble sending data from AWS X-Ray to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as CloudWatch Metrics or CloudTrail Events","Check that the Baselime IAM user has the appropriate permissions to access X-Ray","Make sure that your applications emit X-Ray traces and you can view the traces in the X-Ray section of the AWS Console"]}],[{"l":"Amazon API Gateway Logs","p":["Once you connect your AWS account to Baselime, Baselime automatically create CloudWatch Logs subscription filters to automatically ingest logs from your Amazon API Gateways."]},{"l":"Setup","p":["Baselime can ingest logs only for Amazon API Gateways where access logs are appropriately configured.","We recommend this configuration for Amazon API Gateway logs:","It is possible to enable Amazon API Gateway access logs from your favourite Infrastructure as Code tool, using the CLI or in the AWS console. Below is an example of how to enable Amazon API Gateway logs using the serverless framework."]},{"l":"How it works","p":["Once Baselime is connected to your AWS Account, it automatically creates Logs subscription filters for all the Amazon API Gateways in the account.","Sending API Gateway Logs to Baselime","Moreover, Baselime automatically creates new subscription filters for newly deployed Amazon API Gateways. Baselime listens to new API Gateway events in Amazon CloudTrail and creates subscription filters for newly created Amazon API Gateways."]}],[{"l":"AWS App Runner Logs","p":["Once you connect your AWS account to Baselime, Baselime automatically create CloudWatch Logs subscription filters to automatically ingest logs from your AWS App Runner logs."]},{"l":"How it works","p":["Once Baselime is connected to your AWS Account, it automatically creates Logs subscription filters for all the AWS App Runner services in the account. AWS App Runner automatically creates two log groups for each service:","/aws/apprunner/service-name/unique-id/application: the logs from the container running","/aws/apprunner/service-name/unique-id/service: the internal logs of the AWS App Runner service, typically deployment logs","Baselime create subscription filters for the log groups ending in /application: the logs from the container.","Sending AWS App Runner logs to Baselime","Moreover, Baselime automatically creates new subscription filters for newly deployed AWS App Runner services. Baselime listens to new App Runner events in Amazon CloudTrail and creates subscription filters for newly created AWS App Runner services."]},{"l":"Troubleshooting","p":["If you're having trouble sending data from your AWS App Runner logs to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as CloudWatch Metrics or CloudTrail Events","Check that your App Runner services functions are not already using the maximum number of subscription filters allowed per log group. AWS limits each log group to 2 subscription filters at most. If you're already at the limit, you can remove subscription filters with the cloudwatch-subscription-filters-remover to delete the ones you don't need anymore.","Make sure that your AWS App Runner services are being live and you can view the logs in the CloudWatch section of the AWS Console."]}],[{"l":"Lambda Telemetry Extension","p":["Instrumenting AWS Lambda functions with Baselime is straightforward using our Lambda Extension. The Lambda Extension listens to invocation events and collects telemetry data, such as logs and runtime metrics. Once collected, the telemetry data is sent to Baselime for storage, analysis, and visualization. In this section, we'll walk you through the process of instrumenting your Lambda functions with the Baselime Lambda Extension.","The Baselime Lambda telemetry extension is an optional tool that provides additional telemetry data for your AWS Lambda functions. It is only necessary if you choose not to ingest Lambda logs directly from CloudWatch.","Before getting started you'll need to make sure that you have your Baselime API key ready. You can get it by running the following command using the Baselime CLI."]},{"l":"How it works","p":["The Baselime Lambda Extension is language agnostic and is compressed as a single binary, such that it minimises its impact cold-starts and performance.","The diagram below illustrates how the Baselime Lambda Extension works within your architecture.","Using the Baselime Lambda Extension","All the telemetry data from your Lambda function is collected asynchronously from your invocation, and sent to the Baselime backend in a separate process from your invocation."]},{"l":"Instrumenting","p":["To instrument your AWS Lambda Functions with the Baselime Lambda Extension, we recommend using your Infrastructure as Code tool of choice, and add the Extension as a Lambda Layer.","It is necessary to add the Baselime API key to the extension as an environment variable. The example below shows the process with the Serverless Framework.","Where the BASELIME_KEY is your Baselime API Key and the BASELIME_LAMBDA_LAYER_ARN is the ARN of the Baselime Layer in your region."]}],[{"l":"Amazon CloudTrail","p":["Baselime automatically ingests Amazon CloudTrail events when you connect your AWS account. Baselime will automatically create a new Amazon CloudTrail trail and an Amazon S3 bucket, and configure both to send data to your Baselime account. No additional setup is required.","Once connected, Amazon CloudTrail events will be sent to Baselime and become available for querying."]},{"i":"why-amazon-cloudtrail-","l":"Why Amazon CloudTrail ?","p":["Amazon CloudTrail is a service provided by AWS that records API activity in your AWS account. This data can be used to track changes to your resources, troubleshoot issues, and improve security.","By sending Amazon CloudTrail events to Baselime, you can use our query and visualization tools to analyze and understand your API activity. You can also set up alerts to be notified of specific API activity or trends.","With Amazon CloudTrail events in Baselime, you can gain a deeper understanding of your AWS API activity and use that knowledge to improve the security and reliability of your applications."]},{"l":"How it works","p":["Amazon CloudTrail periodically writes trail data in a pre-configured Amazon S3 bucket in your AWS account. Once the data is written, an Amazon SNS topic is triggered.","Baselime configures this Amazon SNS to invoke an AWS Lambda function. This function reads the data from the bucket and sends it to the Baselime backend.","Sending CloudTrail data to Baselime"]},{"l":"Amazon CloudTrail management events","p":["Amazon CloudTrail events fall into multiple categories, and Baselime automatically ingests CloudTrail management events. Please refer to the complete CloudTrail docs for further details on the CloudTrail concepts."]}],[{"l":"Amazon CloudWatch Metrics","p":["Baselime automatically collects Amazon CloudWatch Metrics from your AWS account. Once you connect your AWS account to Baselime, the necessary resources including a CloudWatch Metrics Stream and a Kinesis Firehose will be automatically created and configured. No additional setup or configuration is required."]},{"i":"why-amazon-cloudwatch-metrics-","l":"Why Amazon CloudWatch Metrics ?","p":["Amazon CloudWatch is a monitoring service provided by AWS that enables you to collect and track metrics for your AWS resources and applications. Metrics are important as they provide insight into the performance and behavior of your applications and the underlying infrastructure.","Amazon CloudWatch Metrics can help you identify issues such as high error rates and latencies, which can help improve the overall reliability and scalability of your applications.","Amazon CloudWatch Metrics cover all aspects of your architecture automatically, from DynamoDB tables to S3 buckets and SQS Queues."]},{"l":"How it works","p":["Once Baselime is connected to an AWS Account, it automatically created the telemetry pipeline for ingesting Amazon CloudWatch metrics into Baselime. The pipeline comprises a CloudWatch Metrics Stream, a Kinesis Firehose and all IAM roles and permissions associated.","This pipeline automatically and continuously sends metrics from your AWS account to Baselime.","Sending Amazon CloudWatch Metrics to Baselime","Amazon CloudWatch Metrics Stream might incur a minimal cost on your AWS account. AWS charges $0.003 per 1,000 metric updates. Refer to the AWS docs for more details."]},{"l":"Custom Amazon CloudWatch Metrics","p":["Baselime automatically ingests all metrics published to Amazon CloudWatch. This includes both standard Amazon CloudWatch metrics and any custom metrics that you may have created.","There is no need to manually configure or set up anything to start ingesting custom Amazon CloudWatch metrics. Once your AWS account is connected, all metrics will be available for querying in Baselime."]},{"l":"Querying Amazon CloudWatch Metrics","p":["Once your AWS account is connected to Baselime, you can use any of the our clients to visualize and query your Amazon CloudWatch Metrics. You'll have access to all the metrics available in your AWS account, and you can use the Observability Reference Language (ORL) to filter and aggregate the data in near real-time."]},{"l":"Troubleshooting","p":["If you're having trouble sending metrics from Amazon CloudWatch to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as AWS Lambda Logs or CloudTrail Events","Check that the Kinesis Firehose created in your AWS account as part of the Baselime connection has the appropriate API key to connect with the Baselime backend. If the API key is missing, please contact us."]}],[{"l":"Vercel","p":["Vercel is a cloud platform for hosting and deploying web applications and websites. It is designed to make the process of deploying, scaling, and managing web apps as simple and efficient as possible. Vercel offers a variety of features and tools for web developers."]},{"l":"Baselime Integration","p":["Baselime provides advanced observability capabilities for applications running on Vercel.","Install the integration for free on the Vercel marketplace and start observing your Vercel apps in minutes."]},{"l":"How it works","p":["When you install the Baselime integration on the Vercel Marketplace, Baselime automatically creates a log drain on your Vercel account to start streaming all your logs in real-time.","Once Baselime receives your data, we automatically discover all your Vercel apps and ingests all your logs into three datasets:","vercel-build: logs from your Vercel build steps","vercel-edge: logs from Vercel Edge functions","vercel-functions: logs from Vercel functions such as API routes","vercel-static: logs from requests to static assets on Vercel such as HTML and CSS files","The logs are separated in multiple datasets to give you a complete view of your Vercel applications."]},{"l":"Using the Vercel integration","p":["All the logs from your Vercel apps are streamed to Baselime. You can search, query or tail your logs from the console and the CLI. You can create alerts from derived metrics from your logs. You can use our dashboard templates to create dashboards based on your Vercel logs, and modify them at will.","Vercel Logs in Baselime"]}],[{"l":"Events API","p":["Baselime provides an HTTP events API which enables you to send data to Baselime by making a POST request to the API endpoint. You send data directly from your applications or services to Baselime, rather than using a logging or monitoring service as an intermediary."]},{"l":"Request Format","p":["Each request ingests a batch of events into Baselime. Events are part of the request body. Baselime supports Content-Type application/json.","The request body must be an array of JSON objects. Any element of the array that cannot be parsed as valid JSON will be rejected.","Requests must be made to the /dataset/service/namespace route:","dataset is the name of the dataset that the events should be ingested into. You can either use an existing dataset or create a new one using the Baselime CLI.","service is the service that the events belong to. If the service doesn't exist beforehand, the events can be queried through the default service. Once you create the service (in the web console or using the Baselime CLI), the events will be available from the service too.","namespace is the namespace within the dataset that the events should be ingested into. The namespace is created automatically for you when events are received, if it didn't exist beforehand."]},{"l":"Authentication","p":["The HTTP API requires a valid Baselime API key to be sent in the x-api-key request header.","You can obtain your API key using the Baselime CLI."]},{"l":"Validation","p":["The HTTP API validates the provided events and returns a 400 Bad Request status code if any of the events fail validation with a list of all the events that failed validation. If some events pass validation and others fail, we will ingest the events that pass validation. If you encounter a 400 Bad Request error when submitting events to the HTTP API, the events that failed validation will be listed in the body of the request under the invalid key."]},{"l":"High-level requirements","p":["Baselime accepts up to 6MB of uncompressed data per request","Each event must be a properly formatted JSON","Each event must be smaller than 128kb of uncompressed JSON"]},{"l":"API Response codes","p":["Baselime returns a 202 response for all valid requests to the HTTP Events API, and a range on of non- 200 responses for errors.","We welcome feedback on API responses and error messages. Reach out to us in our Slack community with any request or suggestion you may have."]},{"l":"Successful responses","p":["Status Code","Body","Meaning","202","{message: Request Accepted}","All the events were successfully queued for ingestion"]},{"l":"Failure responses","p":["Status Code","Body","Meaning","405","{message: Method Not Allowed}","The HTTP method is now allowed","401","{message: Unauthorised}","Missing or invalid API Key","400","{message: Bad Request}","- Missing or invalid path parameters ( v1, dataset, service or namespace) - Unable to parse the request body as valid JSON- Empty request body - At least one of the events exceed the 128kb size limit - At least one of the events could not be parsed as valid JSON","500","{message: Internal Error}","An unexpected error occurred"]}],[{"l":"Rehydrating telemetry data from Amazon S3","p":["This page describes how you can rehydrate your telemetry data from Amazon S3 into Baselime."]},{"l":"How it works","p":["When your data is streamed to Baselime, through different sources described in the Sending Data to Baselime section, it is also streamed to a Kinesis Firehose created in your AWS account. The Data Firehose stores the telemetry data in a S3 bucket in your AWS account.","This gives you full ownership of your data and enables you to use it outside the Baselime; for example to feed it into a data lake. It is also possible to rehydrate the data from the S3 bucket into Baselime once the data is past its expiration period on Baselime.","Data flow","We set the default TTL for objects stored in the bucket to 180 days to prevent extremely long storage of telemetry data you might not need; Feel free to adjust it to your needs."]},{"l":"How to use it","p":["First, you'll need to have Baselime CLI installed. You can find the installation instructions here.","Once you have it installed, you can use the following command to rehydrate your data:","Start date should be formatted in RFC3339 format, and hours to recover should be a number. The process will recover all the data from the start date, for the number of consecutive hours from that date."]}],[{"l":"Data Validation in Baselime","p":["Baselime has a size limit for events of 256kb. This size limit helps ensure that the ingestion process is efficient and that the data stored in Baselime is manageable and fast to query. If an event exceeds this 256kb size limit, it will not be ingested into Baselime."]},{"l":"Sending Semi-Structured Logs to Baselime","p":["Semi-structured logs are logs that are not in the strict JSON format, but still contain structured data that can be extracted.These logs contain a mixture of structured and unstructured data, making them difficult to parse and analyze. Fortunately, Baselime has built-in mechanisms to parse and extract relevant data from semi-structured logs.","Baselime will automatically detect log events that contain JSON data, but are prepended or appended by a generic string.","The generic string will be wrapped in a message attribute, and the JSON data will be wrapped in a data attribute. This enables you to extract and analyze relevant data from semi-structured logs."]},{"l":"Examples","p":["Here are examples of automatic semi-structured logs detection."]}],[{"l":"Automatic Service Discovery","p":["Baselime automatically discovers services in your cloud accounts, and organize logs, metrics, traces, and other telemetry data into services.","With automatic service discovery, Baselime organises your observability following services and teams boundaries, enabling you to quickly sift through the vast amounts of data your applications produce."]},{"l":"Discovering Services","p":["Baselime automatically discovers all cloud resources in your cloud accounts. Each resource is linked to a service. The service is typically based on the deployment framework that you use.","The service name is the name of the CloudFormation template the AWS CDK generates during cdk synth","The name of the service is the name of the SST app","The name of the service is the name of the Serverless Framework App","The name of the service is the name of the CloudFormation template generated when deploying the AWS SAM application","The name of the service is the name of the CloudFormation template","When ingesting data from your architecture, Baselime correlates the incoming data with the service name of the cloud resource the data originates from. For example, all the logs of an AWS Lambda function deployed with CloudFormation are correlated with the name of the CloudFormation stack.","You can then view a list of services in the Baselime console, with key health.","List of services in the Baselime console","Organising telemetry data by service enables you to query only the data for a specific service when exploring telemetry data. For cases where there's a defect that spans multiple services, Baselime enables you to run queries across all services through the default service."]},{"l":"Overriding the service discovery","p":["To force the resources from a CloudFormation stack to belong to a service with a different name, set the value of the tag baselime:service to the desired service name on the CloudFormation template. All resources deployed with the CloudFormation template will be correlated with the desired service name."]}],[{"l":"Baselime AI","p":["Baselime AI provides explanations for any chart, log, event, metric or trace on Baselime. It enables you to identify patterns and anomalies in your systems, providing deeper insights into your system performance and behavior."]},{"l":"Getting Started","p":["You’ll need a Baselime account to start using Baselime AI. If you don't have one already, you can quickly sign up for a free trial on our website.","Once you have an account, follow these steps to use Baselime AI:","Navigate to the Baselime dashboard and select the chart, log, event, metric, or trace you want to analyze.","Click on the \"Ask AI\" button located next to the chart.","Wait for Baselime AI to process your query and provide a response.","Baselime AI explaining an error"]},{"l":"How it Works","p":["Baselime AI is built using OpenAI's Large Language Models. When you ask Baselime AI a question, it analyzes the data in the selected chart or event, and uses machine learning algorithms to identify any anomalies or patterns.","Once Baselime AI has identified potential issues, it generates a response that explains the root cause of the problem, as well as any recommended actions to fix it. The response is presented in natural language, enabling developers of all skill levels to understand."]},{"l":"Benefits","p":["Baselime AI offers a range of benefits, including:","Streamlined debugging: Baselime AI enables you to quickly identify and fix issues, reducing the time and effort required for debugging.","Improved system observability: With Baselime AI, you can gain deeper insights into your system performance and behavior, enabling you to optimize your systems and improve overall system observability.","Accessible to all skill levels: Baselime AI's natural language explanations enable developers at all skill levels to understand and interpret results."]},{"l":"Privacy","p":["Baselime is committed to protecting the privacy of our users' data. We understand the importance of keeping data secure and confidential, and we take appropriate measures to safeguard it.","When you use Baselime AI, your data is processed in accordance with our privacy policy. Before any data is sent to OpenAI, it is completely anonymized to protect the privacy of our users. This is done through a process called obfuscation, which replaces identifiable information with obfuscated tokens.","Once data has been anonymized, it is sent to OpenAI for processing. OpenAI is a trusted provider of AI services, and we have taken steps to ensure that your data is processed in accordance with our privacy policy and our high standards for data security.","Simplified Baselime AI architecture diagram","The diagram above shows a simplified overview of the data flow and privacy measures taken in Baselime AI. When you request an explanation for a chart, log, event, metric, or trace, the data is first processed and analyzed by Baselime AI. This analysis is done locally within our system to ensure that sensitive data is not transmitted. Only after data is anonymized through obfuscation is it sent to OpenAI for processing. OpenAI processes the data and returns an explanation to our system, which is then delivered to you.","We take data privacy seriously and are committed to ensuring that your data is protected at all times. If you have any questions or concerns about our privacy policy or data security, please don't hesitate to contact us."]}],[{"l":"Queries","p":["Queries are the primary way of interacting with your data in Baselime."]},{"l":"Queries in the Console","p":["You can run queries in the Baselime console by navigating to your service and clicking on the New Query button. This will bring up the Visual Query Editor. You can edit the query visually, but also switch to the embedded code editor to write your query using the Observability Reference Language (ORL).","To execute a query, click the Run Query button. The query results will be displayed in visually and in a table below the editor."]},{"l":"Queries in the CLI","p":["You can also run queries using the Baselime CLI. To do so, use the baselime query command.","Use the baselime query without any flags to enter interactive mode where you can specify all the arguments of your query interactively.","You can also run saved queries using the CLI, either in interactive mode or by passing the arguments as flags","You can also save your query results to a file. Use the --format to print the results of the query in JSON, and pipe them to a file.","For more advanced usage of the baselime query command, please refer to the CLI reference."]}],[{"l":"Alerts","p":["Baselime's alerting feature enables you to set up notifications for when certain conditions are met in your telemetry data. This can be helpful for detecting and responding to issues in your system in real-time."]},{"l":"Setting up alerts","p":["To set up an alert, you will need to specify a query and a threshold. When the result of the query meets the conditions the threshold, the alert will be triggered. You must also specify the frequency to check the query, and time window to consider for the alert.","You can set up alerts using the Baselime CLI with Observability as Code using the Observability Reference Language or the web console. Here is an example of how to set up an alert with ORL:","To create this alert, add it to a any .yml file in your .baselime folder. If you don't have a .baselime folder for your service, create it with baselime init.","Once you have the .baselime folder configured, run the following command to create your alert:"]},{"l":"Receiving alerts","p":["When an alert is triggered, you can choose to receive notifications through a variety of channels, such as email, Slack, or PagerDuty (coming soon)."]},{"l":"Tips for effective alerting","p":["Make sure to set appropriate thresholds for your alerts. Setting the threshold too low may result in false positives, while setting it too high may result in missed issues.","Keep alerts specific and actionable: Alerts should be specific and provide clear instructions on what action to take.","Set up alerts for the right things: Make sure to set up alerts for the most important issues that need immediate attention.","Use multiple alerting methods: Use a combination of Slack, email, and webhooks to ensure that you are notified of important issues in a timely manner.","Use alert suppression: Silence repeated alerts to avoid alert fatigue and ensure that you are only notified of important issues.","Consider using webhook alerts to build self-healing systems","Test your alerts to ensure they are working as expected.","Use alert analytics: Use alert analytics to analyze the effectiveness of your alerting strategy and make improvements where necessary.","Regularly review and update alert thresholds and configurations to ensure they are still relevant and effective."]}],[{"l":"Tailing your data","p":["The baselime tail command enables you to stream telemetry data in real time to your terminal. This can be useful for debugging or quickly checking the status of your services.","By default, the baselime tail command will stream all telemetry data for your Baselime environment. You can further filter the data by adding query parameters, such as:","This will only show the events where data.user.id is 123456 and the word error appears in the event.","You can also specify a time range for the data being streamed:","Alternatively, you can define the timerange in relative format","This will stream telemetry data between the specified start and end times.","The baselime tail command can be a useful tool for quickly checking the status of your application and identifying any issues that may be occurring."]}],[{"l":"Snapshots","p":["Snapshots enable you to capture the current state of all your alerts in a service at a given point in time. This can be useful for debugging purposes or for creating a record of the health of your system at a specific moment."]},{"l":"Using Snapshots","p":["To create a snapshot, run the baselime test command in your terminal. This will create a snapshot of the current state of all alerts in the current service, display the results in the terminal, and output them to a file in JSON format. You can specify the output file path using the --out-file flag.","This will create a snapshot of the alerts in the my-service service and save it to the snapshot.json file."]},{"i":"viewing-snapshots-coming-soon","l":"Viewing Snapshots (Coming soon)","p":["You can view your snapshots in the Baselime console under the \"Snapshots\" tab in the navigation menu. From here, you can view the details of each snapshot, including the time it was created, the service it was created for, and the state of each alert at that time."]},{"l":"Tips for Effective Snapshotting","p":["Use snapshots as a debugging tool to help you understand the state of your system at a specific moment in time","Use snapshots to compare the state of your alerts before and after making changes to your service.","Save snapshots for compliance purposes."]}],[{"l":"Reports","p":["Baselime reports allow you to monitor the health of your services and get notified when issues arise. Reports can be triggered on-demand, and can be sent to third party integrations such as Slack or Github."]},{"i":"when-to-use-reports-","l":"When to use reports ?","p":["Baselime reports are a powerful tool that enable your team to compare the state of a service before and after making changes. By incorporating Baselime reports into your CI/CD pipeline, your team can see the impact of their changes on alerts, dashboards, and SLOs in production. This not only improves the reliability of your deployments, but it also enables your team to build self-healing systems. For example, if a report after deployment is negative, your team can roll back or roll forward to ensure the stability of your service.","Here is an example of how you can use the baselime report command in a GitHub Action to compare the state of a service before and after a deployment:","This workflow will take a snapshot with baselime report github before and after running the deployment script ( npm run deploy). The report will be posted on the commit that triggered the workflow as a comment. It contains the current state of your service, including alerts, dashboards, and SLOs (coming soon). By comparing the two snapshots, you can see how the deployment affected your service and take appropriate action if needed."]},{"l":"Running a report","p":["To run a report, use the baselime report command. By default, this will create a snapshot of all the alerts in the current service, display the results in the terminal, and output them to a file.","To generate and publish a report, run the baselime report command followed by the name of the integration you want to publish the report to:","For example, to publish a report to GitHub, you would run:"]},{"i":"slos-and-dashboards-coming-soon","l":"SLOs and Dashboards (Coming Soon)","p":["In the future, the report command will also include support for publishing Service Level Objectives (SLOs) and creating dashboards to visualize your report data. Stay tuned for updates!"]}],[{"l":"Baselime CDK Quick Start","p":["Observability is a first class citizen of your infrastructure with Baselime. You can use the AWS CDK to define your observability configurations in Baselime."]},{"l":"Installation","p":["Download the Baselime CDK on npm:@baselime/cdk"]},{"l":"Configuration","p":["Initialise the Baselime CDK with your Baselime API Key."]},{"l":"Example alert","p":["Set up an alert everytime there's an error in your application logs:","This alert will notify you on Slack when there is an event with LogLevel equal ERROR in your telemetry data."]}],[{"l":"Baselime CDK Queries","p":["Queries are used to retrieve and analyze data from various datasets in order to gain insights from your services."]},{"l":"Sample Query Spec","p":["Here’s a sample query in Baselime CDK that uses all of the supported settings for defining queries in Baselime. Use it to get started creating your own queries."]},{"l":"properties","p":["Queries have a set of properties that define the query's characteristics and behavior."]},{"i":"description-optional","l":"description (optional)","p":["The description of the query is a string that provides more information about the query. It can include details about the data being queried, the calculations being performed, and any other relevant information.","Example:"]},{"l":"parameters","p":["The parameters of a query define the datasets to query, the calculations to perform on the data, and any filters or groupings to apply."]},{"i":"datasets-optional","l":"datasets (optional)","p":["The datasets parameter is an array of strings that specify the names of the datasets to query. Baselime supports querying multiple datasets simultaneously, allowing you to analyze data from different sources in a single query. If no datasets are provided, Baselime CDK defaults to lambda-logs.","Example:"]},{"i":"filters-optional","l":"filters (optional)","p":["eq: Equals","Example:","exists: Exists (applies to fields that may or may not exist in the data)","Filters can be used to narrow down the data being analyzed and focus on specific events or attributes.","gt: Greater than","gte: Greater than or equal to","inArray: In (applies to arrays only)","includes: Includes","lt: Less than","lte: Less than or equal to","Moreover, it is possible to add a filter to a query after the query has been initialised.","neq: Does not equal","notExists: Does not exist (applies to fields that may or may not exist in the data)","notInArray: Not in (applies to arrays only)","notIncludes: Does not include","regex: Matches a regular expression","startsWith: Starts with (applies to strings only)","The filters parameter is an array of strings that specify conditions to filter the data by. Baselime CDK provides multiple helper functions to create query filters:"]},{"i":"calculations-optional","l":"calculations (optional)","p":["The calculations parameter is an array of strings that specify the calculations to perform on the data. Baselime CDK provides multiple helper functions to create query calculations:","count: Counts the number of events.","countDistinct: Counts the number of distinct occurences of a field (applies to strings only).","max: returns the maximum value of a field.","min: returns the minimum value of a field.","sum: returns the sum of all values of a field.","avg: returns the average of all values of a field.","median: returns the median of all values of a field.","stdDev: returns the sample standard deviation of a field.","variance: returns the sample variance of a field.","p001, p01, p05, p10, p25, p75, p90, p95, p99, p999: return the specified percentile of all values of a field.","Calculations can be used to perform statistical analysis on the data and derive insights such as the average request duration, the maximum response size, or the 95th percentile of request latencies.","It is possible to pass an optional alias to each of these functions, such that the results are displayed in the Baselime console or CLI using the alias.","Example:"]},{"i":"groupby-optional","l":"groupBy (optional)","p":["The groupBy parameter is an object that specifies how to segment the data by a field. It has the following fields:","value: The field to group the data by","limit: The maximum number of results to return (default: 10)","type: The type of the data field to group by (string, boolean, or number)","orderBy: The calculation to order the results by (default: the first calculation in the query)","order: The order in which to return the results (ASC or DESC, default: DESC)","Grouping the data by a field allows you to segment the results into distinct groups and analyze them separately.","Example:"]},{"i":"needle-optional","l":"needle (optional)","p":["The needle parameter is an object that specifies a search to perform on the data. It has the following fields:","value: The string to search for","matchCase: A boolean indicating whether the search should be case-sensitive(default: false)","isRegex: A boolean indicating whether the search value is a regular expression (default: false)","The needle can be used to find specific set of events or patterns in the data.","Example:"]},{"l":"Adding an alert","p":["Baselime CDK enables you to add an alert to a query. The alert will run the query on a defined schedule and notify you on your preferred channels when specific conditions are met."]},{"l":"Example Queries","p":["Here are example Baselime CDK queries that combine all of the above properties.","This query retrieves data from the otel traces dataset and performs several calculations on the data. It computes the average request duration, maximum response size, and 95th percentile of request latencies for each user ID in the dataset.","It filters the data to only include user IDs with a request duration greater than 500ms, and limits the results to the top 100 user IDs based on the average request duration. The results are ordered by the average request duration in descending order. The query also searches for the word \"error\" in the data and filters the results based on whether or not the word is present.","This Baselime CDK query calculates the total consumed read capacity units for each DynamoDB table in a service. It filters the data to only include events with a metric_name of ConsumedReadCapacityUnits and a unit of Count, and groups the results by TableName. The query returns the top 10 tables with the highest consumed read capacity units."]}],[{"l":"Baselime CDK Alerts","p":["Alerts are used to run a query on a schedule and notify you if a threshold is crossed. Baselime alerts are based on Baselime queries, which gives you you the flexibility to specify alerts on defects or events of interest, and reduce false positives and alert fatigue."]},{"l":"Sample Alert Spec","p":["Here’s a sample alert in Baselime CDK that uses all of the supported settings for defining alert in Baselime. Use it to get started creating your own alert."]},{"l":"properties"},{"i":"description-optional","l":"description (optional)","p":["The description of the alert is a string that provides more information about the alert. It can include details about the conditions being monitored and any other relevant information.","Example:"]},{"i":"enabled-optional","l":"enabled (optional)","p":["The enabled property is a boolean that indicates whether the alert is enabled or disabled. If set to true, the alert will be active and trigger notifications when thresholds are met. If set to false, the alert will be inactive and no notifications will be sent.","Example:"]},{"l":"parameters","p":["The parameters of an alert define the query to run, the threshold to evaluate, and the frequency and window for monitoring. query"]},{"l":"query","p":["The query parameter specifies the query to run for monitoring. It can reference an existing query object or include an inline query definition.","Example:","With the inline query the calculation defaults to [calc.count()]","or"]},{"l":"threshold","p":["The threshold parameter specifies the condition to evaluate from the query results. It can use helper functions to create comparisons or calculations, such as gt, lt, eq, count, etc.","Example:"]},{"l":"frequency","p":["The frequency parameter specifies the frequency at which the alert should run the query and evaluate the threshold. It uses a string representation of the frequency, such as '5 mins', '1 hour', '1 day', etc.","Example:"]},{"l":"window","p":["The window parameter specifies the time window to look back for data when evaluating the threshold. It uses a string representation of the time window, such as '10 mins', '1 hour', '1 day', etc.","Example:"]},{"l":"channels","p":["The channels property specifies the destinations to send the alert notifications. It is an array of channel objects, where each object defines the channel type and targets."]},{"l":"type","p":["The type property specifies the type of channel for the alert. Baselime CDK supports various channel types, such as 'email', 'slack', 'webhook', etc."]},{"l":"targets","p":["The targets property specifies the target destinations for the alert notifications. The targets can be specific emails, channels, or URLs depending on the channel type.","Example:","Moreover, you can define a defaultChannel when initialising your Baselime CDK, this channel will be used for all alerts in the service, simplifying your CDK code."]}],[{"l":"Baselime CDK Dashboards","p":["Dashboards give you a birds eye view of a collection of your query results. This can help you look at multiple related graphs on a single page to spot interesting trends.","Dashboards are a collection of queries and charts that you want to keep for future reference. Boards help you visualise multiple queries at once, to spot interesting trends and share your findings with your team."]},{"l":"Sample Dashboard Spec","p":["Here’s a sample dashboard in Baselime CDK that uses all of the supported settings for defining dashboard in Baselime. Use it to get started creating your own dashboard."]},{"l":"properties"},{"i":"description-optional","l":"description (optional)","p":["The description of the dashboard is a string that provides more information about the dashboard. It can include high-level details or any other relevant information.","Example:"]},{"l":"parameters","p":["The parameters of a dashboard define the widgets to display on the dashboard."]},{"l":"widgets","p":["The widgets parameter is an array of widget objects that specify the queries to run and the names of the widgets to display on the dashboard. name and description are both optional parameters for a widget.","Example:"]}],[{"i":"observability-reference-language-orl","l":"Observability Reference Language (ORL)","p":["This is the documentation for Baselime's Observability as Code configurations using the Observability Reference Language (ORL).","ORL (Observability Reference Language) is a language used to express queries for observability telemetry data. ORL queries can be used to extract insights from logs, metrics, and traces data sources. ORL queries are defined by a set of parameters that specify the data sources, filters, and calculations to be performed on the data. The result of an ORL query is a set of events that match the criteria defined in the query, optionally aggregated by calculations.","ORL configurations are defined in YAML files.","Generally, ORL files live in the .baselime folder in the root directory of a given project. We refer to this folder as .baselime elsewhere in the documentation, although users can rename it at will.","Multiple integrations and connectors with your favourite Infrastructure as Code platforms are currently being developed."]},{"l":"Best Practice","p":["To streamline your Observability workflows, we recommend keeping your .baselime folder in Git alongside your source code. This enables you to sync and version control your queries, alerts and dashboards, and collaborate with other team members.","To pull the pregenerated queries and dashboards to your local machine, run baselime pull using the Baselime CLI. If the service has not been initialized locally, the CLI will prompt you to select the relevant service from a list of all your services. Once selected, Baselime will download all the queries, alerts, and dashboards for that service, enabling you to work with them locally."]}],[{"l":"ORL Services","p":["ORL (Observability Reference Language) services are used to organize and manage observability resources such as queries, alerts, and dashboards.","Note that the service must be defined in the madatory index.yml file in the .baselime folder."]},{"l":"Sample Service Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining a service in Baselime. Use it to get started creating your own services."]},{"l":"Properties","p":["Services have a set of properties that define the service's characteristics and behavior."]},{"i":"version-required","l":"version (required)","p":["The version property is a string that specifies the version of the Baselime CLI used to generate or deploy the service. It is used for version control and management.","Example:"]},{"i":"service-required","l":"service (required)","p":["The service property is a string that specifies the name of the service. It is used to identify the service and distinguish it from other services.","Example:"]},{"i":"description-optional","l":"description (optional)","p":["The description property is a string that provides more information about the service. It can include details about the purpose of the service, the components it includes, and any other relevant information.","Example:"]},{"i":"provider-required","l":"provider (required)","p":["The provider property is a string that specifies the cloud provider for the service. It is used to identify the provider and distinguish it from other providers. ORL supports the following providers:","aws","gcp(coming soon)","azure(coming soon)","cloudflare(coming soon)","vercel(coming soon)","Example:"]},{"i":"templates-optional","l":"templates (optional)","p":["The templates property is an array of strings that specifies the templates to automatically download and implement for the service. Templates are used to define observability rules that can be shared and reused across multiple services. Each string is in the format workspace/template, where workspace is the name of the workspace where the template was defined and template is the unique ID of the template.","Example:"]},{"i":"variables-optional","l":"variables (optional)","p":["The variables property is an object that enables you to define variables that can be used in the ORL queries and alerts within the service. These variables can be used to parameterize the ORL queries and alerts and make them more flexible and reusable.","Each variable has a name and one or more values. The values can be grouped by environment (e.g. prod, dev, etc.) or by any other criteria that makes sense for your service.","For example, you might define a threshold variable that has different values for different environments:","In this example, the threshold variable has a default value of 30, and different values for the prod and dev environments: 10 and 20, respectively.","To use this variable in an ORL query or alert, you can use the syntax:","In this example, the threshold variable will be replaced with the appropriate value depending on the environment in which the service is deployed.","It is important to note that variables are optional in services. If a variable is defined, it must have at least one value."]},{"l":"Example ORL Services","p":["Here are example ORL services that combine all of the above properties.","This ORL service is for a web application that is hosted on Amazon Web Services (AWS).","The cloud provider is AWS and the infrastructure consists of two CloudFormation stacks: webapp-stack and database-stack.","The service has two templates defined: baselime/lambda-logs-basics and workspace-name/template-name.","The service has two variables defined: threshold and frequency. The threshold variable has a default value of 30 and a value of 10 for the prod environment. The frequency variable has a default value of 30mins and a value of 5mins for the prod environment and a value of 0 9 ? * 2#1 * for the dev environment.","This ORL service is for a microservices architecture that is hosted on AWS."]}],[{"l":"ORL Queries","p":["ORL (Observability Reference Language) queries are used to retrieve and analyze data from various datasets in order to gain insights and improve observability of systems and services."]},{"l":"Sample Query Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining queries in Baselime. Use it to get started creating your own queries."]},{"l":"properties","p":["ORL queries have a set of properties that define the query's characteristics and behavior."]},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL query is a string that provides more information about the query. It can include details about the data being queried, the calculations being performed, and any other relevant information.","Example:"]},{"l":"parameters","p":["The parameters of an ORL query define the datasets to query, the calculations to perform on the data, and any filters or groupings to apply."]},{"l":"datasets","p":["The datasets parameter is an array of strings that specify the names of the datasets to query. ORL supports querying multiple datasets simultaneously, allowing you to analyze data from different sources in a single query.","Example:"]},{"i":"filters-optional","l":"filters (optional)","p":[": Greater than",": Less than","!=: Does not equal","=: Equals","=: Greater than or equal to","=: Less than or equal to","DOES_NOT_EXIST: Does not exist (applies to fields that may or may not exist in the data)","DOES_NOT_INCLUDE: Does not include","Example:","EXISTS: Exists (applies to fields that may or may not exist in the data)","Filters can be used to narrow down the data being analyzed and focus on specific events or attributes.","IN: In (applies to arrays only)","INCLUDES: Includes","MATCH_REGEX: Matches a regular expression","NOT_IN: Not in (applies to arrays only)","STARTS_WITH: Starts with (applies to strings only)","The filters parameter is an array of strings that specify conditions to filter the data by. Each string follows this format: 'key operation value', where key is the field to filter on, operation is the comparison operator to use, and value is the value to compare against. ORL supports the following operations:"]},{"i":"calculations-optional","l":"calculations (optional)","p":["The calculations parameter is an array of strings that specify the calculations to perform on the data. ORL supports the following calculations:","COUNT: Counts the number of events.","COUNT_DISTINCT: Counts the number of distinct occurences of a field (applies to strings only).","MAX: returns the maximum value of a field.","MIN: returns the minimum value of a field.","SUM: returns the sum of all values of a field.","AVG: returns the average of all values of a field.","MEDIAN: returns the median of all values of a field.","STDDEV: returns the sample standard deviation of a field.","VARIANCE: returns the sample variance of a field.","P001, P01, P05, P10, P25, P75, P90, P95, P99, P999: return the specified percentile of all values of a field.","Calculations can be used to perform statistical analysis on the data and derive insights such as the average request duration, the maximum response size, or the 95th percentile of request latencies.","Example:"]},{"i":"groupby-optional","l":"groupBy (optional)","p":["The groupBy parameter is an object that specifies how to segment the data by a field. It has the following fields:","value: The field to group the data by","limit: The maximum number of results to return (default: 10)","type: The type of the data field to group by (string, boolean, or number)","orderBy: The calculation to order the results by (default: the first calculation in the query)","order: The order in which to return the results (ASC or DESC, default: DESC)","Grouping the data by a field allows you to segment the results into distinct groups and analyze them separately.","Example:"]},{"i":"needle-optional","l":"needle (optional)","p":["The needle parameter is an object that specifies a search to perform on the data. It has the following fields:","value: The string to search for","matchCase: A boolean indicating whether the search should be case-sensitive (default: false)","isRegex: A boolean indicating whether the search value is a regular expression (default: false)","The needle can be used to find specific set of events or patterns in the data.","Example:"]},{"l":"Example ORL Queries","p":["Here are example ORL queries that combine all of the above properties.","This ORL query retrieves data from the otel traces dataset and performs several calculations on the data. It computes the average request duration, maximum response size, and 95th percentile of request latencies for each user ID in the dataset.","It filters the data to only include user IDs with a request duration greater than 500ms, and limits the results to the top 100 user IDs based on the average request duration. The results are ordered by the average request duration in descending order. The query also searches for the word \"error\" in the data and filters the results based on whether or not the word is present.","This ORL query calculates the total consumed read capacity units for each DynamoDB table in a service. It filters the data to only include events with a metric_name of ConsumedReadCapacityUnits and a unit of Count, and groups the results by TableName. The query returns the top 10 tables with the highest consumed read capacity units."]}],[{"l":"ORL Alerts","p":["ORL (Observability Reference Language) alerts are used to monitor data from various datasets and trigger notifications when specific conditions are met. ORL alerts are defined by a set of properties that specify the characteristics and behavior of the alert. They are based on ORL queries, which are used to retrieve and analyze the data. This allows you to monitor your systems and services and be notified when there are issues or anomalies that require attention.","Note that an alert can only be set for queries that include calculations. It is not possible to set an alert for a query that does not have any calculations."]},{"l":"Sample Alert Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining alerts in Baselime. Use it to get started creating your own alerts."]},{"l":"properties","p":["ORL alerts have a set of properties that define the alert's characteristics and behavior."]},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL alert is a string that provides more information about the alert. It can include details about the data being monitored, the conditions or thresholds being checked, and any other relevant information.","Example:"]},{"i":"enabled-optional","l":"enabled (optional)","p":["The enabled property is a boolean that specifies whether the ORL alert is currently active or inactive. If set to true, the alert will be triggered when the conditions or thresholds are met. If set to false, the alert will be disabled and will not trigger."]},{"l":"parameters","p":["The parameters of an ORL alert define the query to use, the frequency at which the query is run, the window of time over which the query's results are analyzed, and the threshold or condition that triggers the alert."]},{"l":"query","p":["The query parameter is a reference to an ORL query that defines the data to be monitored for the alert. It is specified as a string in the format !ref query_id, where query_id is the id of the ORL query.","Example:"]},{"l":"frequency","p":["The frequency parameter is a string that specifies how often the alert is checked. It can follows the format number time_unit, where number is a positive integer and time_unit is one of the following:","mins/ minutes: minutes","h/ hours: hours","d/ days: days","months: months","y/ years: years","The frequency can also be defined as a cron expression, following the AWS Cron Reference","Examples:","15 10 * * ? *: 10:15 AM (UTC) every day 0 18 ? * MON-FRI *: 6:00 PM Monday through Friday 0 8 1 * ? *: 8:00 AM on the first day of the month 0/10 * ? * MON-FRI *: Every 10 min on weekdays 0/5 8-17 ? * MON-FRI *: Every 5 minutes between 8:00 AM and 5:55 PM weekdays 0 9 ? * 2#1 *: 9:00 AM on the first Monday of each month","The alert is checked at the specified interval, and if the conditions are met, the alert is triggered.","Example:"]},{"l":"window","p":["The window parameter is a string that specifies the time window to consider for the alert. It follows the same format as the frequency parameter, but cannot be defined as a CRON expression.","The alert is only triggered if the conditions are met within the specified time window.","Example:"]},{"l":"threshold","p":["The threshold parameter is a string that specifies the threshold at which the alert is triggered. It is a value that inculdes the comparison and the value (e.g. 5).","The threshold is compared to the result of the first calculation in the query of the alert. If the result meets the specified condition, the alert is triggered.","The following comparison operators are supported:","=: Equals","!=: Does not equal",": Greater than","=: Greater than or equal to",": Less than","=: Less than or equal to","Example:"]},{"l":"channels","p":["The channels parameter is an array of objects that specify the channels to send the alert to. ORL supports the following types of channels:","slack: Sends the alert to a Slack channel email: Sends the alert to an email address pagerduty: Triggers a PagerDuty incident webhook: Sends the alert to a custom webhook URL","Each channel type has its own set of properties that define the behavior of the channel."]},{"l":"slack","p":["The slack channel type sends the alert to a Slack channel. It has the following properties:","targets: An array of strings that specify the Slack channels to send the alert to. Each string should be the name of a Slack channel (e.g. general). Example:","Note that it is necessary to install the Baselime Slack app and follow the Slack onboarding to get alerts on Slack."]},{"l":"email","p":["The email channel type sends the alert to an email address. It has the following properties:","targets: An array of strings that specify the email addresses to send the alert to. Each string should be a valid email address.","Example:"]},{"i":"pagerduty-coming-soon","l":"pagerduty [Coming Soon]","p":["The pagerduty channel type triggers a PagerDuty incident. It has the following properties:","serviceKey: A string that specifies the PagerDuty service key to use for the incident. This key is used to identify the PagerDuty service that the incident should be created in. eventAction: A string that specifies the action to take when creating the PagerDuty incident. Valid values are trigger (default) and resolve. client: A string that specifies the name of the client that the incident should be associated with. This is optional and can be used to provide context for the incident. clientUrl: A string that specifies the URL of the client that the incident should be associated with. This is optional and can be used to provide context for the incident.","Example:"]},{"l":"webhook","p":["The webhook channel type sends the alert to a custom webhook URL. It has the following properties:","url: A string that specifies the URL to send the alert to. method: A string that specifies the HTTP method to use when sending the alert. Valid values are POST (default) and GET. headers: An object that specifies the headers to include in the request. body: A string or object that specifies the body of the request. If a string is provided, it will be sent as-is. If an object is provided, it will be serialized as JSON and sent as the request body. (Coming Soon)","Example:"]},{"l":"Example ORL Alerts","p":["Here are example ORL alerts that combine all of the above properties."]},{"l":"DynamoDB ConsumedWriteCapacityUnits Alert","p":["This alert is triggered when the ConsumedWriteCapacityUnits metric for a DynamoDB table exceeds a specified threshold over a specified time window.","The alert is set to run every 15 minutes and check the metric over the past hour.","If the ConsumedWriteCapacityUnits exceed 5 over the past hour, the alert is triggered.","The alert is sent to a Slack channel called #dynamodb-alerts."]},{"l":"Lambda Timeout Alarm","p":["This alert checks the number of invocations that have timed out for Lambda functions in the service, and triggers if the count exceeds 10 over the past 15 minutes. It sends a notification to a custom webhook URL every 5 minutes."]}],[{"l":"ORL Dashboards","p":["ORL (Observability Reference Language) dashboards are used to visualise data."]},{"l":"Sample Dashboard Spec","p":["Here’s a sample ORL spec that uses all of the supported settings for defining dashboards in Baselime. Use it to get started creating your own dashboards."]},{"l":"properties"},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL dashboard is a string that provides more information about the dashboard.","Example:"]},{"l":"parameters","p":["The parameters of an ORL dashboard define the widgets to display in the dashboard."]},{"l":"widgets","p":["A widget is a graphical element that displays data in a compact and user-friendly way. It can be customized and configured to show specific the results of a specific query in a given view."]},{"l":"query","p":["The query parameter is a reference to an ORL query that defines the data to be displayed in the widget. It is specified as a string in the format !ref query_id, where query_id is the id of the ORL query.","Example:"]},{"l":"view","p":["In ORL, there are three types of widget views:","calculations: This view presents your data as line charts, enabling you to quickly calculate key performance metrics like averages, sums, and counts.","events: This view allows you to explore individual events by filtering and searching, making it a valuable tool for investigating specific occurrences and trends.","traces: This view provides a scatter plot of distributed traces, giving you insights into bottlenecks and latency issues. It's perfect for investigating specific requests or flows and optimizing performance.","Example:"]},{"l":"Example ORL Dashboard","p":["Here is an example ORL dashboard that combine all of the above properties."]}],[{"l":"Installing the Baselime CLI","p":["The Baselime CLI enables you to interact with Baselime and your observability data through the command line."]},{"l":"Installing","p":["Installing with Homebrew","Installing with curl","Installing with npm","Optionally, you can download the latest version of the Baselime CLI binary from the releases page on GitHub.","Download the binary for your operating system and architecture (e.g., baselime_linux_x64 or baselime_darwin_x64).","Unzip the tarball with tar -xf baselime-os-arch-version.tar.gz","Make the binary executable with chmod +x baselime.","Move the binary to a directory in your PATH, such as /usr/local/bin, with mv baselime /usr/local/bin/baselime.","On some systems, you might need to run these commands with sudo."]},{"l":"Verifying the installation","p":["Verify that the Baselime CLI was installed with:"]},{"l":"Authenticating the CLI","p":["Before you can use the Baselime CLI, you must authenticate it with your Baselime account.","To use the Baselime CLI in non-interactive evironments, such as in CI pipelines, set the BASELIME_API_KEY environment variable to your Baselime API key and the CLI will use it for all commands."]},{"l":"Updating the Baselime CLI","p":["To update the Baselime CLI to the latest version, use one of the following commands depending on how you originally installed it:","If you installed with brew, run brew upgrade @baselime/cli","If you installed with curl, run baselime upgrade","If you installed with npm, run npm update -g @baselime/cli"]}],[{"l":"Anonymous Telemetry","p":["Baselime collects completely anonymous telemetry data about general CLI usage. Participation in this anonymous program is optional, and you can opt-out if you'd not like to share any information."]},{"i":"how-do-i-opt-out","l":"How do I opt-out?","p":["You can opt out-by running the following command:","You can re-enable telemetry if you'd like to rejoin the program by running."]},{"i":"why-do-we-collect-telemetry-data","l":"Why do we collect telemetry data?","p":["Telemetry data help up to accurately measure the Baselime CLI feature usage, pain points, and customisation across all developers. This data empowers us to build a better product for more developers.","It also allows us to verify if the improvements we make to the Baselime CLI are having a positive impact on the developer experience."]},{"i":"what-is-being-collected","l":"What is being collected?","p":["We measure the following anonymously:","Command invoked (ie. baselime deploy, baselime query, or baselime tail)","Version of Baselime in use","General machine information (e.g. number of CPUs, macOS/Windows/Linux, whether or not the command was run within CI)","An example telemetry event looks like:","These events are then sent to an endpoint hosted on our side."]},{"i":"what-about-sensitive-data-or-secrets","l":"What about sensitive data or secrets?","p":["We do not collect any metrics which may contain sensitive data.","This includes, but is not limited to: environment variables, file paths, contents of files, logs, or serialized errors."]},{"i":"will-the-telemetry-data-be-shared","l":"Will the telemetry data be shared?","p":["The data we collect is completely anonymous, not traceable to the source, and only meaningful in aggregate form.","No data we collect is personally identifiable.","In the future, we plan to share relevant data with the community through public dashboards or reports."]}],[{"l":"baselime connect","p":["Use the baselime connect command to connect your AWS account to Baselime."]}],[{"l":"baselime console","p":["Use the baselime console command to open the Baselime console."]}],[{"l":"baselime deploy","p":["Use the baselime deploy command to deploy your Observability as Code configurations from your local folder to your Baselime account."]}],[{"l":"baselime iam","p":["Use the baselime iam command to display the currently logged-in user and environment."]}],[{"l":"baselime init","p":["Use the baselime init command to initialize a new service in the current directory."]}],[{"l":"baselime login","p":["Use the baselime login command to log in your Baselime account and select an environment."]}],[{"l":"baselime logout","p":["Use the baselime logout command to log out of Baselime."]}],[{"l":"baselime mark","p":["Use the baselime mark command to create a marker."]}],[{"l":"baselime pull","p":["Use the baselime pull command to update local observability as code configurations with the latest state from the remote systems. If the service has not been initialised locally yet then the cli will prompt you to select a service and download everything to your machine."]}],[{"l":"baselime query","p":["Use the baselime query command to run a query on your telemetry data data."]}],[{"l":"baselime rehydrate","p":["Use the baselime rehydrate to rehydrate Baselime hot storage with data from your Amazon S3 Bucket."]}],[{"l":"baselime report","p":["Use the baselime report command to generate a report based on your observability data and assess the health and performance of your service."]}],[{"l":"baselime tail","p":["Use the baselime tail command to stream events from your telemetry data in real-time."]}],[{"l":"baselime telemetry","p":["Use the baselime telemetry command to manage the usage telemetry data collected by the Baselime CLI."]}],[{"l":"baselime templates","p":["Use the baselime templates command to manage your observability templates."]}],[{"l":"baselime test","p":["Use the baselime test command to check all the alerts in your current service, display the results in the terminal, and output them to a file."]}],[{"l":"baselime upgrade","p":["Use the baselime upgrade command to upgrade the Baselime CLI to the latest version. This method will work only if you installed the Baselime CLI with curl -s https://get.baselime.io | bash."]}],[{"l":"baselime validate","p":["Use the baselime validate command to validate your ORL configuration files."]}],[{"l":"Webhook Integration","p":["The Webhook integration enables you and your team to send POST requests to an http endpoint when an alert is triggered.","To set this up, set [channel].properties.type to webhook and a valid URL in the targets array.","When an alert triggers to a webhook channel, HTTP requests are made to the channel targets using the /POST method. Each request carries an event similar to the example outlines below."]}],[{"i":"baselime--slack-integration","l":"Baselime + Slack Integration","p":["The Baselime integration for Slack gives you and your team full visibility into your applications right in Slack channels, where you can get alerted, investigate incidents, and manage your observability, as a team."]}],[{"l":"Authentication","p":["Install the Baselime integration for Slack.","Once you install the app in your Slack workspace, you can start interacting with Baselime app as a Personal app or access from channels. By default, the Baselime app is enabled in all the public channels. For private channels, you need to explicitly invite /invite @baselime","At this point, your Slack and Baselime user accounts are not linked. You will be prompted to log in Baselime. This is a primary step required to access the app.","Slack welcome message","The primary button will redirect you to the Baselime console where you can login and connect your Slack.","Once this is completed, you will be greeted with a help message \uD83C\uDF89.","Successful login"]}],[{"l":"Automated Alerts","p":["You can set one or multiple Slack channels to receive automated alerts.","Please make sure that the channels defined either are public, or you have manually added the Baselime Slack app to those.","Once configured, When an alert that is configured to send notifications to Slack is triggered, the Baselime Slack app will notify all the configured channels.","Slack alert"]}],[{"l":"Commands","p":["You can use the /baselime command to interact with Baselime straight from Slack."]},{"l":"queries"},{"l":"run","p":["Run a query.","Options","--application: Name of the application","--ref: Query reference","--from: UTC start time - may also be relative eg: 1h, 20mins","--to: UTC end time - may also be relative eg: 1h, 20mins, now","--id: Query id","Result","Slack queries run result"]},{"l":"list","p":["[Coming Soon]"]},{"l":"help","p":["Displays help."]}],[{"l":"Data Security","p":["Baselime is committed to ensuring the security and privacy of our users' data. We have implemented a number of measures to ensure that data is encrypted in transit and at rest, and that it is not accessible from the public internet. Here are some of the key data security features of Baselime:"]},{"l":"Data Encryption","p":["All data transferred to and from Baselime is encrypted in transit using industry-standard protocols such as HTTPS and TLS. In addition, all data is encrypted at rest."]},{"l":"Private VPCs and IAM Roles","p":["Baselime runs in private Virtual Private Clouds (VPCs) and utilizes IAM roles to ensure that data is only accessed by authorized users and processes."]},{"l":"No Public Access","p":["Baselime does not expose any data to the public internet. All data is accessed via secure, authenticated channels."]},{"l":"Modern Best Practices","p":["Baselime follows modern best practices for data security, including regularly updating and patching our systems, implementing network segmentation and access controls, and conducting regular security audits and penetration testing."]},{"l":"Data Scrubbing and Obfuscation","p":["Baselime provides tools for scrubbing and obfuscating sensitive data, such as passwords, secrets, and API keys. Users can block or obfuscate specific keys by dataset using the .baselimeignore file. In addition, Baselime automatically scrubs a predefined list of sensitive fields, including \"password\" and \"secret\".","To learn more about how to use these features to protect your data, see the Baselime Telemetry Data Privacy documentation."]},{"l":"Compliance","p":["We're currently working towards compliance with a number of industry-standard security and privacy frameworks, including GDPR, SOC2 and HIPAA. Please contact us for more information on our compliance status."]},{"l":"Support","p":["If you have any questions or concerns about the security of your data in Baselime, please don't hesitate to contact our support team. We are always here to help!"]}],[{"l":"Telemetry Data Privacy","p":["Baselime is designed to help you observe the health and performance of your applications, and part of that involves collecting telemetry data. To ensure the privacy of your data, Baselime provides a number of features that enable you to control which data is collected and how it is used."]},{"l":"Obfuscating Keys","p":["Baselime enables you to obfuscate certain keys from being ingested into your datasets. This is particularly useful for sensitive information such as passwords, API keys, and other personal data. You can obfuscate keys for a specific dataset by using the baselime obfuscate-key command:","You can also obfuscate keys for multiple datasets at once by specifying the --dataset flag multiple times:","In addition to the command-line interface, you can also use a .baselimeignore file to block keys. The .baselimeignore file should be located in the root of your repository and should contain a list of keys to block, one per line, with the associated dataset. For example:","Moreover, you can obfuscate keys using the Baselime console, in the datasets section.","Keep in mind that obfuscating keys is a one-way process, meaning that once a key has been obfuscated, there is no way to recover the original value. Make sure to carefully consider which keys you want to obfuscate."]},{"i":"baselimeignore-coming-soon","l":".baselimeignore [Coming soon]","p":["The .baselimeignore file allows you to specify keys that should be obfuscated when data is ingested into Baselime. You can use this file to block or obfuscate multiple keys across multiple datasets.","To obfuscate a key, add a line to the .baselimeignore file in the following format:","For example, to obfuscate the data.user.email key in the lambda-logs dataset, you would add the following line to your .baselimeignore file:","Note that the .baselimeignore file should be placed in the root folder of your service and will be applied when you run baselime deploy.","Keep in mind that the .baselimeignore file is only applied to data that is ingested after the .baselimeignore file is pushed. Data that was ingested before the .baselimeignore file was pushed will not be affected."]},{"l":"Automatic scrubbing","p":["access_token","Any nested field in your telemetry data that contains any of these automatically scrubbed keys will be blocked from ingestion by default.","api_key","apikey","auth","Baselime that automatically obfuscate sensitive information from being ingested into the telemetry data by default. This is done to ensure that sensitive data is not accidentally exposed.","credentials","creds","Or using the Baselime console, in the datasets section.","passwd","password","pwd","secret","sourceip","The following keys are automatically scrubbed:","To turn automatic scrubbing on or off for a specific dataset, use the following commands [Coming soon]:"]}],[{"l":"Connectors","p":["Baselime uses connctors to automatically ingest telemetry data from your cloud environments."]}],[{"l":"AWS Connector on Baselime","p":["The AWS Connector allows you to send data from your AWS resources to Baselime. This includes logs, traces, and metrics. By connecting your AWS account to Baselime, you can get a unified view of your architecture, query your data, and set up alerts."]},{"l":"Setting up the AWS Connector","p":["The connector is an automated flow based on a CloudFormation template.","It can be done using the Baselime CLI or through the web console."]},{"l":"Using the CLI","p":["To connect a cloud account to Baselime using the CLI, run the following command in your terminal","Once you've followed the interactive steps, the CLI will generate a CloudFormation template for you to deploy on your AWS account. FOllow the link in your terminal to deploy the temple on your AWS account.","Once deployed, login in your newly connected environment from the CLI.","The interactive prompt should list your newly connected environment.","Within minutes you should get telemetry data flowing through with the command"]},{"l":"Using the Web Console","p":["Navigate to the Baselime web console and login.","Follow the steps on the homescreen to connect a new AWS Account. Baselime will generate a CloudFormation template for you to deploy on your AWS account.","Once the template is deployed on AWS, return to the Baselime web console and refresh the page. You should see the newly connected AWS environment in the list of connected environment.","Within minutes telemetry data from your AWS environment should start displaying in the events streams in the Baselime web console."]},{"l":"Troubleshooting","p":["If you encounter any issues or error when connecting your AWS environment, please don't hesitate to contact us, or join our Slack community where we are always available to support."]},{"l":"CloudFormation Template","p":["The CloudFormation template is open-source and available here."]},{"l":"Your data","p":["Once connected, Baselime will automatically ingest data from your AWS environment. This includes:","Lambda Logs","API Gateway Logs","Cloudtrail Logs","Cloudwatch Metrics","ECS Logs (through fluentd)","Open Telemetry Metrics","X-Ray Traces","Once ingested, the telemetry data is streamed through a Kinesis Firehose to an Amazon S3 bucket in your AWS account for cold storage. There you can access the raw data and use it for your own purposes.","The default retention period of the telemetry data in your bucket is set to 180 days by default."]}],[{"l":"Baselime CDK","p":["Baselime natively integrates with any CDK or SST application to let you set up your applications observability along side your existing infrastructure. By defining your Observability resources along side your application using the CDK you get some awesome productivity benefits like being able to reuse the types and resources created by your IAC directly in your observability code.","These queries can then be used within your alerts and dashboards to paint a complete picture of how your application is performing"]},{"l":"Usage"},{"l":"Setting up the Baselime CDK","p":["The @baselime/cdk package should be installed in the same folder as your cdk application.","Next and store your api key in ssm. You can find your api key in CLI when running","or download it from console.baselime.io","Get your API Key from the Console","Then add the parameter to SSM","If you deploy your application to multiple aws accounts and multiple Baselime environments make sure you add a baselime api key per environment and prefix with the correct stage variable","Finally you can initialise @baselime/cdk in your CDK stack."]},{"l":"Instrumenting your application","p":["Here we have an lambda function that creates a subscription. This is a critical flow within the application. We need to know about any problems asap but also the business metrics that it produces can tell us about harder to detect issues in other parts of the system. Using @baselime/cdk we are going to create a comprehensive set of alerts and a dashboard that gives us insight into the business metrics and performance of our application.","In this lambda function we have added structured json logs to each critical path of this application that give us context of what happens. The lambda runtime will also emit START, END, and REPORT logs that we can use to understand the performance of the application."]},{"l":"Catching Errors","p":["The first thing we want to do is set up alerts for any errors in our new lambda function. Our billing team want to be informed about any subscription related errors separately. This can be done by putting a custom target in the channel once the slack integration is set up","This adds a query and alert to your applications service in Baselime that notify you in slack when ever any log messages contain an error or Unhandled Exceptions caught by the lambda runtime. The alerts will check every 30 minutes for any errors.","The billing team come back and explain that they want to see the errors broken down by customer so they can see which customers where effected by the broken code.","This now shows us exactly the customers that where effected by the outage."]},{"l":"Business Metrics","p":["O11y is not just for code errors. It's also about painting a richer picture of your application. Imagine the scenario where your company doesn't start any new subscriptions in a day. This is an example of where having sensible alerts and dashboards for your system metrics can spot issues in your whole application. i.e. maybe the new marketing campaign emails failed or your signup page has a glitch and the submit button has been set to display:hidden;. It's hard to write tests for every possibility but having alerts on key business metrics can give you useful feedback where tests cannot.","To do this we are going to set up a query that tracks the amount of revenue we are taking per hour.","We can then use this query in dashboards and alerts to show the performance of our business.","An alert that warns us if the subscriptions fall bellow the expected level could warn us of a wide range of problems so we are going to set that up like this f"]},{"l":"Conclusion","p":["Baselime CDK is a useful tool to test in prod. It can help catch issues as they happen so you can take effective corrective action, setting it up in your CDK stack is super effective because its now front of mind when designing the infrastructure for your service."]}],[{"i":"materialized-keys-in-baselime-coming-soon","l":"Materialized Keys in Baselime [Coming Soon]","p":["Materialized keys in Baselime allow you to calculate and create new keys from existing keys in your events. These new keys are based on calculations performed on one or multiple existing keys, which must be of type number.","To create a materialized key, you can use the baselime keys materialize command and specify the key name and calculation you want to perform. For example","This will create a new materialized key called total_revenue which is the result of multiplying the price and quantity keys in your events in the dataset lambda-logs.","Once you have created a materialized key, it will automatically be included in all your events going forward. You can view and manage your materialized keys in the Baselime console.","It's important to note that materialized keys are only recalculated when a new event is ingested, so any changes to the calculation or the underlying keys will not be reflected in historical data."]},{"l":"Syntax","p":["Materialized keys are calculated keys that are generated based on one or multiple existing keys in your events. Materialized keys can only be generated from existing keys of type number.","Materialized keys can only take existing key s of type number.","In Baselime, you can create Materialized Keys through the following operations:"]},{"l":"Basic arithmetic operations","p":["Addition: +","Subtraction: -","Multiplication: *","Division: /"]},{"l":"Advanced operations","p":["Modulo: %","Exponentiation: ^"]},{"l":"Trigonometric operations","p":["Sine: sin","Cosine: cos","Tangent: tan","Inverse Sine: asin","Inverse Cosine: acos","Inverse Tangent: atan"]},{"l":"Creating materialized keys","p":["To create a Materialized Key, you can use the baselime keys materialize command and specify the operation you want to perform on the existing keys. For example:","This command will create a Materialized Key called materialized_key in the dataset lambda-logs that is the result of adding key1 and key2.","You can also use multiple operations and keys to create more complex Materialized Keys. For example:","This command will create a Materialized Key called materialized_key that is the result of adding key1 and key2, and then multiplying that result with key3.","Once you have created a Materialized Key, you can use it just like any other key in your queries and alert rules."]},{"l":"Examples","p":["Here are some examples of how you can use materialized keys in your events:","Calculate the average response time for an API by dividing the total response time by the number of requests","Calculate the total revenue for an e-commerce store by multiplying the price and quantity keys for each order","Calculate the conversion rate for a marketing campaign by dividing the number of conversions by the number of impressions","You can use these materialized keys to set up alerts, create dashboards, and run queries to gain deeper insights into your data."]},{"l":"Troubleshooting","p":["If you encounter any issues with materialized keys, here are a few things you can try:","Check the syntax of your calculation to make sure it is correct","Make sure that all the keys used in your calculation exist in your events and are of type number","If you are using multiple keys, make sure they are all present in every event","If you are still experiencing issues, you can contact the Baselime support team for help."]}]]
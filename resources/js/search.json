[[{"l":"Quickstart Guide","p":["Welcome to Baselime! This quickstart guide will help you get up and running with the platform in just a few steps.","All you need is:","An AWS Account","Permissions to deploy a CloudFormation stack with IAM role.","A deployed application leveraging AWS Lambda and other AWS serverless services","If you do not have a deployed application, you can use our example pokedex.","Baselime in your ecosystem"]},{"i":"step-1-sign-up-for-baselime","l":"Step 1: Sign up for Baselime","p":["To use Baselime, you'll need to sign up for an account. You can sign up for a free account here.","Create a workspace. Typically this will be the name of your organisation.","Start exploring the telemetry data in your workspace sandbox."]},{"i":"step-2-optional-install-the-baselime-cli","l":"Step 2: (Optional) Install the Baselime CLI","p":["The Baselime CLI is a command-line tool that you can use to interact with the platform. Installing the CLI can make it easier to work with Baselime, unlocks Observability as Code and provides additional functionality not available in the web console. To install the CLI, follow the instructions here."]},{"i":"step-3-connect-your-aws-account","l":"Step 3: Connect your AWS account","p":["To start collecting telemetry data from your serverless application, you'll need to connect your AWS account to Baselime. This is done by deploying a CloudFormation template onto your AWS account.","To generate and download the CloudFormation template:","Go to the Baselime Console or run the following command in the Baselime CLI:","Follow the prompts to generate and download the template.","Next, you must deploy the template to your AWS account:","Click the link provided by the Baselime Console or CLI to open the CloudFormation service in your AWS account","Check the box to acknowledge that the template creates IAM roles","Click \"Create stack\" to deploy the stack, making sure to use the correct credentials and region for your AWS account","We've open-sourced the CloudFormation template here.","Once the stack is deployed, telemetry data from your AWS account will be automatically ingested by Baselime and will be available through the various clients.","To verify the connection, invoke any deployed AWS Lambda function in your account and you should see data from it in the Baselime console within seconds. Additionally, you can stream all the events ingested by Baselime directly in your terminal using the baselime tail command.","If you do not complete any of the above steps, Baselime will not be able to ingest data from your AWS account.","If you do not see any data in the Baselime UI or using the baselime tail command within seconds of completing the above steps, something went wrong. Please contact us."]},{"i":"step-4-explore-the-data","l":"Step 4: Explore the data","p":["Once your AWS Account is connected, you can start exploring the telemetry data it generates. You can use the web console or the CLI (if installed) to access and analyze the data.","Baselime ingests and indexes every field and nested field in your telemetry data."]},{"l":"Accessing data in the web console","p":["To explore the data in the web console:","Go to the Baselime Console and sign in with your account","Select your environment from the list of environments","Use the various filters and tools in the console to slice and dice the data, such as:","Filtering by resource type, key-value pair, operation type, or time range","Searching for specific strings or regexes in the data","Viewing the trace data for a specific request or operation","Viewing the logs and metrics for a specific resource or operation","Segmenting the results by specific field or nested field"]},{"l":"Accessing data in the CLI","p":["To access the data in the CLI:","If you installed the CLI, you can use the baselime query command to interactively explore the data. Here's how it works:","If you haven't already done so, sign in to the CLI using the baselime login command","Run the following command:","Select the service you want to query","Select one of your saved queries or interactively build a query","Enter the start and end time for the query (optional - defaults to the past hour)","The command will output a table with the results of the query and a unique URL that you can share with your team"]},{"i":"step-5-implement-observability-as-code","l":"Step 5: Implement Observability as Code","p":["Baselime enables you to define and manage your observability configurations, such as queries and alert rules, using code that can be stored and versioned in your source control repository. This is known as Observability as Code.","To implement observability as code:","Use the baselime init command to create a .baselime folder in your repository and generate an index.yaml file","Answer the prompts to specify metadata about your service, such as its name, description, and details about the cloud infrastructure of the service","The baselime init command will create one or more configuration files in the .baselime folder using the Baselime Observability Reference Language (ORL). The ORL provides a set of commands and syntax for defining configurations such as queries, alert rules, and dashboards.","Use the baselime push command to apply your configurations to your environment.","For example:","This will apply your observability configurations to your environment, replacing any existing configurations.","To verify that your configurations have been applied, use the baselime plan command to compare your local configuration files with the ones deployed in your environment.","For more information about the Baselime ORL and how to use it, check out the Reference Guide."]},{"i":"step-6-set-up-integrations","l":"Step 6: Set up integrations","p":["Baselime offers a variety of integrations with popular tools and services to help you get the most out of your observability data.","To set up an integration:","Go to the Integrations page in the Baselime console","Choose the integration you want to set up from the list","Follow the prompts to configure the integration. This may include providing credentials or setting up webhooks.","Save your changes to complete the setup","For more information about the available integrations and how to use them, check out the Integrations Guide."]},{"l":"Guides","p":["Sending Data: Learn how to ingest telemetry data from your serverless applications","Analyzing Data: Discover how to use the various interfaces provided by Baselime to analyze and understand your data","Integrations: Find out how to connect Baselime with your favorite tools"]},{"l":"Reference","p":["Reference Guide: Learn about the Baselime Observability Reference Language (ORL) and how to use it to define observability configurations","CLI Reference: Complete reference for the Baselime command-line interface","API Reference: Detailed documentation of the Baselime API"]},{"l":"Community","p":["Join the Baselime community to get help with using the platform, share your own experiences, and stay up-to-date with the latest developments.","Slack: Join our Slack community to connect with other Baselime users and get real-time support from the Baselime team","Blog: Read about the latest features, best practices, and more from the Baselime team","Social media: Follow us on Twitter, LinkedIn, and YouTube to stay up-to-date with the latest news and updates from Baselime","We look forward to connecting with you!"]}],[{"l":"Baselime Concepts","p":["Before we continue, we need to learn some of the core Baselime concepts: Observability as Code, datasets, namespaces, and services. They are crucial to understanding how the platform functions. In this page, you will learn about these concepts and how they can help you effectively use Baselime to monitor and troubleshoot your serverless applications.","Baselime calculations, events, and traces"]},{"l":"Observability as Code","p":["Observability as Code is a way to manage your observability configurations using code instead of a graphical user interface. This makes it easier to version control and deploy your configurations, and collaborate with your team.","To use Observability as Code in Baselime, you'll use the Baselime Observability Reference Language (ORL) to define your configurations in a .baselime folder in your code repository. The .baselime folder contains YAML files that define different aspects of your observability configurations, such as alerts, dashboards, and integrations.","To get started with ORL, you can use the Baselime CLI and commands like baselime init, baselime push, baselime pull or baselime plan. These commands help you manage your observability configurations from your terminal or CI/CD pipelines.","Observability as code change","Overall, Observability as Code makes it easier to manage and collaborate on observability configurations using code, helping you automate and streamline your observability practices."]},{"l":"Environments","p":["In Baselime, an Environment represents your whole system. Environments can contain the data for many AWS accounts and regions enabling you to unify your O11y. We recommend you create an environment for each instance of your system. I.E. seperate environments for prod, testing ect. This keeps your test and prod data seperate in baselime making it easy to find what you are looking for.","When using the CLI you can setup multiple local profiles for each environment by running `baselime login --profile 'new profile name goes here' and selecting the environment you want. Every baselime cli command supports the --profile flag to use the environment you want.","Diagram showing the relationship between environments and services"]},{"l":"Services","p":["In Baselime, a service is a logical grouping of cloud resources automatically discovered from your Cloudformation Templates and typically corresponds to a repository in your version control system or a folder in your mono-repo.","For example, you might have a service called order-management with multiple Lambda functions, databases, and event queues. In Baselime you can run queries within the service, and query only the data emitted by one of the cloud resources that is part of the service, without the clutter of the rest of your architecture. This enables you to isolate, manage and query the observability for each component of your architecture separately."]},{"l":"Namespaces","p":["In Baselime, a namespace is a group of data for a single or type of resource. Namespaces are automatically created on ingestion and are dataset specific. For lambda-logs, otel, and x-ray, the namespace will be the lambda function name. For metrics and cloudtrail it's the resource type, i.e. AWS/SQS."]}],[{"l":"Sending Data to Baselime","p":["Baselime supports a variety of data sources, including logs, metrics, traces, and events. You can start sending your data to Baselime and gain valuable insights into the performance and reliability of your serverless applications with a few steps.","You can send your telemetry data to Baselime with either of:","Overview Overview Overview Overview Overview Overview Overview Overview Overview"]},{"l":"Validation","p":["Baselime has a size limit for events of 128kb. This size limit helps ensure that the ingestion process is efficient and that the data stored in Baselime is manageable and fast to query. If an event exceeds this 128kb size limit, it will not be ingested into Baselime."]},{"l":"Sending Semi-Structured Logs to Baselime","p":["Semi-structured logs are logs that are not in the strict JSON format, but still contain structured data that can be extracted.These logs contain a mixture of structured and unstructured data, making them difficult to parse and analyze. Fortunately, Baselime has built-in mechanisms to parse and extract relevant data from semi-structured logs.","Baselime will automatically detect log events that contain JSON data, but are prepended or appended by a generic string.","The generic string will be wrapped in a message attribute, and the JSON data will be wrapped in a data attribute. This enables you to easily extract and analyze relevant data from semi-structured logs."]},{"l":"Examples","p":["Here are examples of automatic semi-structured logs detection."]}],[{"l":"AWS Lambda Logs","p":["Once you connect your AWS account to Baselime, Baselime automatically create CloudWatch Logs subscription filters to automatically ingest logs from your AWS Lambda functions."]},{"l":"How it works","p":["The following diagram illustrates the process for sending AWS Lambda logs to Baselime. Once Baselime is connected to an AWS Account, it automatically creates Logs subscription filters for all the Lambda functions in the account. Log subscription filters enable Baselime to asynchronously ingest logs from the Lambda functions through CloudWatch, without any impact on the performance of the Lambda functions.","Sending Lambda Logs to Baselime","Moreover, Baselime automatically creates new subscription filters for newly deployed Lambda functions, thanks to it CloudTrail integration. Baselime listens to new Lambda events in CloudTrail and appropriately creates subscription filters for newly created Lambda functions.","This method however has one drawback: Lambda logs appear in Baselime with a delay (typically less than 3 seconds) as a results of the logs going through CloudWatch before being piped to Baselime.","An alternative method to ingesting AWS Lambda logs is with the use of the Baselime Lambda Extension."]},{"l":"Discovered Keys","p":["Baselime automatically discovers key - value pairs from your AWS Lambda logs. This enables you to run complex queries and setup alerts on data that otherwise would be difficult to work with from the AWS Lambda service. For instance, from the discovered keys from the Lambda logs, it's possible to set alerts on the maximum memory used by lambda functions during execution, compared to the amount of memory they are assigned at deployment time."]},{"l":"Instrumentation","p":["By default this instrumentation is agentless but if you want to see your request and response data in baselime you can either add the following logs to your lambdas","or alternatively install our SDK.","Its just a 2.5kb javascript file with 0 dependencies so adds minimal overhead to your lambda function."]},{"l":"Usage","p":["It also exports a middy middleware"]},{"l":"Logging best practices","p":["Baselime works best when you log JSON, The @baselime/lambda-logger helps you do this but you can also use Lambda Power Tools for your favourite language.","and with the baselime logger","Its also important you format errors correctly to make sure you can find all the information you need to make your queries.","with the baselime logger this is simpler"]},{"l":"Lambda Discovered Keys","p":["The Lambda service automatically writes logs at the start and end of every function invocation. These logs are parsed as events in Baselime, and keys are automatically discovered from those messages."]},{"l":"START Log Message","p":["The following keys are discovered from the START message:","@type: is always START","@requestId: the request ID of the Lambda invocation","@version: the invoked version of the Lambda function"]},{"l":"END Log Message","p":["The following keys are discovered from the END message:","@type: is always END","@requestId: the request ID of the Lambda invocation"]},{"l":"REPORT Log Message","p":["The following keys are discovered from the REPORT message:","@type: is always REPORT","@requestId: the request ID of the Lambda invocation","@duration: the duration in milliseconds","@billedDuration: the billed duration in milliseconds","@memorySize: the total memory available to the invocation, in MB","@maxMemoryUsed: the max memory used, in MB","@initDuration: the duration of the lambda initialisation in milliseconds (cold starts)","If the Lambda function is instrumented with XRAY, additional keys are discovered:","@xRAYTraceId: the XRAY trace ID","@segmentId: the XRAY segment ID","@sampled: always true"]},{"l":"Timeout Invocations","p":["If your async Lambda invocation times out, Additional keys are automatically discovered:","@timedOut: always true","@timeout: the duration after which the invocation timed-out in seconds","@message: always Task timed out after {@timeout} seconds","@timestamp: the timestamp at the moment the invocation timed out."]},{"i":"consolelog-log-message","l":"console.log Log Message","p":["We recommend writing directly to stdout and stderr from your Lambda functions. For Node.js environments, AWS Lambda uses a modified version of console.log(and other console logging functions) to write to stdout and stderr. These add fields to the log message which are parsed in discovered keys.","@timestamp: the timestamp at the moment the log message was written","@requestId: the request ID of the Lambda invocation","LogLevel: the log level ( INFO, DEBUG, WARN, ERROR)","@message: the message.","If the message in @message is a valid JSON object, Baselime will parse it, otherwise it will be considered a string."]},{"l":"Troubleshooting","p":["If you're having trouble sending data from your AWS Lambda logs to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as CloudWatch Metrics or CloudTrail Events","Check that your Lambda functions are not already using the maximum number of subscription filters allowed per log group. AWS limits each log group to 2 subscription filters at most. If you're already at the limit, you can remove subscription filters with the cloudwatch-subscription-filters-remover to delete the ones you don't need anymore.","Make sure that your AWS Lambda functions are being invoked and you can view the logs in the CloudWatch section of the AWS Console"]}],[{"l":"Amazon ECS Fargate Container Logs","p":["This page describes how to collect application container logs from Amazon ECS clusters launched with AWS Fargate using AWS FireLens. This method can also be used to collect ECS clusters with EC2 containers."]},{"l":"How it works","p":["The following diagram illustrates the process for sending container logs from ECS containers running on AWS Fargate or EC2 to Baselime using the FireLens log driver.","FireLens is an Amazon ECS native log router that enables you to send logs from your containerized applications to different destinations, including Baselime. By adding the FireLens sidecar to your task definitions, you can easily configure and route your container logs to different destinations without modifying your application code.","Sending ECS Logs to Baselime","Each of your ECS tasks can take a sidecar container running the FireLens log driver that will forward all the logs from the containers to Baselime."]},{"l":"Configuring your ECS Tasks"},{"i":"step-1-obtaining-your-baselime-api-key","l":"Step 1: Obtaining your Baselime API Key","p":["You can get your Baselime API key using the Baselime CLI. Ensure you have downloaded the Baselime CLI and logged in your environment. To get your Baselime API Key, run the following command","Copy the Baselime API Key in the output of the command and keep it safe. In the following instructions we will use BASELIME_API_KEY to refer to your Baselime API key, make sure to replace it with the key you copied from the output of the command."]},{"i":"step-2-adding-the-firelens-sidecar-to-your-task-definitions","l":"Step 2: Adding the FireLens sidecar to your task definitions","p":["Adding the FireLens sidecar to your task definitions is a straightforward process that can be accomplished using various Infrastructure as Code solutions or manually in the console.","Add the Baselime ECS endpoint to your FireLens configuration:","Endpoint ecs-logs-ingest.baselime.io","Header: x-api-key BASELIME_API_KEY"]},{"l":"Using Terraform"},{"l":"Using AWS CDK for TypeScript"},{"l":"Directly in AWS console"},{"l":"Troubleshooting","p":["If you're having trouble sending data from your AWS ECS logs to Baselime, here are a few things to check:","Verify that you're using the correct API key and host in the FireLens configuration","Make sure that your containers are receiving traffic and are writing logs to either stdout or stderr","Check the logs of the FireLens container to look for any anomaly"]}],[{"l":"Amazon API Gateway Logs","p":["Once you connect your AWS account to Baselime, Baselime automatically create CloudWatch Logs subscription filters to automatically ingest logs from your Amazon API Gateways."]},{"l":"Setup","p":["Baselime can ingest logs only for API Gateways where access logs are appropriately configured.","We recommend this configuration for API Gateway logs:","It is possible to enable API Gateway access logs from your favourite Infrastructure as Code tool, using the CLI or in the AWS console. Below is an example of how to enable API Gateway logs using the serverless framework."]},{"l":"How it works","p":["The following diagram illustrates the process for sending Amazon API Gateway logs to Baselime. Once Baselime is connected to an AWS Account, it automatically creates Logs subscription filters for all the API Gateways in the account.","Sending Lambda Logs to Baselime","Moreover, Baselime automatically creates new subscription filters for newly deployed API Gateways, thanks to it CloudTrail integration. Baselime listens to new API Gateway events in CloudTrail and appropriately creates subscription filters for newly created API Gateways functions."]}],[{"l":"Lambda Telemetry Extension","p":["Instrumenting AWS Lambda functions with Baselime is straightforward using our Lambda Extension. The Lambda Extension listens to invocation events and collects telemetry data, such as logs and runtime metrics. Once collected, the telemetry data is sent to Baselime for storage, analysis, and visualization. In this section, we'll walk you through the process of instrumenting your Lambda functions with the Baselime Lambda Extension.","Before getting started you'll need to make sure that you have your Baselime API key ready. You can get it by running the following command using the Baselime CLI."]},{"l":"How it works","p":["The Baselime Lambda Extension is language agonistic and is compressed as a single binary, such that it minimises its impact cold-starts and performance.","The diagram below illustrates how the Baselime Lambda Extension works within your architecture.","Usingt the Baselime Lambda Extension","All the telemetry data from your Lambda function is collected asynchronously from your invocation, and sent to the Baselime backend in a separate process from your invocation."]},{"l":"Instrumenting","p":["ap-northeast-1","ap-south-1","ap-southeast-1","ap-southeast-2","Architecture","arm64","ARN","arn:aws:lambda:ap-northeast-1:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:ap-northeast-1:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:ap-south-1:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:ap-south-1:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:ap-southeast-1:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:ap-southeast-1:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:ap-southeast-2:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:ap-southeast-2:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:ca-central-1:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:ca-central-1:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:eu-west-1:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:eu-west-1:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:eu-west-2:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:eu-west-2:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:us-central-1:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:us-central-1:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:us-east-1:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:us-east-1:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:us-east-2:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:us-east-2:097948374213:layer:baselime-telemetry-extension-x86_64:3","arn:aws:lambda:us-west-2:097948374213:layer:baselime-telemetry-extension-arm64:3","arn:aws:lambda:us-west-2:097948374213:layer:baselime-telemetry-extension-x86_64:3","ca-central-1","eu-west-1","eu-west-2","It is necessary to add the Baselime API key to the extension as an environment variable. The example below shows the process with the serverless framework.","Region","To instrument your AWS Lambda Functions with the Baselime Lambda Extension, we recommend using your Infrastructure as Code tool of choice, and add the Extension as a Lambda Layer.","us-central-1","us-east-1","us-east-2","us-west-2","Where the BASELIME_KEY is your Baselime API Key and the BASELIME_LAMBDA_LAYER_ARN is the ARN of the Baselime Layer in your region. The table below outlines the list of Baselime Layer ARNs for every supported region.","x86-64"]}],[{"l":"OpenTelemetry Traces","p":["If your codebase is already instrumented with OpenTelemetry, you can start sending us your tracing data today.","Add the Baselime OTel endpoint to your exporter:","Endpoint https://otel.baselime.io/v1/","Header: x-api-key: YOUR_BASELIME_API_KEY","You can get your Baselime API key from the Baselime CLI with","If you have not instrumented your codebase with OTel yet, you can use the Baselime Node.js OTel tracer as outlined below. We're currently working on lightweight OTel tracers for other languages."]},{"i":"lambda-opentelemetry-for-node-js","l":"\uD83C\uDFB8 Lambda Opentelemetry for Node.JS","p":["The @baselime/lambda-node-opentelemetry package instruments your lambda functions and automatically ships OTEL compatible trace data to Baselime. This is the most powerful and flexible way to instrument your node service.","The downside of this node tracer is it adds a small performance hit to each lambda invocation. We are working as hard as possible to minimise this but for now if this matters to you use our x-ray integration instead."]},{"l":"Manual Installation","p":["--require @baselime/lambda-node-opentelemetry","Add the following environment variables to your service","BASELIME_NAMESPACE","BASELIME_OTEL_KEY","Description","Example","Get the baselime key using our cli","Get this key from the cli running baselime iam","If you use export const export function or export default for your handler you need to rename it to a cjs export like module.exports = or exports.handler =. Even if you use esbuild. We are tracking issues in esbuild and open-telemetry and are looking to see how we can help out.","Install the @baselime/lambda-node-opentelemetry package","Key","NODE_OPTIONS","nora-is-the-cutest-baselime-dog","Preloads the tracing sdk at startup","prod-users","The name of the service the traces belong to","You need to make sure the lambda-wrapper file is included in the .zip file that is used by aws-lambda. The exact steps depend on the packaging step of the framework you are using."]},{"l":"Architect","p":["Copy the lambda-wrapper.js file from our node modules to the shared folder of your architect project, this way it is automatically included in all of your lambdas bundles.","Add the environment variables to your architect project","Watch out for the '--' in the NODE_OPTIONS command. This is required to escape options parsing. This totally didn't frustrate me for a whole day! :D"]},{"l":"Serverless","p":["By default the serverless framework includes your whole node_module directory in the .zip file. If you are using the serverless-esbuild plugin to avoid this then you need to add the following configuration to your project.","https://www.serverless.com/framework/docs/providers/aws/guide/packaging","Add the following line to the package.patterns block of your serverless.yml","Example","Add the following environment variables"]},{"l":"SST","p":["Fun fact Baselime is built using SST :)","Copy the lambda-wrapper file to your srcPath directory","Then add the default props to include the wrapper in your bundle and add your environment variables"]},{"i":"automatic-instrumentation-wip","l":"Automatic Instrumentation [WIP]","p":["Lambda Extension coming soon"]},{"l":"Send data to another OpenTelemetry Backend","p":["Add the environment variable COLLECTOR_URL to send the spans somewhere else."]}],[{"l":"AWS X-Ray Traces","p":["AWS X-Ray enables developers to gather traces across their distributed services. In order to gain visibility into their applications, developers can use X-Ray to trace requests as they travel through their application, and collect data about the performance and errors of their application.","Baselime enables you to easily ingest this tracing data and make it available for analysis and troubleshooting."]},{"l":"How it works","p":["To start ingesting traces from AWS X-Ray to Baselime, you'll need to connect your AWS account to Baselime. Once connected, Baselime will periodically poll your AWS account for new traces and automatically ingest them into your Baselime dataset.","Sending X-Ray Traces to Baselime"]},{"l":"Troubleshooting","p":["If you're having trouble sending data from AWS X-Ray to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as CloudWatch Metrics or CloudTrail Events","Check that the Baselime IAM user has the appropriate permissions to access X-Ray","Make sure that your applications emit X-Ray traces and you can view the traces in the X-Ray section of the AWS Console"]}],[{"l":"CloudTrail","p":["Baselime automatically ingests CloudTrail events when you connect your AWS account. Baselime will automatically create a new CloudTrail and S3 bucket for you, and configure them to send data to your Baselime account. No additional setup is required on your end.","Once connected, CloudTrail events will be sent to Baselime and made available for querying."]},{"i":"why-cloudtrail-","l":"Why CloudTrail ?","p":["CloudTrail is a service provided by AWS that records API activity in your AWS account. This data can be used to track changes to your resources, troubleshoot issues, and improve security.","By sending CloudTrail events to Baselime, you can use our query and visualization tools to more easily analyze and understand your API activity. You can also set up alerts to be notified of specific API activity or trends.","With CloudTrail events in Baselime, you can gain a deeper understanding of your AWS API activity and use that knowledge to improve the security and reliability of your applications."]},{"l":"How it works","p":["CloudTrail writes trail data periodically in a pre-configured S3 bucket in your AWS account. Once the data is written, it signals to an SNS topic that the trail is written.","Baselime configures this SNS to invoke a Lambda function that reads the data from the S3 bucket and ingests it in the Baselime backend.","Sending CloudTrail data to Baselime"]},{"l":"CloudTrail management events","p":["CloudTrail events fall into multiple categories, and Baselime automatically ingests CloudTrail management events. Please refer to the complete CloudTrail docs for further details on the CloudTrail concepts."]}],[{"l":"CloudWatch Metrics","p":["Once you connect your AWS account to Baselime, the necessary resources including a CloudWatch Metrics Stream and a Kinesis Firehose will be automatically created and configured for you. This means that you don't need to do any additional setup or configuration."]},{"i":"why-cloudwatch-metrics-","l":"Why CloudWatch Metrics ?","p":["CloudWatch is a monitoring service provided by Amazon Web Services (AWS) that enables you to collect and track metrics and log data for your AWS resources and applications. Metrics are important as they provide insight into the performance and behavior of your serverless applications and the underlying infrastructure.","CloudWatch Metrics can help you identify issues such as high error rates and latencies, which can help improve the overall reliability and scalability of your applications.","Moreover, CloudWatch Metrics cover all aspects of your serverless architecture automatically, from DynamoDB tables to S3 buckets and SQS Queues."]},{"l":"How it works","p":["The following diagram illustrates the process for sending Amazon CloudWatch metrics to Baselime. Once Baselime is connected to an AWS Account, it automatically created the telemetry pipeline for ingesting CloudWatch metrics into Baselime. The pipeline comprises a CloudWatch Metrics Stream, a Kinesis Firehose and all IAM roles and permissions associated.","This pipeline will automatically continiously send metrics from your AWS account to Baselime.","Sending CloudWatch Metrics to Baselime"]},{"l":"Custom CloudWatch Metrics","p":["Baselime automatically ingests all metrics published to CloudWatch. This includes both standard CloudWatch metrics and any custom metrics that you may have created.","There is no need to manually configure or set up anything to start ingesting custom CloudWatch metrics. Once your AWS account is connected, all metrics will be available for querying in Baselime."]},{"l":"Querying CloudWatch Metrics","p":["Once your AWS account is connected to Baselime, you can use any of the our clients to visualize and query your CloudWatch Metrics. You'll have access to all the metrics available in your AWS account, and you can use the Observability Reference Language (ORL) to filter and aggregate the data in near real-time."]},{"l":"Troubleshooting","p":["If you're having trouble sending metrics from CloudWatch to Baselime, here are a few things to check:","Verify that your AWS account is correctly connected to Baselime and you receive data in other datasets such as AWS Lambda Logs or CloudTrail Events","Check that the Kinesis Firehose created in your AWS account as part of the Baselime connection has the appropriate API key to connect with the Baselime backend. If the API key is missing, please contact us."]}],[{"l":"Events API","p":["Baselime provides an HTTP events API which enables developers to send data to Baselime by making a POST request to the API endpoint. This enables developers to send data directly from their applications or services to Baselime, rather than using a logging or monitoring service as an intermediary."]},{"l":"Request Format","p":["Each request ingests a batch of events into Baselime. Events are part of the request body. Baselime supports Content-Type application/json.","The request body must be an array of JSON objects. Any element of the array that cannot be parsed as valid JSON will be rejected.","Requests must be made to the /dataset/service/namespace route:","dataset is the name of the dataset that the events should be ingested into. You can either use an existing dataset or create a new one using the Baselime CLI.","service is the service that the events belong to. If the service doesn't exist beforehand, the events can be queried through the default service. Once you create the service (in the web console or using the Baselime CLI), the events will be available from the service too.","namespace is the namespace within the dataset that the events should be ingested into. The namespace is created automatically for you when events are received, if it didn't exist beforehand."]},{"l":"Authentication","p":["The HTTP API requires a valid Baselime API key to be sent in the x-api-key request header.","You can obtain your API key using the Baselime CLI."]},{"l":"Validation","p":["The HTTP API validates the provided events and returns a 400 Bad Request status code if any of the events fail validation with a list of all the events that failed validation. If some events pass validation and others fail, we will ingest the events that pass validation. If you encounter a 400 Bad Request error when submitting events to the HTTP API, the events that failed validation will be listed in the body of the request under the invalid key."]},{"l":"High-level requirements","p":["Baselime accepts up to 6MB of uncompressed data per request","Each event must be a properly formatted JSON","Each event must be smaller than 128kb of uncompressed JSON"]},{"l":"API Response codes","p":["Baselime returns a 202 response for all valid requests to the HTTP Events API, and a range on of non- 200 responses for errors.","We welcome feeback on API responses and error messages. Reach out to us in our Slack community with any request or suggestion you may have."]},{"l":"Successfull responses","p":["Status Code","Body","Meaning","202","{message: Request Accepted}","All the events were successfully queued for ingestion"]},{"l":"Failure responses","p":["Status Code","Body","Meaning","405","{message: Method Not Allowed}","The HTTP method is now allowed","401","{message: Unauthorised}","Missing or invalid API Key","400","{message: Bad Request}","- Missing or invalid path parameters ( v1, dataset, service or namespace) - Unable to parse the request body as valid JSON- Empty request body - At least one of the events exceed the 128kb size limit - At least one of the events could not be parsed as valid JSON","500","{message: Internal Error}","An unexpected error occured"]}],[{"l":"Analyzing Data in Baselime","p":["Baselime provides a variety of tools and clients for analyzing your data and gaining insights into your serverless system's performance and behavior.","Queries","ALerts","Live tailing","Snapshots","Reports"]}],[{"l":"Queries","p":["Queries are the primary way of interacting with your data in Baselime."]},{"l":"Queries in the Console","p":["You can run queries in the Baselime console by navigating to your service and clicking on the New Query button. This will bring up the Visual Query Editor. You can edit the query visually, but also switch to the embedded code editor to write your query using the Observability Reference Language (ORL).","To execute a query, simply click the Run Query button. The query results will be displayed in visually and in a table below the editor."]},{"l":"Queries in the CLI","p":["You can also run queries using the Baselime CLI. To do so, use the baselime query command.","Use the baselime query without any flags to enter interactive mode where you can specify all the arguments of your query interactively.","You can also run saved queries using the CLI, either in interactive mode or by passing the arguments as flags","You can also save your query results to a file. Use the --format to print the results of the query in JSON, and pipe them to a file.","For more advanced usage of the baselime query command, please refer to the CLI reference."]}],[{"l":"Alerts","p":["Baselime's alerting feature allows you to set up notifications for when certain conditions are met in your telemetry data. This can be helpful for detecting and responding to issues in your system in real-time."]},{"l":"Setting up alerts","p":["To set up an alert, you will need to specify a query and a threshold. When the result of the query meets the conditions the threshold, the alert will be triggered. You must also specify the frequency to check the query, and time window to consider for the alert.","You can set up alerts using the Baselime CLI with Observability as Code using the Observability Reference Language or the web console. Here is an example of how to set up an alert with ORL:","To create this alert, add it to a any .yml file in your .baselime folder. If you don't have a .baselime folder for your service, create it with baselime init.","Once you have the .baselime folder configured, run the following command to create your alert:"]},{"l":"Receiving alerts","p":["When an alert is triggered, you can choose to receive notifications through a variety of channels, such as email, Slack, or PagerDuty (coming soon)."]},{"l":"Managing alerts","p":["You can view and manage your alerts through the CLI or the web console. Here are some example CLI commands for managing alerts:"]},{"l":"Tips for effective alerting","p":["Make sure to set appropriate thresholds for your alerts. Setting the threshold too low may result in false positives, while setting it too high may result in missed issues.","Keep alerts specific and actionable: Alerts should be specific and provide clear instructions on what action to take.","Set up alerts for the right things: Make sure to set up alerts for the most important issues that need immediate attention.","Use multiple alerting methods: Use a combination of Slack, email, and webhooks to ensure that you are notified of important issues in a timely manner.","Use alert suppression: Silence repeated alerts to avoid alert fatigue and ensure that you are only notified of important issues.","Use alert grouping: Use alert grouping to group related alerts together, making it easier to triage and resolve issues.","Consider using webhook alerts to build self-healing systems","Test your alerts to ensure they are working as expected.","Use alert analytics: Use alert analytics to analyze the effectiveness of your alerting strategy and make improvements where necessary.","Regularly review and update alert thresholds and configurations to ensure they are still relevant and effective."]}],[{"l":"Tailing your data","p":["The baselime tail command enables you to stream telemetry data in real time to your terminal. This can be useful for debugging or quickly checking the status of your services.","By default, the baselime tail command will stream all telemetry data for your Baselime environment. You can further filter the data by adding query parameters, such as:","This will only show the events where data.user.id is 123456 and the word error appears in the event.","You can also specify a time range for the data being streamed:","Alternatively, you can define the timerange in relative format","This will stream telemetry data between the specified start and end times.","The baselime tail command can be a useful tool for quickly checking the status of your application and identifying any issues that may be occurring."]}],[{"l":"Snapshots","p":["Snapshots enable you to capture the current state of all your alerts in a service at a given point in time. This can be useful for debugging purposes or for creating a record of the health of your system at a specific moment."]},{"l":"Using Snapshots","p":["To create a snapshot, simply run the baselime snapshot command in your terminal. This will create a snapshot of the current state of all alerts in the current service, display the results in the terminal, and output them to a file in JSON format. You can specify the output file path using the --out-file flag.","This will create a snapshot of the alerts in the my-service service and save it to the snapshot.json file."]},{"i":"viewing-snapshots-coming-soon","l":"Viewing Snapshots (Coming soon)","p":["You can view your snapshots in the Baselime console under the \"Snapshots\" tab in the navigation menu. From here, you can view the details of each snapshot, including the time it was created, the service it was created for, and the state of each alert at that time."]},{"l":"Tips for Effective Snapshotting","p":["Use snapshots as a debugging tool to help you understand the state of your system at a specific moment in time","Use snapshots to compare the state of your alerts before and after making changes to your service.","Save snapshots for compliance purposes."]}],[{"l":"Reports","p":["Baselime reports allow you to monitor the health of your services and get notified when issues arise. Reports can be triggered on-demand, and can be sent to third party integrations such as Slack or Github."]},{"i":"when-to-use-reports-","l":"When to use reports ?","p":["Baselime reports are a powerful tool that enable your team to compare the state of a service before and after making changes. By incorporating Baselime reports into your CI/CD pipeline, your team can see the impact of their changes on alerts, dashboards, and SLOs in production. This not only improves the reliability of your deployments, but it also enables your team to build self-healing systems. For example, if a report after deployment is negative, your team can roll back or roll forward to ensure the stability of your service.","Here is an example of how you can use the baselime report command in a GitHub Action to compare the state of a service before and after a deployment:","This workflow will take a snapshot with baselime report github before and after running the deployment script ( npm run deploy). The report will be posted on the commit that triggered the workflow as a comment. It contains the current state of your service, including alerts, dashboards, and SLOs (coming soon). By comparing the two snapshots, you can see how the deployment affected your service and take appropriate action if needed."]},{"l":"Running a report","p":["To run a report, use the baselime report command. By default, this will create a snapshot of all the alerts in the current service, display the results in the terminal, and output them to a file.","To generate and publish a report, run the baselime report command followed by the name of the integration you want to publish the report to:","For example, to publish a report to GitHub, you would run:"]},{"i":"slos-and-dashboards-coming-soon","l":"SLOs and Dashboards (Coming Soon)","p":["In the future, the report command will also include support for publishing Service Level Objectives (SLOs) and creating dashboards to visualize your report data. Stay tuned for updates!"]}],[{"i":"observability-reference-language-orl","l":"Observability Reference Language (ORL)","p":["This is the documentation for Baselime's Observability as Code configurations using the Observability Reference Language (ORL).","ORL (Observability Reference Language) is a language used to express queries for observability telemetry data. ORL queries can be used to extract insights from logs, metrics, and traces data sources. ORL queries are defined by a set of parameters that specify the data sources, filters, and calculations to be performed on the data. The result of an ORL query is a set of events that match the criteria defined in the query, optionally aggregated by calculations.","ORL configurations are defined in YAML files.","Generally, ORL files live in the .baselime folder in the root directory of a given project. We refer to this folder as .baselime elsewhere in the documentation, although users can rename it at will.","Multiple integrations and connectors with your favourite Infrastructure as Code platforms are currently being developed."]}],[{"l":"ORL Services","p":["ORL (Observability Reference Language) services are used to organize and manage observability resources such as queries, alerts, and dashboards.","Note that the service must be defined in the madatory index.yml file in the .baselime folder."]},{"l":"Properties","p":["Services have a set of properties that define the service's characteristics and behavior."]},{"i":"version-required","l":"version (required)","p":["The version property is a string that specifies the version of the Baselime CLI used to generate or deploy the service. It is used for version control and management.","Example:"]},{"i":"service-required","l":"service (required)","p":["The service property is a string that specifies the name of the service. It is used to identify the service and distinguish it from other services.","Example:"]},{"i":"description-required","l":"description (required)","p":["The description property is a string that provides more information about the service. It can include details about the purpose of the service, the components it includes, and any other relevant information.","Example:"]},{"i":"provider-required","l":"provider (required)","p":["The provider property is a string that specifies the cloud provider for the service. It is used to identify the provider and distinguish it from other providers. ORL supports the following providers:","aws","gcp(coming soon)","azure(coming soon)","cloudflare(coming soon)","vercel(coming soon)","Example:"]},{"i":"infrastructure-optional","l":"infrastructure (optional)","p":["The infrastructure property is an object that specifies the cloud infrastructure for the service. It has the following properties:"]},{"i":"stacks-optional","l":"stacks (optional)","p":["The stacks property is an array of strings that specifies the CloudFormation stacks that are part of the service. Baselime will automatically find all the cloud resources in the specified stacks and limit all observability rules (queries, alerts, etc.) to these stacks. If the stacks property is not specified, Baselime will include all cloud resources in the environment.","Example:"]},{"i":"templates-optional","l":"templates (optional)","p":["The templates property is an array of strings that specifies the templates to automatically download and implement for the service. Templates are used to define observability rules that can be shared and reused across multiple services. Each string is in the format workspace/template, where workspace is the name of the workspace where the template was defined and template is the unique ID of the template.","Example:"]},{"l":"variables","p":["The variables property is an object that enables you to define variables that can be used in the ORL queries and alerts within the service. These variables can be used to parameterize the ORL queries and alerts and make them more flexible and reusable.","Each variable has a name and one or more values. The values can be grouped by environment (e.g. prod, dev, etc.) or by any other criteria that makes sense for your service.","For example, you might define a threshold variable that has different values for different environments:","In this example, the threshold variable has a default value of 30, and different values for the prod and dev environments: 10 and 20, respectively.","To use this variable in an ORL query or alert, you can use the syntax:","In this example, the threshold variable will be replaced with the appropriate value depending on the environment in which the service is deployed.","It is important to note that variables are optional in services. If a variable is defined, it must have at least one value."]},{"l":"Example ORL Services","p":["Here are example ORL services that combine all of the above properties.","This ORL service is for a web application that is hosted on Amazon Web Services (AWS).","The cloud provider is AWS and the infrastructure consists of two CloudFormation stacks: webapp-stack and database-stack.","The service has two templates defined: baselime/lambda-logs-basics and workspace-name/template-name.","The service has two variables defined: threshold and frequency. The threshold variable has a default value of 30 and a value of 10 for the prod environment. The frequency variable has a default value of 30mins and a value of 5mins for the prod environment and a value of 0 9 ? * 2#1 * for the dev environment.","This ORL service is for a microservices architecture that is hosted on AWS."]}],[{"l":"ORL Queries","p":["ORL (Observability Reference Language) queries are used to retrieve and analyze data from various datasets in order to gain insights and improve observability of systems and services."]},{"l":"properties","p":["ORL queries have a set of properties that define the query's characteristics and behavior."]},{"i":"name-optional","l":"name (optional)","p":["The name of the ORL query is a string used to identify the query. It can be a human-readable name that describes the purpose of the query.","Example:"]},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL query is a string that provides more information about the query. It can include details about the data being queried, the calculations being performed, and any other relevant information.","Example:"]},{"l":"parameters","p":["The parameters of an ORL query define the datasets to query, the calculations to perform on the data, and any filters or groupings to apply."]},{"l":"datasets","p":["The datasets parameter is an array of strings that specify the names of the datasets to query. ORL supports querying multiple datasets simultaneously, allowing you to analyze data from different sources in a single query.","Example:"]},{"i":"filters-optional","l":"filters (optional)","p":[": Greater than",": Less than","!=: Does not equal","=: Equals","=: Greater than or equal to","=: Less than or equal to","DOES_NOT_EXIST: Does not exist (applies to fields that may or may not exist in the data)","DOES_NOT_INCLUDE: Does not include","Example:","EXISTS: Exists (applies to fields that may or may not exist in the data)","Filters can be used to narrow down the data being analyzed and focus on specific events or attributes.","IN: In (applies to arrays only)","INCLUDES: Includes","LIKE: Matches a string","MATCH_REGEX: Matches a regular expression","NOT_IN: Not in (applies to arrays only)","NOT_LIKE: Does not match a string","STARTS_WITH: Starts with (applies to strings only)","The filters parameter is an array of strings that specify conditions to filter the data by. Each string follows this format: 'key operation value', where key is the field to filter on, operation is the comparison operator to use, and value is the value to compare against. ORL supports the following operations:"]},{"i":"calculations-optional","l":"calculations (optional)","p":["The calculations parameter is an array of strings that specify the calculations to perform on the data. ORL supports the following calculations:","COUNT: Counts the number of events.","COUNT_DISTINCT: Counts the number of distinct occurences of a field (applies to strings only).","MAX: returns the maximum value of a field.","MIN: returns the minimum value of a field.","SUM: returns the sum of all values of a field.","AVG: returns the average of all values of a field.","MEDIAN: returns the median of all values of a field.","STDDEV: returns the sample standard deviation of a field.","VARIANCE: returns the sample variance of a field.","P001, P01, P05, P10, P25, P75, P90, P95, P99, P999: return the specified percentile of all values of a field.","Calculations can be used to perform statistical analysis on the data and derive insights such as the average request duration, the maximum response size, or the 95th percentile of request latencies.","Example:"]},{"i":"groupby-optional","l":"groupBy (optional)","p":["The groupBy parameter is an object that specifies how to segment the data by a field. It has the following fields:","value: The field to group the data by","limit: The maximum number of results to return (default: 10)","type: The type of the data field to group by (string, boolean, or number)","orderBy: The calculation to order the results by (default: the first calculation in the query)","order: The order in which to return the results (ASC or DESC, default: DESC)","Grouping the data by a field allows you to segment the results into distinct groups and analyze them separately.","Example:"]},{"i":"needle-optional","l":"needle (optional)","p":["The needle parameter is an object that specifies a search to perform on the data. It has the following fields:","value: The string to search for","matchCase: A boolean indicating whether the search should be case-sensitive (default: false)","isRegex: A boolean indicating whether the search value is a regular expression (default: false)","The needle can be used to find specific set of events or patterns in the data.","Example:"]},{"l":"Example ORL Queries","p":["Here are example ORL queries that combine all of the above properties.","This ORL query retrieves data from the otel traces dataset and performs several calculations on the data. It computes the average request duration, maximum response size, and 95th percentile of request latencies for each user ID in the dataset.","It filters the data to only include user IDs with a request duration greater than 500ms, and limits the results to the top 100 user IDs based on the average request duration. The results are ordered by the average request duration in descending order. The query also searches for the word \"error\" in the data and filters the results based on whether or not the word is present.","This ORL query calculates the total consumed read capacity units for each DynamoDB table in a service. It filters the data to only include events with a metric_name of ConsumedReadCapacityUnits and a unit of Count, and groups the results by TableName. The query returns the top 10 tables with the highest consumed read capacity units."]}],[{"l":"ORL Alerts","p":["ORL (Observability Reference Language) alerts are used to monitor data from various datasets and trigger notifications when specific conditions are met. ORL alerts are defined by a set of properties that specify the characteristics and behavior of the alert. They are based on ORL queries, which are used to retrieve and analyze the data. This allows you to monitor your systems and services and be notified when there are issues or anomalies that require attention.","Note that an alert can only be set for queries that include calculations. It is not possible to set an alert for a query that does not have any calculations."]},{"l":"properties","p":["ORL alerts have a set of properties that define the alert's characteristics and behavior."]},{"i":"name-optional","l":"name (optional)","p":["The name of the ORL alert is a string used to identify the alert. It can be a human-readable name that describes the purpose of the alert.","Example:"]},{"i":"description-optional","l":"description (optional)","p":["The description of the ORL alert is a string that provides more information about the alert. It can include details about the data being monitored, the conditions or thresholds being checked, and any other relevant information.","Example:"]},{"i":"enabled-optional","l":"enabled (optional)","p":["The enabled property is a boolean that specifies whether the ORL alert is currently active or inactive. If set to true, the alert will be triggered when the conditions or thresholds are met. If set to false, the alert will be disabled and will not trigger."]},{"l":"parameters","p":["The parameters of an ORL alert define the query to use, the frequency at which the query is run, the window of time over which the query's results are analyzed, and the threshold or condition that triggers the alert."]},{"l":"query","p":["The query parameter is a reference to an ORL query that defines the data to be monitored for the alert. It is specified as a string in the format !ref query_name, where query_name is the name of the ORL query.","Example:"]},{"l":"frequency","p":["The frequency parameter is a string that specifies how often the alert is checked. It can follows the format number time_unit, where number is a positive integer and time_unit is one of the following:","mins/ minutes: minutes","h/ hours: hours","d/ days: days","months: months","y/ years: years","The frequency can also be defined as a cron expression, following the AWS Cron Reference","Examples:","15 10 * * ? *: 10:15 AM (UTC) every day 0 18 ? * MON-FRI *: 6:00 PM Monday through Friday 0 8 1 * ? *: 8:00 AM on the first day of the month 0/10 * ? * MON-FRI *: Every 10 min on weekdays 0/5 8-17 ? * MON-FRI *: Every 5 minutes between 8:00 AM and 5:55 PM weekdays 0 9 ? * 2#1 *: 9:00 AM on the first Monday of each month","The alert is checked at the specified interval, and if the conditions are met, the alert is triggered.","Example:"]},{"l":"window","p":["The window parameter is a string that specifies the time window to consider for the alert. It follows the same format as the frequency parameter, but cannot be defined as a CRON expression.","The alert is only triggered if the conditions are met within the specified time window.","Example:"]},{"l":"threshold","p":["The threshold parameter is a string that specifies the threshold at which the alert is triggered. It is a value that inculdes the comparison and the value (e.g. 5).","The threshold is compared to the result of the first calculation in the query of the alert. If the result meets the specified condition, the alert is triggered.","The following comparison operators are supported:","=: Equals","!=: Does not equal",": Greater than","=: Greater than or equal to",": Less than","=: Less than or equal to","Example:"]},{"l":"channels","p":["The channels parameter is an array of objects that specify the channels to send the alert to. ORL supports the following types of channels:","slack: Sends the alert to a Slack channel email: Sends the alert to an email address pagerduty: Triggers a PagerDuty incident webhook: Sends the alert to a custom webhook URL","Each channel type has its own set of properties that define the behavior of the channel."]},{"l":"slack","p":["The slack channel type sends the alert to a Slack channel. It has the following properties:","targets: An array of strings that specify the Slack channels to send the alert to. Each string should be the name of a Slack channel (e.g. general). Example:","Note that it is necessary to install the Baselime Slack app and follow the Slack onboarding to get alerts on Slack."]},{"l":"email","p":["The email channel type sends the alert to an email address. It has the following properties:","targets: An array of strings that specify the email addresses to send the alert to. Each string should be a valid email address.","Example:"]},{"i":"pagerduty-coming-soon","l":"pagerduty [Coming Soon]","p":["The pagerduty channel type triggers a PagerDuty incident. It has the following properties:","serviceKey: A string that specifies the PagerDuty service key to use for the incident. This key is used to identify the PagerDuty service that the incident should be created in. eventAction: A string that specifies the action to take when creating the PagerDuty incident. Valid values are trigger (default) and resolve. client: A string that specifies the name of the client that the incident should be associated with. This is optional and can be used to provide context for the incident. clientUrl: A string that specifies the URL of the client that the incident should be associated with. This is optional and can be used to provide context for the incident.","Example:"]},{"l":"webhook","p":["The webhook channel type sends the alert to a custom webhook URL. It has the following properties:","url: A string that specifies the URL to send the alert to. method: A string that specifies the HTTP method to use when sending the alert. Valid values are POST (default) and GET. headers: An object that specifies the headers to include in the request. body: A string or object that specifies the body of the request. If a string is provided, it will be sent as-is. If an object is provided, it will be serialized as JSON and sent as the request body. (Coming Soon)","Example:"]},{"l":"Example ORL Alerts","p":["Here are example ORL alerts that combine all of the above properties."]},{"l":"DynamoDB ConsumedWriteCapacityUnits Alert","p":["This alert is triggered when the ConsumedWriteCapacityUnits metric for a DynamoDB table exceeds a specified threshold over a specified time window.","The alert is set to run every 15 minutes and check the metric over the past hour.","If the ConsumedWriteCapacityUnits exceed 5 over the past hour, the alert is triggered.","The alert is sent to a Slack channel called #dynamodb-alerts."]},{"l":"Lambda Timeout Alarm","p":["This alert checks the number of invocations that have timed out for Lambda functions in the service, and triggers if the count exceeds 10 over the past 15 minutes. It sends a notification to a custom webhook URL every 5 minutes."]}],[{"l":"Service Discovery for AWS","p":["Baselime organises your observability data around your services, we do this by creating a dictionary of resources within your AWS account. The services are based around your CloudFormation stacks and customised for your specific deployment framework like SST which often deploys multiple stacks per service. The services are created and synced within Baselime automatically and synced automatically when you deploy changes.","Then when we ingest your logs, metrics and traces the events are then automatically tagged with the service they belong to you can fully understand how your systems are operating. Baselime does not charge for number of services but rather number of events, so their are no limits on the number of services or resources you can have in your workspace.","Table of service with some key metrics"]},{"l":"Service Home","p":["Each service gets a home view which displays summary stats about the discovered services grouped by the types of resources in that service. The time range and resource type is adjustable using the drop down in the top right corner. A list of all the errros based on log-level error and uncaught exceptions is on the second tab.","lambda overview with 4 charts"]},{"l":"Supported services","p":["Currently there are pregenerated dashboards for lambda, dynamodb, sqs, and kinesis - if you have resources that you want to see in this page tell us about it in the slack community and we will add it right away."]}],[{"l":"Templates","p":["Templates are a powerful way to share patterns and best practices when it comes to observing your Serverless app. A template is a collection of .yml files that contain queries, alerts. Templates can be shared publicly or within organisations to help teams consistently manage their o11y, and prevent us from all having to write the same lambda timeout alert for every project!"]},{"l":"Getting started","p":["You can find the templates in the UI when creating new services, or through the baselime cli.","To list the available templates run","To preview a template run","preview a template","To save the template as a file you can run baselime templates get -w baselime -n metrics metrics.yml. This uses the \"redirection\" operator to redirect the standard output to a file"]},{"l":"Adding templates to services","p":["When you run baselime init to create a service you will be prompted to select templates that you can add.","baselime init service templates","The queries and alerts for this template will be available instantly.","To add templates to an existing service edit the .baselime/index.yml file, appending - ${workspace}/${name} to the templates array."]},{"l":"Becoming a template author","p":["Anyone can publish a template to the Baselime Template Repository. All our example templates can be found on github. If you would like an example templates for an AWS service or find a bug please create an issue","These templates are currently publicly accessible, to publish a private template please get in touch via the baselime-community slack channel","The templates are made up of","index.yml file",".yml files","Optional Licence and README files","The index.yml at the root of the templates folder must contain","You can structure your queries and alerts into any number of files and directories as you would like. Each file can also contain multiple queries and alerts.","To publish a template run"]}],[{"l":"Templates for Lambdas","p":["Monitor your Lambda functions with the Starter template full of queries, and alerts. These queries use your lambda-logs, to make sure your Lambdas are running great!"]},{"l":"Datasets","p":["Dataset","Docs","lambda-logs","Sending Lambda Logs to Baselime"]},{"l":"Queries","p":["Count of the number of events with LogLevel ERROR","Dataset","Description","Duration of lambda cold-starts","Duration of lambda invocations","Events with LogLevel ERROR","Highest Billed Duration","Highest Billed Duration Invocations","highest-billed-invocation","ID","Invocations with the highed billed duration","Lambda Invocations that reported a timeout","Lambda Timeouts","lambda-cold-start-duration","lambda-errors","lambda-invocations-durations","lambda-logs","Max and P99 duration for lambda invocations","Name","Statistics on the duration of lambda cold starts across the application","timeouts"]},{"l":"Alarms","p":["Name","Description","Triggered by","Threshold","Window","Errors during Lambda invocations","There were more than errors over the past 30mins","lambda-errors","10","30mins","Lambda invocations timeouts","There were more than 5 timeouts over the past 30mins","timeouts","5"]}],[{"l":"Templates for API Backends","p":["Monitor your API Backends with the Starter template for queries, and alerts. These queries use your Api Gateway Logs, and CloudWatch Metrics to help you make sure your API's are scalable and have great uptime."]},{"l":"Required Datasets","p":["Dataset","Docs","apigateway-logs","Sending API Gateway Logs to Baselime","cloudwatch-metrics","Sending CloudWatch Metrics to Baselime"]},{"l":"Queries","p":["API Errors","API Integration Latency","API Latency","API Overview","api-errors","api-integration-latency","api-latency","api-overview","apigateway-logs","cloudwatch-metrics","Counts the number of API errors and groups by status","Counts the number of requests","Dataset","Description","ID","Name","Provides stats on API latency (AVG, P90, P95, P99)","Request Count","request-count","Sums the values of API metrics","The speed of API gateway compared to the speed integration"]},{"l":"Alarms","p":["10","10mins","1min","4000","60000","60mins","API Errors Alarm","API Latency Alarm","api-errors","api-latency","Description","Name","Request Count Alarm","request-count","Threshold","Triggered by","Triggers an alarm if the AVG response latency is over a threshold","Triggers an alarm if the count of API errors is over a threshold","Triggers an alarm if the count of requests is over a threshold","Window"]},{"l":"Combos","p":["This template works great with","Lambda Logs Basics"]}],[{"l":"Templates for DynamoDB","p":["Monitor DynamoDB Performance with the baselime dynamodb template. These queries use your CloudWatch Metrics to show you how your tables are performing"]},{"l":"Datasets","p":["Dataset","Docs","cloudwatch-metrics","Sending CloudWatch Metrics to Baselime"]},{"l":"Queries","p":["cloudwatch-metrics","Dataset","Description","Duration of scans","DynamoDB Consumed Read Capacity","DynamoDB Consumed Write Capacity","dynamodb-consumed-read-capacity","dynamodb-consumed-write-capacity","dynamodb-streams","How many records have you processed with dynamodb streams","How much read capacity you have consumed","How much write capacity you have consumed","Id","Name","Number of items returned by a scan","Number of scans","Records Processed By A Stream","Scan Items Returned","Scan Latency","scan-count","scan-items-returned","scan-latency","Scans","Slow Get Item Requests","Slow Put Item Requests","Slow Update Item Requests","slow-get-item-requests","slow-put-item-requests","slow-update-item-requests","The 10 slowest Put Item requests Grouped By Table","The 10 slowest Update Item requests Grouped By Table","TTL DELETE ITEMS"]},{"l":"Alarms","p":["Name","Description","Triggered by","Threshold","Window","Friends don't let friends run scans","If the scan count is more than 10 for a 10 minute window alert","scan-count","10","10mins"]}],[{"l":"Installing the Baselime CLI","p":["The Baselime CLI is the primary way to interact with Baselime and your serverless observability data. It allows you to connect your serverless applications, query and explore your data, and set up integrations with your tools.","You can install the Baselime CLI using one of the following methods:"]},{"l":"Installing"},{"i":"installing-with-homebrew-for-macos","l":"Installing with Homebrew (for MacOS)","p":["Make sure you have Homebrew installed on your system. If you don't, you can install it by following the instructions here.","Run the following commands to add the Baselime tap to your Homebrew installation:"]},{"i":"installing-with-curl-for-macos-and-linux","l":"Installing with curl (for MacOS and Linux)","p":["Run the following command to download and install the Baselime CLI:","MacOS","Linux"]},{"i":"installing-with-npm-for-macos-linux-and-windows","l":"Installing with npm (for MacOS, Linux, and Windows)","p":["Make sure you have npm installed on your system. If you don't, you can install it by following the instructions here.","Run the following command to install the Baselime CLI:"]},{"i":"downloading-the-binary-for-macos-and-linux","l":"Downloading the binary (for MacOS, and Linux)","p":["You can download the latest version of the Baselime CLI binary from the releases page on GitHub.","Download the binary for your operating system and architecture (e.g., baselime_linux_x64 or baselime_darwin_x64).","Unzip the tarball with tar -xf baselime-os-arch-version.tar.gz","Make the binary executable with chmod +x baselime.","Move the binary to a directory in your PATH, such as /usr/local/bin, with mv baselime /usr/local/bin/baselime.","On some systems, you might need to run these commands with sudo."]},{"l":"Verifying the installation","p":["To verify that the Baselime CLI has been installed correctly, run the following command:","You should see the version number of the Baselime CLI that you installed.","If you encounter any issues during the installation process, please don't hesitate to contact us."]},{"l":"Authenticating the CLI","p":["Before you can use the Baselime CLI, you must authenticate it with your Baselime account. To do this, run the following command:","This command opens a new browser window and asks you to sign in to your Baselime account. Once you sign in, the CLI is authenticated and you can start using it to interact with your Baselime account."]},{"l":"Updating the Baselime CLI","p":["To update the Baselime CLI to the latest version, use one of the following commands depending on how you originally installed it:","If you installed with brew, run brew upgrade @baselime/cli","If you installed with curl, run baselime upgrade","If you installed with npm, run npm update -g @baselime/cli"]}],[{"l":"Getting Started with the Baselime CLI","p":["Welcome to the Baselime CLI! This guide will help you get up and running with the CLI so you can start using Baselime to gain visibility into your serverless architecture."]},{"l":"Prerequisites","p":["Before you can use the Baselime CLI, you'll need to:","Install the Baselime CLI. See the installation instructions for more details.","Connect your AWS Account to Baselime. See the quick start guide for instructions on how to do this."]},{"l":"First Steps","p":["Once you have the Baselime CLI installed and your AWS Account connected, you're ready to start using Baselime! Here are a few commands to get you started:","baselime iam: displays information about the current user logged in to the CLI.","baselime services list: List all of the services in the authenticated environment.","baselime query: Run a query against your telemetry data to find specific events or metrics.","baselime tail: Stream all of the events ingested into Baselime in real-time."]},{"l":"Next Steps","p":["Now that you've gotten your feet wet with the Baselime CLI, you can learn more about the other commands and features available. Here are a few places to start:","Check out the CLI reference for a full list of available commands and their options."]}],[{"l":"Anonymous Telemetry","p":["Baselime collects completely anonymous telemetry data about general CLI usage. Participation in this anonymous program is optional, and you can opt-out if you'd not like to share any information."]},{"i":"how-do-i-opt-out","l":"How do I opt-out?","p":["You can opt out-by running the following command:","You can re-enable telemetry if you'd like to rejoin the program by running."]},{"i":"why-do-we-collect-telemetry-data","l":"Why do we collect telemetry data?","p":["Telemetry data help up to accurately measure the Baselime CLI feature usage, pain points, and customisation across all developers. This data empowers us to build a better product for more developers.","It also allows us to verify if the improvements we make to the Baselime CLI are having a positive impact on the developer experience."]},{"i":"what-is-being-collected","l":"What is being collected?","p":["We measure the following anonymously:","Command invoked (ie. baselime apply, baselime queries run, or baselime events stream)","Version of Baselime in use","General machine information (e.g. number of CPUs, macOS/Windows/Linux, whether or not the command was run within CI)","An example telemetry event looks like:","These events are then sent to an endpoint hosted on our side."]},{"i":"what-about-sensitive-data-or-secrets","l":"What about sensitive data or secrets?","p":["We do not collect any metrics which may contain sensitive data.","This includes, but is not limited to: environment variables, file paths, contents of files, logs, or serialized errors."]},{"i":"will-the-telemetry-data-be-shared","l":"Will the telemetry data be shared?","p":["The data we collect is completely anonymous, not traceable to the source, and only meaningful in aggregate form.","No data we collect is personally identifiable.","In the future, we plan to share relevant data with the community through public dashboards or reports."]}],[{"l":"baselime alerts","p":["Use the baselime alerts to manage the alerts in your Baselime environment."]}],[{"l":"baselime destroy","p":["Use the baselime destroy to services previously created through Observability as Code."]}],[{"l":"baselime environments","p":["Use the baselime environments command to manage the connection of your AWS accounts to Baselime."]}],[{"l":"baselime iam","p":["Use the baselime iam command to display the currently logged-in user and environment."]}],[{"l":"baselime init","p":["Use the baselime init command to initialize a new service in the current directory."]}],[{"l":"baselime login","p":["Use the baselime login command to log in your Baselime account and select an environment."]}],[{"l":"baselime logout","p":["Use the baselime logout command to log out of Baselime."]}],[{"l":"baselime namespaces","p":["Use the baselime namespaces command to manage namespaces for organizing data within datasets."]}],[{"l":"baselime pull","p":["Use the baselime pull command to update local observability as code configurations with the latest state from the remote systems."]}],[{"l":"baselime push","p":["Use the baselime push command to sync Observability as Code configurations from your local folder to your Baselime account."]}],[{"l":"baselime queries","p":["Use the baselime queries command to manage saved queries."]}],[{"l":"baselime query","p":["Use the baselime query command to run a query on your telemetry data data."]}],[{"l":"baselime report","p":["Use the baselime report command to generate a report based on your observability data and assess the health and performance of your service."]}],[{"l":"baselime services","p":["Use the baselime services command to manage Baselime services."]}],[{"l":"baselime snapshot","p":["Use the baselime snapshot command to check all the alerts in your current service, create snapshots of the results, display them in the terminal, and output them to a file."]}],[{"l":"baselime tail","p":["Use the baselime tail command to stream events from your telemetry data in real-time."]}],[{"l":"baselime telemetry","p":["Use the baselime telemetry command to manage the usage telemetry data collected by the Baselime CLI."]}],[{"l":"baselime templates","p":["Use the baselime telemetry command to manage your observability templates."]}],[{"l":"baselime upgrade","p":["Use the baselime upgrade command to upgrade the Baselime CLI to the latest version. This method will work only if you installed the Baselime CLI with curl -s https://get.baselime.io | bash."]}],[{"l":"baselime validate","p":["Use the baselime validate command to validate your ORL configuration files."]}],[{"l":"Webhook Integration","p":["The Webhook integration enables you and your team to send POST requests to an http endpoint when an alert is triggered.","To set this up, set [channel].properties.type to webhook and a valid URL in the targets array.","When an alert triggers to a webhook channel, HTTP requests are made to the channel targets using the /POST method. Each request carries an event similar to the example outlines below."]}],[{"i":"baselime--slack-integration","l":"Baselime + Slack Integration","p":["The Baselime integration for Slack gives you and your team full visibility into your applications right in Slack channels, where you can get alerted, investigate incidents, and manage your observability, as a team."]}],[{"l":"Authentication","p":["Install the Baselime integraton for Slack.","Once you install the app in your Slack workspace, you can start interacting with Baselime app as a Personal app or access from channels. By default, the Baselime app is enabled in all the public channels. For private channels, you need to explicitly invite /invite @baselime","At this point, your Slack and Baselime user accounts are not linked. You will be prompted to log in Baselime. This is a primary step required to access the app.","Slack welcome message","The primary button will redirect you to the Baselime console where you can login and connect your Slack.","Once this is completed, you will be greeted with a help message \uD83C\uDF89.","Successful login"]}],[{"l":"Automated Alerts","p":["You can set one or multiple Slack channels to receive automated alerts.","Please make sure that the channels defined either are public, or you have manually added the Baselime Slack app to those.","Once configured, When an alert that is configured to send notifications to Slack is triggered, the Baselime Slack app will notify all the configured channels.","Slack alert"]}],[{"l":"Commands","p":["You can use the /baselime command to intercat with Baselime straight from Slack."]},{"l":"queries"},{"l":"run","p":["Run a query.","Options","--application: Name of the application","--ref: Query reference","--from: UTC start time - may also be relative eg: 1h, 20mins","--to: UTC end time - may also be relative eg: 1h, 20mins, now","--id: Query id","Result","Slack queries run result"]},{"l":"list","p":["[Coming Soon]"]},{"l":"help","p":["Displays help."]}],[{"l":"Data Security in Baselime","p":["Baselime is committed to ensuring the security and privacy of our users' data. We have implemented a number of measures to ensure that data is encrypted in transit and at rest, and that it is not accessible from the public internet. Here are some of the key data security features of Baselime:"]},{"l":"Data Encryption","p":["All data transferred to and from Baselime is encrypted in transit using industry-standard protocols such as HTTPS and TLS. In addition, all data is encrypted at rest."]},{"l":"Private VPCs and IAM Roles","p":["Baselime runs in private Virtual Private Clouds (VPCs) and utilizes IAM roles to ensure that data is only accessed by authorized users and processes."]},{"l":"No Public Access","p":["Baselime does not expose any data to the public internet. All data is accessed via secure, authenticated channels."]},{"l":"Modern Best Practices","p":["Baselime follows modern best practices for data security, including regularly updating and patching our systems, implementing network segmentation and access controls, and conducting regular security audits and penetration testing."]},{"l":"Data Scrubbing and Obfuscation","p":["Baselime provides tools for scrubbing and obfuscating sensitive data, such as passwords, secrets, and API keys. Users can block or obfuscate specific keys by dataset using the .baselimeignore file, or by using the baselime scrubbing command. In addition, Baselime automatically scrubs a predefined list of sensitive fields, including \"password\" and \"secret\".","To learn more about how to use these features to protect your data, see the Baselime Telemetry Data Privacy documentation."]},{"l":"Compliance","p":["We're currently working towards compliance with a number of industry-standard security and privacy frameworks, including GDPR, SOC2 and HIPAA. Please contact us for more information on our compliance status."]},{"l":"Support","p":["If you have any questions or concerns about the security of your data in Baselime, please don't hesitate to contact our support team. We are always here to help!"]}],[{"i":"telemetry-data-privacy-in-baselime-coming-soon","l":"Telemetry Data Privacy in Baselime [Coming Soon]","p":["Baselime is designed to help you observe the health and performance of your applications, and part of that involves collecting telemetry data. To ensure the privacy of your data, Baselime provides a number of features that enable you to control which data is collected and how it is used."]},{"l":"Blocking Keys","p":["Baselime enables you to block certain keys from being ingested into your datasets. This is particularly useful for sensitive information such as passwords, API keys, and other personal data. You can block keys for a specific dataset by using the baselime block-key command:","You can also block keys for multiple datasets at once by specifying the --dataset flag multiple times:","In addition to the command-line interface, you can also use a .baselimeignore file to block keys. The .baselimeignore file should be located in the root of your repository and should contain a list of keys to block, one per line, with the associated dataset. For example:"]},{"l":"Obfuscating Keys","p":["In addition to blocking keys, Baselime also allows you to obfuscate keys by replacing their values with a hash. This is useful for cases where you want to keep the structure of your data, but don't want to reveal sensitive information. You can obfuscate keys using the baselime obfuscate-key command:","As with blocking keys, you can obfuscate keys for multiple datasets by specifying the --dataset flag multiple times:","You can also use the .baselimeignore file to obfuscate keys. Just add the obfuscate keyword after the dataset name:","Keep in mind that obfuscating keys is a one-way process, meaning that once a key has been obfuscated, there is no way to recover the original value. Make sure to carefully consider which keys you want to obfuscate."]},{"i":"baselimeignore","l":".baselimeignore","p":["The .baselimeignore file allows you to specify keys that should be either blocked or obfuscated when data is ingested into Baselime. You can use this file to block or obfuscate multiple keys across multiple datasets.","To block or obfuscate a key, add a line to the .baselimeignore file in the following format:","For example, to block the data.user.email key in the logs dataset, you would add the following line to your .baselimeignore file:","To obfuscate the data.user.password key in the metrics dataset, you would add the following line:","Note that the .baselimeignore file should be placed in the root folder of your service and will be applied when you run baselime push.","Keep in mind that the .baselimeignore file is only applied to data that is ingested after the .baselimeignore file is pushed. Data that was ingested before the .baselimeignore file was pushed will not be affected."]},{"l":"Automatic scrubbing","p":["Baselime that automatically blocks sensitive information from being ingested into the telemetry data by default. This is done to ensure that sensitive data is not accidentally exposed.","The following keys are automatically scrubbed:","password","secret","passwd","api_key","pwd","apikey","access_token","auth","credentials","creds","Any nested field in your telemetry data that contains any of these automatically scrubbed keys will be blocked from ingestion by default.","To turn automatic scrubbing on or off for a specific dataset, use the following commands:"]}],[{"l":"Overview","p":["Baselime uses connctors to automatically ingest telemetry data from your cloud environments."]}],[{"l":"AWS Connector on Baselime","p":["The AWS Connector allows you to send data from your AWS resources to Baselime. This includes logs, traces, and metrics. By connecting your AWS account to Baselime, you can get a unified view of your serverless architecture, query your data, and set up alerts."]},{"l":"Setting up the AWS Connector","p":["The connector is an automated flow based on a CloudFormation template.","It can be done using the Baselime CLI or throught the web console."]},{"l":"Using the CLI","p":["To connect a cloud account to Baselime using the CLI, run the following command in your terminal","Once you've followed the interactive steps, the CLI will generate a CloudFormation template for you to deploy on your AWS account. FOllow the link in your terminal to deploy the temple on your AWS account.","Once deployed, login in your newly connected environment from the CLI.","The interactive prompt should list your newly connected environment.","Within minutes you should get telemetry data flowing through with the command"]},{"l":"Using the Web Console","p":["Navigate to the Baselime web console and login.","Follow the steps on the homescreen to connect a new AWS Account. Baselime will generate a CloudFormation template for you to deploy on your AWS account.","Once the template is deployed on AWS, return to the Baselime web console and refresh the page. You should see the newly connected AWS environment in the list of connected environment.","Within minutes telemtry data from your AWS environment should start displaying in the events streams in the Baselime web console."]},{"l":"Troubleshooting","p":["If you encounter any issers or error when connecting your AWS environment, please don't hesitate to contact us, or join our Slack community where we are always available to support."]},{"l":"CloudFormation Template","p":["The CloudFormation template is open-source and available here."]}],[{"i":"materialized-keys-in-baselime-coming-soon","l":"Materialized Keys in Baselime [Coming Soon]","p":["Materialized keys in Baselime allow you to calculate and create new keys from existing keys in your events. These new keys are based on calculations performed on one or multiple existing keys, which must be of type number.","By creating materialized keys, you can gain additional insights into your data and make it easier to track and analyze specific metrics.","To create a materialized key, you can use the baselime keys materialize command and specify the key name and calculation you want to perform. For example","This will create a new materialized key called total_revenue which is the result of multiplying the price and quantity keys in your events in the dataset lambda-logs.","Once you have created a materialized key, it will automatically be included in all your events going forward. You can view and manage your materialized keys in the Baselime console.","It's important to note that materialized keys are only recalculated when a new event is ingested, so any changes to the calculation or the underlying keys will not be reflected in historical data."]},{"l":"Syntax","p":["Materialized keys are calculated keys that are generated based on one or multiple existing keys in your events. Materialized keys can only be generated from existing keys of type number.","Materialized keys can only take existing key s of type number.","In Baselime, you can create Materialized Keys through the following operations:"]},{"l":"Basic arithmetic operations","p":["Addition: +","Subtraction: -","Multiplication: *","Division: /"]},{"l":"Advanced operations","p":["Modulo: %","Exponentiation: ^"]},{"l":"Trigonometric operations","p":["Sine: sin","Cosine: cos","Tangent: tan","Inverse Sine: asin","Inverse Cosine: acos","Inverse Tangent: atan"]},{"l":"Creating materialized keys","p":["To create a Materialized Key, you can use the baselime keys materialize command and specify the operation you want to perform on the existing keys. For example:","This command will create a Materialized Key called materialized_key in the dataset lambda-logs that is the result of adding key1 and key2.","You can also use multiple operations and keys to create more complex Materialized Keys. For example:","This command will create a Materialized Key called materialized_key that is the result of adding key1 and key2, and then multiplying that result with key3.","Once you have created a Materialized Key, you can use it just like any other key in your queries and alert rules."]},{"l":"Examples","p":["Here are some examples of how you can use materialized keys in your events:","Calculate the average response time for an API by dividing the total response time by the number of requests","Calculate the total revenue for an e-commerce store by multiplying the price and quantity keys for each order","Calculate the conversion rate for a marketing campaign by dividing the number of conversions by the number of impressions","You can use these materialized keys to set up alerts, create dashboards, and run queries to gain deeper insights into your data."]},{"l":"Troubleshooting","p":["If you encounter any issues with materialized keys, here are a few things you can try:","Check the syntax of your calculation to make sure it is correct","Make sure that all the keys used in your calculation exist in your events and are of type number","If you are using multiple keys, make sure they are all present in every event","If you are still experiencing issues, you can contact the Baselime support team for help."]}]]
---
label: FAQ
order: -3
---

# Tux FAQ

---

## General

### Does Tux train on my telemetry data?

No, Tux does not train on your telemetry data. Our third-party Language Model (LLM) providers also do not train on your specific data. Tux operates by following a specific process to generate answers to your queries:

- **User query**: A user asks a question
- **Code retrieval**: Baselime, performs a query to retrieve the relevant telemetry data to the user's question. During this process, strict permissions are enforced to ensure that only telemetry data for the user is retrieved
- **Prompt to Language Model**: Baselime sends a prompt, and the telemetry data to a Language Model (LLM). This prompt provides the context for the LLM to generate a meaningful response
- **Response to user**: The response generated by the LLM is then sent back to Tux and presented to the user

This process ensures that Tux can provide helpful answers to your questions while respecting data privacy and security by not training on or retaining your specific telemetry data.

### Is there a public facing Cody API?

Currently, there is no public-facing Tux API available.

### Does Tux require Baselime to function?

Yes, Tux relies on Baselime for two essential functions:

- It is used to retrieve context relevant to user queries
- Baselime acts as a proxy for the LLM provider to facilitate the interaction between Tux and the LLM

### Can Tux answer questions non-related to observability?

Tux is an expert in cloud computing and observability. Tux is not designed to answer non-observability related questions or provide general information on topics outside of cloud computing or your applications.

---

## Third party dependencies

### What third-party cloud services does Tux depend on?

Cody relies on one primary third-party dependency, i.e., OpenAI API.

### Can I use my own API keys?

No, you cannot use your own API keys at this point.

